<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>The Multiple Linear Regression Model</title>
    <meta content="Multiple linear regression is a statistical technique used to model the relationship between a single dependent variable and two or more independent variables." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper"><article-section id="article-body">
            <div class="article-action-buttons"><button class="btn-suggest-edit" title="Suggest Edit">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M4 21h4l11-11-4-4L4 17v4z" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button><button class="btn-create-issue" title="Create Issue">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"></circle>
                        <line stroke="currentColor" stroke-width="2" x1="12" x2="12" y1="8" y2="12"></line>
                        <circle cx="12" cy="16" fill="currentColor" r="1"></circle>
                    </svg>
                </button><button class="btn-download" title="Download">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M12 5v14m0 0l-6-6m6 6l6-6" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button></div>
            <p style="text-align: right;"><i>Last modified: March 08, 2023</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <header>Multiple Linear Regression</header>
            <p>Multiple linear regression is a statistical technique used to model the relationship between a single dependent variable and two or more independent variables. It extends the concept of simple linear regression by incorporating multiple predictors to explain the variability in the dependent variable. This method is widely used in fields such as economics, engineering, social sciences, and natural sciences to predict outcomes and understand the impact of various factors.</p>
            <h2 id="the-multiple-linear-regression-model">The Multiple Linear Regression Model</h2>
            <p>The general form of the multiple linear regression model is:</p>
            <p>$$
                y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \varepsilon
                $$</p>
            <p>Where:</p>
            <ul>
                <li>$y$ is the dependent variable (response variable).</li>
                <li>$x_1, x_2, \dots, x_p $ are the independent variables (predictor variables).</li>
                <li>$\beta_0 $ is the intercept term (the expected value of $y$ when all $x_j = 0 $).</li>
                <li>$\beta_1, \beta_2, \dots, \beta_p $ are the coefficients representing the change in $y$ for a one-unit change in $x_j $, holding all other variables constant.</li>
                <li>$\varepsilon $ is the random error term, accounting for the variability in $y$ not explained by the linear relationship.</li>
            </ul>
            <h3 id="matrix-representation">Matrix Representation</h3>
            <p>In matrix notation, the model can be expressed as:</p>
            <p>$$
                \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}
                $$</p>
            <p>Where:</p>
            <ul>
                <li>$\mathbf{y}$ is an $n \times 1$ vector of observations of the dependent variable.</li>
                <li>$\mathbf{X}$ is an $n \times (p+1)$ matrix of independent variables, including a column of ones for the intercept.</li>
                <li>$\boldsymbol{\beta}$ is a $(p+1) \times 1$ vector of coefficients.</li>
                <li>$\boldsymbol{\varepsilon}$ is an $n \times 1$ vector of error terms.</li>
            </ul>
            <h2 id="assumptions-of-the-model">Assumptions of the Model</h2>
            <p>For the multiple linear regression model to provide valid results, several key assumptions must be met:</p>
            <ol>
                <li><strong>Linearity</strong> means that the relationship between the dependent variable and each independent variable is linear.</li>
                <li><strong>Independence</strong> assumes that the observations are independent of one another.</li>
                <li><strong>Homoscedasticity</strong> ensures that the variance of the error terms remains constant across all levels of the independent variables.</li>
                <li><strong>Normality</strong> requires that the error terms are normally distributed with a mean of zero.</li>
                <li>Finally, <strong>no multicollinearity</strong> ensures that the independent variables are not perfectly correlated with each other.</li>
            </ol>
            <h2 id="estimation-of-coefficients">Estimation of Coefficients</h2>
            <h3 id="least-squares-method">Least Squares Method</h3>
            <p>The coefficients $\boldsymbol{\beta}$ are estimated using the Ordinary Least Squares (OLS) method, which minimizes the sum of squared residuals (the differences between observed and predicted values of $y$).</p>
            <p>The objective is to find $\hat{\boldsymbol{\beta}}$ such that:</p>
            <p>$$
                \hat{\boldsymbol{\beta}} = \underset{\boldsymbol{\beta}}{\text{argmin}} \, (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\top (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})
                $$</p>
            <h3 id="solution-using-matrix-algebra">Solution Using Matrix Algebra</h3>
            <p>By taking the derivative of the sum of squared residuals with respect to $\boldsymbol{\beta}$ and setting it to zero, we obtain the normal equations:</p>
            <p>$$
                \mathbf{X}^\top \mathbf{X} \hat{\boldsymbol{\beta}} = \mathbf{X}^\top \mathbf{y}
                $$</p>
            <p>Solving for $\hat{\boldsymbol{\beta}}$:</p>
            <p>$$
                \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
                $$</p>
            <p><strong>Conditions for Invertibility</strong>:</p>
            <ul>
                <li>The matrix $\mathbf{X}^\top \mathbf{X}$ must be invertible.</li>
                <li>This requires that the columns of $\mathbf{X}$ are linearly independent (no perfect multicollinearity).</li>
            </ul>
            <h2 id="interpretation-of-coefficients">Interpretation of Coefficients</h2>
            <ul>
                <li>The <strong>intercept ($\hat{\beta}_0$)</strong> represents the expected value of $y$ when all independent variables are zero.</li>
                <li>The <strong>slope coefficients ($\hat{\beta}_j$)</strong> indicate the expected change in $y$ for a one-unit increase in $x_j$, while holding all other variables constant.</li>
            </ul>
            <h2 id="assessing-model-fit">Assessing Model Fit</h2>
            <h3 id="coefficient-of-determination-r-2-">Coefficient of Determination ($R^2$)</h3>
            <p>$$
                R^2 = 1 - \frac{\text{SSR}}{\text{SST}}
                $$</p>
            <ul>
                <li><strong>SSR (Sum of Squared Residuals)</strong> measures the unexplained variation in the model, representing the error.</li>
                <li><strong>SST (Total Sum of Squares)</strong> captures the total variation in $y$, showing the overall variability in the dependent variable.</li>
            </ul>
            <p>An $R^2$ value close to 1 indicates a good fit.</p>
            <h3 id="adjusted-r-2-">Adjusted $R^2$</h3>
            <p>Adjusts $R^2$ for the number of predictors in the model:</p>
            <p>$$
                \text{Adjusted } R^2 = 1 - \left( \frac{\text{SSR} / (n - p - 1)}{\text{SST} / (n - 1)} \right)
                $$</p>
            <h2 id="hypothesis-testing">Hypothesis Testing</h2>
            <h3 id="testing-individual-coefficients">Testing Individual Coefficients</h3>
            <ul>
                <li><strong>Null Hypothesis ($H_0 $)</strong>: $\beta_j = 0 $</li>
                <li><strong>Alternative Hypothesis ($H_a $)</strong>: $\beta_j \neq 0 $</li>
                <li><strong>Test Statistic</strong>:</li>
            </ul>
            <p>$$
                t_j = \frac{\hat{\beta}_j}{\text{SE}(\hat{\beta}_j)}
                $$</p>
            <ul>
                <li><strong>Degrees of Freedom</strong>: $n - p - 1$</li>
            </ul>
            <h3 id="testing-overall-model-significance">Testing Overall Model Significance</h3>
            <ul>
                <li><strong>Null Hypothesis ($H_0 $)</strong>: All $\beta_j = 0 $</li>
                <li><strong>Alternative Hypothesis ($H_a $)</strong>: At least one $\beta_j \neq 0 $</li>
                <li><strong>F-Statistic</strong>:</li>
            </ul>
            <p>$$
                F = \frac{(\text{SST} - \text{SSR}) / p}{\text{SSR} / (n - p - 1)}
                $$</p>
            <ul>
                <li><strong>Degrees of Freedom</strong>: $p $ and $n - p - 1$</li>
            </ul>
            <h2 id="diagnosing-multicollinearity">Diagnosing Multicollinearity</h2>
            <h3 id="variance-inflation-factor-vif-">Variance Inflation Factor (VIF)</h3>
            <p>Measures how much the variance of an estimated coefficient increases due to multicollinearity:</p>
            <p>$$
                \text{VIF}_j = \frac{1}{1 - R_j^2}
                $$</p>
            <ul>
                <li>$R_j^2$ is the $R^2$ from regressing $x_j $ on the other predictors.</li>
                <li>A VIF value greater than 5 or 10 indicates high multicollinearity.</li>
            </ul>
            <h3 id="remedies">Remedies</h3>
            <ul>
                <li><strong>Remove variables</strong> by excluding one of the correlated variables from the model.</li>
                <li><strong>Combine variables</strong> to create composite or aggregated variables that reduce redundancy.</li>
                <li><strong>Regularization</strong> techniques, such as ridge regression, can be applied to handle multicollinearity effectively.</li>
            </ul>
            <h2 id="assumption-diagnostics">Assumption Diagnostics</h2>
            <h3 id="residual-analysis">Residual Analysis</h3>
            <ul>
                <li><strong>Plot residuals vs. fitted values</strong> to check for any patterns that may indicate non-linearity or heteroscedasticity.</li>
                <li><strong>Normal Q-Q plot</strong> to assess whether the residuals follow a normal distribution.</li>
            </ul>
            <h3 id="durbin-watson-test">Durbin-Watson Test</h3>
            <p>Checks for autocorrelation in residuals:</p>
            <p>$$
                D = \frac{\sum_{i=2}^n (e_i - e_{i-1})^2}{\sum_{i=1}^n e_i^2}
                $$</p>
            <p>Values close to 2 indicate no autocorrelation.</p>
            <h2 id="extensions">Extensions</h2>
            <h3 id="interaction-terms">Interaction Terms</h3>
            <p>Include products of independent variables to model interactions:</p>
            <p>$$
                y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 (x_1 x_2) + \varepsilon
                $$</p>
            <h3 id="polynomial-regression">Polynomial Regression</h3>
            <p>Model non-linear relationships by including polynomial terms:</p>
            <p>$$
                y = \beta_0 + \beta_1 x + \beta_2 x^2 + \dots + \beta_k x^k + \varepsilon
                $$</p>
            <h3 id="regularization-techniques">Regularization Techniques</h3>
            <p><strong>Ridge Regression</strong>: Adds penalty for large coefficients.</p>
            <p>$$
                \hat{\boldsymbol{\beta}}_{\text{ridge}} = (\mathbf{X}^\top \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^\top \mathbf{y}
                $$</p>
            <p><strong>Lasso Regression</strong>: Encourages sparsity in coefficients.</p>
            <p>$$
                \text{Minimize } \sum_{i=1}^n (y_i - \hat{y}<em>i)^2 + \lambda \sum</em>{j=1}^p |\beta_j|
                $$</p>
            <h2 id="example-multicollinearity-between-variables">Example: Multicollinearity Between Variables</h2>
            <p>Suppose we have the following data on the number of hours studied ($x_1$), the number of practice exams taken ($x_2$), and the test scores ($y$):</p>
            <p>
            <table>
                <tr>
                    <td>Hours Studied ($x_1$)</td>
                    <td>Practice Exams ($x_2$)</td>
                    <td>Test Score ($y$)</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td>1</td>
                    <td>50</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td>2</td>
                    <td>60</td>
                </tr>
                <tr>
                    <td>6</td>
                    <td>3</td>
                    <td>70</td>
                </tr>
                <tr>
                    <td>8</td>
                    <td>4</td>
                    <td>80</td>
                </tr>
            </table>
            </p>
            <h3 id="observations">Observations</h3>
            <p>Before proceeding, it's important to notice that $x_2$ is directly proportional to $x_1$:</p>
            <p>$$
                x_2 = \frac{1}{2} x_1
                $$</p>
            <p>This means that there is perfect multicollinearity between $x_1$ and $x_2$. In multiple linear regression, perfect multicollinearity causes the design matrix to be singular, making it impossible to uniquely estimate the regression coefficients for $x_1$ and $x_2$.</p>
            <h3 id="implications-of-multicollinearity">Implications of Multicollinearity</h3>
            <p>When independent variables are perfectly correlated, the matrix $X^\top X $ (where $X $ is the design matrix) becomes singular (non-invertible). This prevents us from calculating the coefficients using the normal equation:</p>
            <p>$$
                \hat{\boldsymbol{\beta}} = (X^\top X)^{-1} X^\top \boldsymbol{y}
                $$</p>
            <h3 id="adjusted-approach">Adjusted Approach</h3>
            <p>Given the perfect linear relationship between $x_1$ and $x_2$, we can simplify the model by combining $x_1$ and $x_2$ into a single variable or by using only one of them in the regression.</p>
            <h4 id="simplifying-the-model">Simplifying the Model</h4>
            <p>Since $x_2 = \frac{1}{2} x_1$, we can express $y$ solely in terms of $x_1$:</p>
            <p>$$
                y = \beta_0 + \beta_1 x_1 + \beta_2 \left( \frac{1}{2} x_1 \right) = \beta_0 + \left( \beta_1 + \frac{\beta_2}{2} \right) x_1
                $$</p>
            <p>Let $\gamma = \beta_1 + \frac{\beta_2}{2}$. The model becomes:</p>
            <p>$$
                y = \beta_0 + \gamma x_1
                $$</p>
            <p>Now, we can proceed with a simple linear regression of $y$ on $x_1$.</p>
            <h3 id="step-by-step-calculation">Step-by-Step Calculation</h3>
            <h4 id="1-calculate-the-means">1. Calculate the Means</h4>
            <p>Compute the mean of $x_1$ and $y$:</p>
            <p>$$
                \bar{x}_1 = \frac{2 + 4 + 6 + 8}{4} = \frac{20}{4} = 5
                $$</p>
            <p>$$
                \bar{y} = \frac{50 + 60 + 70 + 80}{4} = \frac{260}{4} = 65
                $$</p>
            <h4 id="2-calculate-the-sum-of-squares">2. Calculate the Sum of Squares</h4>
            <p>Compute the sum of squares for $x_1$ and the cross-product of $x_1$ and $y$:</p>
            <p>$$
                SS_{x_1 x_1} = \sum_{i=1}^{n} (x_{1i} - \bar{x}_1)^2 = (2 - 5)^2 + (4 - 5)^2 + (6 - 5)^2 + (8 - 5)^2 = 20
                $$</p>
            <p>$$
                SS_{x_1 y} = \sum_{i=1}^{n} (x_{1i} - \bar{x}_1)(y_i - \bar{y}) = (2 - 5)(50 - 65) + (4 - 5)(60 - 65) + (6 - 5)(70 - 65) + (8 - 5)(80 - 65) = 100
                $$</p>
            <h4 id="3-calculate-the-regression-coefficients">3. Calculate the Regression Coefficients</h4>
            <p>Compute the slope ($\hat{\gamma}$) and intercept ($\hat{\beta}_0 $):</p>
            <p>$$
                \hat{\gamma} = \frac{SS_{x_1 y}}{SS_{x_1 x_1}} = \frac{100}{20} = 5
                $$</p>
            <p>$$
                \hat{\beta}_0 = \bar{y} - \hat{\gamma} \bar{x}_1 = 65 - (5)(5) = 40
                $$</p>
            <h4 id="4-write-the-regression-equation">4. Write the Regression Equation</h4>
            <p>The best-fitting line is:</p>
            <p>$$
                \hat{y} = 40 + 5 x_1
                $$</p>
            <h4 id="5-verify-the-model-with-the-data">5. Verify the Model with the Data</h4>
            <p>Compute the predicted $y$ values using the regression equation:</p>
            <p>For $x_1 = 2$:</p>
            <p>$$
                \hat{y} = 40 + 5(2) = 50
                $$</p>
            <p>For $x_1 = 4 $:</p>
            <p>$$
                \hat{y} = 40 + 5(4) = 60
                $$</p>
            <p>For $x_1 = 6 $:</p>
            <p>$$
                \hat{y} = 40 + 5(6) = 70
                $$</p>
            <p>For $x_1 = 8 $:</p>
            <p>$$
                \hat{y} = 40 + 5(8) = 80
                $$</p>
            <p>The predicted values match the actual test scores perfectly.</p>
            <p>Plot:</p>
            <p><img alt="output(16)" src="https://github.com/user-attachments/assets/ee0c3bb1-ce28-4418-9bb9-def7d90f273e" /></p>
            <h2 id="example-no-perfect-multicollinearity">Example: No Perfect Multicollinearity</h2>
            <p>Suppose we have the following data on the number of hours studied ($x_1$), attendance rate ($x_2$) as a percentage, and test scores ($y$):</p>
            <p>
            <table>
                <tr>
                    <td>Student ($i $)</td>
                    <td>Hours Studied ($x_{1i}$)</td>
                    <td>Attendance Rate ($x_{2i}$)</td>
                    <td>Test Score ($y_i$)</td>
                </tr>
                <tr>
                    <td>1</td>
                    <td>2</td>
                    <td>70</td>
                    <td>65</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td>3</td>
                    <td>80</td>
                    <td>70</td>
                </tr>
                <tr>
                    <td>3</td>
                    <td>5</td>
                    <td>60</td>
                    <td>75</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td>7</td>
                    <td>90</td>
                    <td>85</td>
                </tr>
                <tr>
                    <td>5</td>
                    <td>9</td>
                    <td>95</td>
                    <td>95</td>
                </tr>
            </table>
            </p>
            <h3 id="objective">Objective</h3>
            <p>We aim to fit a multiple linear regression model of the form:</p>
            <p>$$
                y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon
                $$</p>
            <p>where:</p>
            <ul>
                <li>$y$ is the dependent variable (test score),</li>
                <li>$x_1$ is the number of hours studied,</li>
                <li>$x_2$ is the attendance rate,</li>
                <li>$\varepsilon $ is the error term.</li>
            </ul>
            <h3 id="step-by-step-calculation">Step-by-Step Calculation</h3>
            <h4 id="1-organize-the-data">1. Organize the Data</h4>
            <p>First, compute the necessary sums and products:</p>
            <p>
            <table>
                <tr>
                    <td>$i $</td>
                    <td>$x_{1i}$</td>
                    <td>$x_{2i}$</td>
                    <td>$y_i$</td>
                    <td>$x_{1i}^2$</td>
                    <td>$x_{2i}^2$</td>
                    <td>$x_{1i} x_{2i}$</td>
                    <td>$x_{1i} y_i$</td>
                    <td>$x_{2i} y_i$</td>
                </tr>
                <tr>
                    <td>1</td>
                    <td>2</td>
                    <td>70</td>
                    <td>65</td>
                    <td>4</td>
                    <td>4,900</td>
                    <td>140</td>
                    <td>130</td>
                    <td>4,550</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td>3</td>
                    <td>80</td>
                    <td>70</td>
                    <td>9</td>
                    <td>6,400</td>
                    <td>240</td>
                    <td>210</td>
                    <td>5,600</td>
                </tr>
                <tr>
                    <td>3</td>
                    <td>5</td>
                    <td>60</td>
                    <td>75</td>
                    <td>25</td>
                    <td>3,600</td>
                    <td>300</td>
                    <td>375</td>
                    <td>4,500</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td>7</td>
                    <td>90</td>
                    <td>85</td>
                    <td>49</td>
                    <td>8,100</td>
                    <td>630</td>
                    <td>595</td>
                    <td>7,650</td>
                </tr>
                <tr>
                    <td>5</td>
                    <td>9</td>
                    <td>95</td>
                    <td>95</td>
                    <td>81</td>
                    <td>9,025</td>
                    <td>855</td>
                    <td>855</td>
                    <td>9,025</td>
                </tr>
                <tr>
                    <td><strong>Total</strong></td>
                    <td><strong>26</strong></td>
                    <td><strong>395</strong></td>
                    <td><strong>390</strong></td>
                    <td><strong>168</strong></td>
                    <td><strong>32,025</strong></td>
                    <td><strong>2,165</strong></td>
                    <td><strong>2,165</strong></td>
                    <td><strong>31,325</strong></td>
                </tr>
            </table>
            </p>
            <h4 id="2-compute-the-means">2. Compute the Means</h4>
            <p>$$
                \bar{x}<em>1 = \frac{\sum x</em>{1i}}{n} = \frac{26}{5} = 5.2
                $$</p>
            <p>$$
                \bar{x}<em>2 = \frac{\sum x</em>{2i}}{n} = \frac{395}{5} = 79
                $$</p>
            <p>$$
                \bar{y} = \frac{\sum y_i}{n} = \frac{390}{5} = 78
                $$</p>
            <h4 id="3-compute-sum-of-squares-and-cross-products">3. Compute Sum of Squares and Cross Products</h4>
            <p><strong>Sum of Squares for $x_1$:</strong></p>
            <p>$$
                SS_{x_1 x_1} = \sum x_{1i}^2 - n \bar{x}_1^2 = 168 - 5 (5.2)^2 = 168 - 135.2 = 32.8
                $$</p>
            <p><strong>Sum of Squares for $x_2$:</strong></p>
            <p>$$
                SS_{x_2 x_2} = \sum x_{2i}^2 - n \bar{x}_2^2 = 32{,}025 - 5 (79)^2 = 32{,}025 - 31{,}205 = 820
                $$</p>
            <p><strong>Sum of Cross Products between $x_1$ and $x_2$:</strong></p>
            <p>$$
                SS_{x_1 x_2} = \sum x_{1i} x_{2i} - n \bar{x}_1 \bar{x}_2 = 2{,}165 - 5 (5.2)(79) = 2{,}165 - 2{,}054 = 111
                $$</p>
            <p><strong>Sum of Cross Products between $x_1$ and $y$:</strong></p>
            <p>$$
                SS_{x_1 y} = \sum x_{1i} y_i - n \bar{x}_1 \bar{y} = 2{,}165 - 5 (5.2)(78) = 2{,}165 - 2{,}028 = 137
                $$</p>
            <p><strong>Sum of Cross Products between $x_2$ and $y$:</strong></p>
            <p>$$
                SS_{x_2 y} = \sum x_{2i} y_i - n \bar{x}_2 \bar{y} = 31{,}325 - 5 (79)(78) = 31{,}325 - 30{,}810 = 515
                $$</p>
            <h4 id="4-compute-the-regression-coefficients">4. Compute the Regression Coefficients</h4>
            <p>We use the formulas for multiple linear regression coefficients:</p>
            <p><strong>Denominator (Determinant):</strong></p>
            <p>$$
                D = SS_{x_1 x_1} SS_{x_2 x_2} - (SS_{x_1 x_2})^2 = (32.8)(820) - (111)^2 = 26{,}896 - 12{,}321 = 14{,}575
                $$</p>
            <p><strong>Coefficient $\hat{\beta}_1$:</strong></p>
            <p>$$
                \hat{\beta}<em>1 = \frac{SS</em>{x_1 y} SS_{x_2 x_2} - SS_{x_1 x_2} SS_{x_2 y}}{D}
                $$</p>
            <p>$$
                \hat{\beta}_1 = \frac{(137)(820) - (111)(515)}{14{,}575} = \frac{112{,}340 - 57{,}165}{14{,}575} = \frac{55{,}175}{14{,}575} \approx 3.785
                $$</p>
            <p><strong>Coefficient $\hat{\beta}_2$:</strong></p>
            <p>$$
                \hat{\beta}<em>2 = \frac{SS</em>{x_2 y} SS_{x_1 x_1} - SS_{x_1 x_2} SS_{x_1 y}}{D}
                $$</p>
            <p>$$
                \hat{\beta}_2 = \frac{(515)(32.8) - (111)(137)}{14{,}575} = \frac{16{,}892 - 15{,}207}{14{,}575} = \frac{1{,}685}{14{,}575} \approx 0.116
                $$</p>
            <p><strong>Intercept $\hat{\beta}_0 $:</strong></p>
            <p>$$
                \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}_1 - \hat{\beta}_2 \bar{x}_2
                $$</p>
            <p>$$
                \hat{\beta}_0 = 78 - (3.785)(5.2) - (0.116)(79) = 78 - 19.642 - 9.164 \approx 49.194
                $$</p>
            <h4 id="5-write-the-regression-equation">5. Write the Regression Equation</h4>
            <p>The estimated multiple linear regression model is:</p>
            <p>$$
                \hat{y} = 49.194 + 3.785 x_1 + 0.116 x_2
                $$</p>
            <h4 id="6-interpret-the-coefficients">6. Interpret the Coefficients</h4>
            <ul>
                <li>The <strong>intercept ($\beta_0 = 49.194$)</strong> represents the estimated test score when both hours studied and attendance rate are zero. While this value may not have practical significance, it is necessary for constructing the mathematical model.</li>
                <li>The <strong>coefficient of $x_1$ ($\beta_1 = 3.785$)</strong> suggests that for each additional hour studied, the test score increases by approximately 3.785 points, assuming the attendance rate remains constant.</li>
                <li>The <strong>coefficient of $x_2$ ($\beta_2 = 0.116$)</strong> indicates that for each 1% increase in attendance rate, the test score rises by approximately 0.116 points, holding hours studied constant.</li>
            </ul>
            <h4 id="7-verify-the-model-with-the-data">7. Verify the Model with the Data</h4>
            <p>Compute the predicted test scores ($\hat{y}_i$) and residuals ($e_i = y_i - \hat{y}_i$).</p>
            <p><strong>For Student 1:</strong></p>
            <p>$$
                \hat{y}_1 = 49.194 + 3.785 (2) + 0.116 (70) = 49.194 + 7.570 + 8.120 = 64.884
                $$</p>
            <p>$$
                e_1 = y_1 - \hat{y}_1 = 65 - 64.884 = 0.116
                $$</p>
            <p><strong>For Student 2:</strong></p>
            <p>$$
                \hat{y}_2 = 49.194 + 3.785 (3) + 0.116 (80) = 49.194 + 11.355 + 9.280 = 69.829
                $$</p>
            <p>$$
                e_2 = 70 - 69.829 = 0.171
                $$</p>
            <p><strong>For Student 3:</strong></p>
            <p>$$
                \hat{y}_3 = 49.194 + 3.785 (5) + 0.116 (60) = 49.194 + 18.925 + 6.960 = 75.079
                $$</p>
            <p>$$
                e_3 = 75 - 75.079 = -0.079
                $$</p>
            <p><strong>For Student 4:</strong></p>
            <p>$$
                \hat{y}_4 = 49.194 + 3.785 (7) + 0.116 (90) = 49.194 + 26.495 + 10.440 = 86.129
                $$</p>
            <p>$$
                e_4 = 85 - 86.129 = -1.129
                $$</p>
            <p><strong>For Student 5:</strong></p>
            <p>$$
                \hat{y}_5 = 49.194 + 3.785 (9) + 0.116 (95) = 49.194 + 34.065 + 11.020 = 94.279
                $$</p>
            <p>$$
                e_5 = 95 - 94.279 = 0.721
                $$</p>
            <p>The residuals are small, indicating a good fit of the model to the data.</p>
            <p>Plot:</p>
            <p><img alt="output(17)" src="https://github.com/user-attachments/assets/4cd38537-ee4a-4c6c-b788-fa614394967c" /></p>
            <h3 id="checking-for-multicollinearity">Checking for Multicollinearity</h3>
            <p>Compute the correlation coefficient between $x_1$ and $x_2$:</p>
            <p>$$
                r_{x_1 x_2} = \frac{SS_{x_1 x_2}}{\sqrt{SS_{x_1 x_1} \times SS_{x_2 x_2}}}
                $$</p>
            <p>$$
                r_{x_1 x_2} = \frac{111}{\sqrt{32.8 \times 820}} = \frac{111}{\sqrt{26{,}896}} = \frac{111}{164} \approx 0.677
                $$</p>
            <p>A correlation coefficient of approximately 0.677 indicates a moderate correlation, not perfect multicollinearity.</p>
        </article-section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#the-multiple-linear-regression-model">The Multiple Linear Regression Model</a>
                    <ol>
                        <li><a href="#matrix-representation">Matrix Representation</a></li>
                    </ol>
                </li>
                <li><a href="#assumptions-of-the-model">Assumptions of the Model</a></li>
                <li><a href="#estimation-of-coefficients">Estimation of Coefficients</a>
                    <ol>
                        <li><a href="#least-squares-method">Least Squares Method</a></li>
                        <li><a href="#solution-using-matrix-algebra">Solution Using Matrix Algebra</a></li>
                    </ol>
                </li>
                <li><a href="#interpretation-of-coefficients">Interpretation of Coefficients</a></li>
                <li><a href="#assessing-model-fit">Assessing Model Fit</a>
                    <ol>
                        <li><a href="#coefficient-of-determination-r-2-">Coefficient of Determination ($R^2$)</a></li>
                        <li><a href="#adjusted-r-2-">Adjusted $R^2$</a></li>
                    </ol>
                </li>
                <li><a href="#hypothesis-testing">Hypothesis Testing</a>
                    <ol>
                        <li><a href="#testing-individual-coefficients">Testing Individual Coefficients</a></li>
                        <li><a href="#testing-overall-model-significance">Testing Overall Model Significance</a></li>
                    </ol>
                </li>
                <li><a href="#diagnosing-multicollinearity">Diagnosing Multicollinearity</a>
                    <ol>
                        <li><a href="#variance-inflation-factor-vif-">Variance Inflation Factor (VIF)</a></li>
                        <li><a href="#remedies">Remedies</a></li>
                    </ol>
                </li>
                <li><a href="#assumption-diagnostics">Assumption Diagnostics</a>
                    <ol>
                        <li><a href="#residual-analysis">Residual Analysis</a></li>
                        <li><a href="#durbin-watson-test">Durbin-Watson Test</a></li>
                    </ol>
                </li>
                <li><a href="#extensions">Extensions</a>
                    <ol>
                        <li><a href="#interaction-terms">Interaction Terms</a></li>
                        <li><a href="#polynomial-regression">Polynomial Regression</a></li>
                        <li><a href="#regularization-techniques">Regularization Techniques</a></li>
                    </ol>
                </li>
                <li><a href="#example-multicollinearity-between-variables">Example: Multicollinearity Between Variables</a>
                    <ol>
                        <li><a href="#observations">Observations</a></li>
                        <li><a href="#implications-of-multicollinearity">Implications of Multicollinearity</a></li>
                        <li><a href="#adjusted-approach">Adjusted Approach</a>
                            <ol>
                                <li><a href="#simplifying-the-model">Simplifying the Model</a></li>
                            </ol>
                        </li>
                        <li><a href="#step-by-step-calculation">Step-by-Step Calculation</a>
                            <ol>
                                <li><a href="#1-calculate-the-means">1. Calculate the Means</a></li>
                                <li><a href="#2-calculate-the-sum-of-squares">2. Calculate the Sum of Squares</a></li>
                                <li><a href="#3-calculate-the-regression-coefficients">3. Calculate the Regression Coefficients</a></li>
                                <li><a href="#4-write-the-regression-equation">4. Write the Regression Equation</a></li>
                                <li><a href="#5-verify-the-model-with-the-data">5. Verify the Model with the Data</a></li>
                            </ol>
                        </li>
                    </ol>
                </li>
                <li><a href="#example-no-perfect-multicollinearity">Example: No Perfect Multicollinearity</a>
                    <ol>
                        <li><a href="#objective">Objective</a></li>
                        <li><a href="#step-by-step-calculation">Step-by-Step Calculation</a>
                            <ol>
                                <li><a href="#1-organize-the-data">1. Organize the Data</a></li>
                                <li><a href="#2-compute-the-means">2. Compute the Means</a></li>
                                <li><a href="#3-compute-sum-of-squares-and-cross-products">3. Compute Sum of Squares and Cross Products</a></li>
                                <li><a href="#4-compute-the-regression-coefficients">4. Compute the Regression Coefficients</a></li>
                                <li><a href="#5-write-the-regression-equation">5. Write the Regression Equation</a></li>
                                <li><a href="#6-interpret-the-coefficients">6. Interpret the Coefficients</a></li>
                                <li><a href="#7-verify-the-model-with-the-data">7. Verify the Model with the Data</a></li>
                            </ol>
                        </li>
                        <li><a href="#checking-for-multicollinearity">Checking for Multicollinearity</a></li>
                    </ol>
                </li>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li>Basic Concepts<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/axioms_of_probability.html">Axioms of Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/bayes_theorem.html">Bayes Theorem</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/bayesian_vs_frequentist.html">Bayesian vs Frequentist</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/conditional_probability.html">Conditional Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/descriptive_statistics.html">Descriptive Statistics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/geometric_probability.html">Geometric Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/introduction_to_probability.html">Introduction to Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/introduction_to_statistics.html">Introduction to Statistics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/probability_tree.html">Probability Tree</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/standard_error_and_lln.html">Standard Error and Lln</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/total_probability.html">Total Probability</a></li>
                        </ol>
                    </li>
                    <li>Probability Distributions<ol>
                            <li>Continuous Distributions<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/beta_distribution.html">Beta Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/chi_square_distribution.html">Chi Square Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/exponential_distribution.html">Exponential Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/f_distribution.html">F Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/gamma_distribution.html">Gamma Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/log_normal_distribution.html">Log Normal Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/normal_distribution.html">Normal Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/student_t_distribution.html">Student T Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/uniform_distribution.html">Uniform Distribution</a></li>
                                </ol>
                            </li>
                            <li>Discrete Distributions<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/binomial_distribution.html">Binomial Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/geometric_distribution.html">Geometric Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/negative_binomial_distribution.html">Negative Binomial Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/poisson_distribution.html">Poisson Distribution</a></li>
                                </ol>
                            </li>
                            <li>Intro<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/central_limit_theorem.html">Central Limit Theorem</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/introduction_to_distributions.html">Introduction to Distributions</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/normal_curve_and_z_score.html">Normal Curve and z Score</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/statistical_moments.html">Statistical Moments</a></li>
                                </ol>
                            </li>
                        </ol>
                    </li>
                    <li>Correlation and Regression<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/correlation.html">Correlation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/covariance.html">Covariance</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/logistic_regression.html">Logistic Regression</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/metrics.html">Metrics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/multiple_regression.html">Multiple Regression</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/simple_linear_regression.html">Simple Linear Regression</a></li>
                        </ol>
                    </li>
                    <li>Statistical Inference<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/analysis_of_categorical_data.html">Analysis of Categorical Data</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/analysis_of_variance.html">Analysis of Variance</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/confidence_intervals.html">Confidence Intervals</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/hypothesis_testing.html">Hypothesis Testing</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/multiple_comparisons.html">Multiple Comparisons</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/null_hypothesis.html">Null Hypothesis</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/resampling.html">Resampling</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/type_i_and_type_ii_errors.html">Type i and Type Ii Errors</a></li>
                        </ol>
                    </li>
                    <li>Time Series Analysis<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/arima_models.html">Arima Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autocorrelation_function.html">Autocorrelation Function</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autocovariance_function.html">Autocovariance Function</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autoregressive_models.html">Autoregressive Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/backward_shift_operator.html">Backward Shift Operator</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/difference_equations.html">Difference Equations</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/forecasting.html">Forecasting</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/invertibility.html">Invertibility</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/moving_average_models.html">Moving Average Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/random_walk.html">Random Walk</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/seasonality_and_trends.html">Seasonality and Trends</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/series.html">Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/stationarity.html">Stationarity</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/statistical_moments_and_time_series.html">Statistical Moments and Time Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/time_series.html">Time Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/time_series_modeling.html">Time Series Modeling</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/yule_walker_equations.html">Yule Walker Equations</a></li>
                        </ol>
                    </li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>Thank you for visiting my personal website. All content here is free to use, but please remember to be respectful and avoid any misuse of the site. If youâ€™d like to get in touch, feel free to reach out via my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or connect with me on <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have technical questions or ideas to share. Wishing you all the best and a fantastic life ahead!</p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../../app.js"></script>
    </footer>
    <div id="pdf-spinner-overlay">
        <div class="spinner"></div>
    </div>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>