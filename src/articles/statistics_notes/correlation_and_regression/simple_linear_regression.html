<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Simple Linear Regression</title>
    <meta content="Simple linear regression is a statistical method used to model the relationship between a single dependent variable and one independent variable." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <div class="article-action-buttons"><button class="btn-suggest-edit" title="Suggest Edit">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M4 21h4l11-11-4-4L4 17v4z" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button><button class="btn-create-issue" title="Create Issue">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"></circle>
                        <line stroke="currentColor" stroke-width="2" x1="12" x2="12" y1="8" y2="12"></line>
                        <circle cx="12" cy="16" fill="currentColor" r="1"></circle>
                    </svg>
                </button><button class="btn-download" title="Download">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M12 5v14m0 0l-6-6m6 6l6-6" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button></div>
            <p style="text-align: right;"><i>Last modified: May 06, 2025</i></p>
            <p style="text-align: right;"><i>This article is written in: üá∫üá∏</i></p>
            <h2 id="simple-linear-regression">Simple Linear Regression</h2>
            <p>Simple linear regression is a statistical method used to model the relationship between a single dependent variable and one independent variable. It aims to find the best-fitting straight line through the data points, which can be used to predict the dependent variable based on the independent variable.</p>
            <p>In everyday terms, you can think of it as drawing a single straight line that best summarizes how one quantity (for instance, house price) tends to move whenever another quantity (such as house size) changes.</p>
            <h3 id="the-simple-linear-regression-model">The Simple Linear Regression Model</h3>
            <p>The mathematical representation of the simple linear regression model is:</p>
            <p>$$
                y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \quad i = 1, 2, \dots, n
                $$</p>
            <p>Where:</p>
            <ul>
                <li>$y_i$ is the $i $-th observation of the dependent variable.</li>
                <li>$x_i$ is the $i $-th observation of the independent variable.</li>
                <li>$\beta_0$ is the intercept (the expected value of $y$ when $x = 0$).</li>
                <li>$\beta_1$ is the slope (the average change in $y$ for a one-unit change in $x$).</li>
                <li>$\varepsilon_i$ is the error term, assumed to be independently and identically distributed with mean zero and constant variance $\sigma^2 $.</li>
            </ul>
            <p>Conceptually, $\varepsilon_i$ captures everything about $y_i$ that is <strong>not</strong> explained by $x_i$. If the model is perfect, $\varepsilon_i$ would always be exactly zero‚Äîsomething that almost never happens with real-world data.</p>
            <h4 id="assumptions-of-the-model">Assumptions of the Model</h4>
            <p>For the simple linear regression model to be valid, several key assumptions must be met:</p>
            <ol>
                <li><strong>Linearity</strong> indicates that the relationship between $x$ and $y$ is linear.</li>
                <li><strong>Independence</strong> means the residuals (errors) $\varepsilon_i$ are independent of one another.</li>
                <li><strong>Homoscedasticity</strong> suggests that the residuals have a constant variance ($\sigma^2$) across all values of $x$.</li>
                <li><strong>Normality</strong> assumes that the residuals follow a normal distribution.</li>
                <li>Finally, <strong>no measurement error in $x</strong> ensures that the independent variable $x$ is measured without error.</li>
            </ol>
            <p>A quick way to remember these assumptions is ‚ÄúLINE‚Äù plus ‚Äúno‚Äêerror-in-$x$‚Äù: <strong>L</strong>inearity, <strong>I</strong>ndependence, <strong>N</strong>ormality, <strong>E</strong>qual variance. Diagnostic plots such as residual-vs-fit, Q-Q plots, and scale-location plots are routinely used to check whether LINE holds in practice.</p>
            <h3 id="estimation-of-coefficients-using-the-least-squares-method">Estimation of Coefficients Using the Least Squares Method</h3>
            <p>The goal is to find estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ that minimize the sum of squared residuals (differences between observed and predicted values):</p>
            <p>$\mathrm{SSE} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$</p>
            <p>$\mathrm{SSE} = \sum_{i=1}^{n} \bigl(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i\bigr)^2$</p>
            <p>‚ÄúLeast squares‚Äù literally means ‚Äúmake the squares of the vertical distances from the data points to the line as small as possible.‚Äù Squaring the distances keeps positive and negative residuals from canceling and penalizes large misses more heavily than small ones, giving us a single objective function (SSE) to minimize.</p>
            <h3 id="calculating-the-slope-hat-beta-_1-and-intercept-hat-beta-_0-">Calculating the Slope ($\hat{\beta}_1$) and Intercept ($\hat{\beta}_0$)</h3>
            <p>The least squares estimates are calculated using the following formulas:</p>
            <h5>Slope ($\hat{\beta}_1$)</h5>
            <p>$$
                \hat{\beta}_1 = \frac{Sxy}{Sxx}
                $$</p>
            <p>$$
                S_{xy} = \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})
                $$</p>
            <p>$$
                S_{xx} = \sum_{i=1}^n (x_i - \bar{x})^2
                $$</p>
            <p>$$\hat{\beta}_1 = \frac{\mathrm{Cov}(x, y)}{\mathrm{Var}(x)}$$</p>
            <h5>Intercept ($\hat{\beta}_0$)</h5>
            <p>$$
                \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
                $$</p>
            <p>Where:</p>
            <ul>
                <li>$\bar{x} = \dfrac{1}{n} \sum_{i=1}^{n} x_i$ is the mean of the independent variable.</li>
                <li>$\bar{y} = \dfrac{1}{n} \sum_{i=1}^{n} y_i$ is the mean of the dependent variable.</li>
            </ul>
            <p>Notice how $\hat{\beta}_1$ is essentially the ratio of how $x$ and $y$ co-vary to how $x$ varies by itself. Intuitively, if $x$ and $y$ move together a lot compared with how much $x$ moves on its own, the slope must be steep; if they hardly move together, the slope must be shallow.</p>
            <h3 id="interpretation-of-the-coefficients">Interpretation of the Coefficients</h3>
            <ul>
                <li>The <strong>intercept ($\hat{\beta}_0$)</strong> represents the expected value of $y$ when $x = 0$, marking the point where the regression line intersects the $y$-axis.</li>
                <li>The <strong>slope ($\hat{\beta}_1$)</strong> indicates the average change in $y$ for each one-unit increase in $x$.</li>
            </ul>
            <p>In some applications (for example, predicting salary from years of experience), $x = 0$ may not make practical sense. In such cases, $\hat{\beta}_0$ is still needed mathematically‚Äîeven if we never literally interpret it‚Äîbecause it ‚Äúanchors‚Äù the regression line.</p>
            <h3 id="assessing-the-fit-of-the-model">Assessing the Fit of the Model</h3>
            <h4 id="total-sum-of-squares-sst-">Total Sum of Squares (SST)</h4>
            <p>Measures the total variability in the dependent variable:</p>
            <p>$$
                \text{SST} = \sum_{i=1}^{n} (y_i - \bar{y})^2
                $$</p>
            <p>SST is what the variability looks like <strong>before</strong> we apply any model. It is the baseline against which every model is judged.</p>
            <h4 id="regression-sum-of-squares-ssr-">Regression Sum of Squares (SSR)</h4>
            <p>Measures the variability explained by the regression:</p>
            <p>$$
                \text{SSR} = \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2
                $$</p>
            <p>SSR tells us how much of the baseline variability SST our straight line succeeds in accounting for.</p>
            <h4 id="sum-of-squared-errors-sse-">Sum of Squared Errors (SSE)</h4>
            <p>Measures the unexplained variability:</p>
            <p>$$
                \text{SSE} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
                $$</p>
            <p>By construction, $\text{SST} = \text{SSR} + \text{SSE}$. Whenever SSE is small relative to SST, our line is doing a good job.</p>
            <h4 id="coefficient-of-determination-r-2-">Coefficient of Determination ($R^2 $)</h4>
            <p>Indicates the proportion of variance in $y$ explained by $x$:</p>
            <p>$$
                R^2 = \frac{\text{SSR}}{\text{SST}} = 1 - \frac{\text{SSE}}{\text{SST}}
                $$</p>
            <p>An $R^2 $ value close to 1 suggests a strong linear relationship.</p>
            <p>For example, $R^2 = 0.87$ means ‚Äú87 % of the variability in the outcome is captured by our one-predictor model,‚Äù leaving 13 % to chance, omitted variables, or measurement noise.</p>
            <h3 id="hypothesis-testing">Hypothesis Testing</h3>
            <h4 id="testing-the-significance-of-the-slope">Testing the Significance of the Slope</h4>
            <ul>
                <li><strong>Null Hypothesis ($H_0$)</strong>: $\beta_1 = 0$ (no linear relationship)</li>
                <li><strong>Alternative Hypothesis ($H_a $)</strong>: $\beta_1 \neq 0$</li>
            </ul>
            <p><strong>Test Statistic</strong>:</p>
            <p>$$
                t = \frac{\hat{\beta}_1}{\text{SE}(\hat{\beta}_1)}
                $$</p>
            <p>Where:</p>
            <p>$$
                \mathrm{SE}(\hat\beta_1)
                = \frac{s}{\sqrt{\displaystyle\sum_{i=1}^n (x_i - \bar{x})^2}}
                $$</p>
            <p>And:</p>
            <p>$$
                s = \sqrt{\frac{\text{SSE}}{n - 2}}
                $$</p>
            <p>The test statistic follows a T-distribution with $n - 2$ degrees of freedom.</p>
            <p>If the absolute value of $t$ is large (or equivalently, the $p$-value is small), we reject $H_0$ and conclude that the slope is statistically different from zero‚Äîi.e., $x$ provides meaningful information about $y$. In practice, analysts often complement the $t$-test with a confidence interval for $\beta_1$ to see a plausible range of slopes.</p>
            <h3 id="example">Example</h3>
            <p>Suppose we have the following data on the number of hours studied ($x$) and the test scores ($y$):</p>
            <p>
            <table>
                <tr>
                    <td>Hours Studied ($x$)</td>
                    <td>Test Score ($y$)</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td>50</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td>60</td>
                </tr>
                <tr>
                    <td>6</td>
                    <td>70</td>
                </tr>
                <tr>
                    <td>8</td>
                    <td>80</td>
                </tr>
            </table>
            </p>
            <p>With only four paired observations this is a <strong>toy data set</strong>‚Äîperfect for hand calculations and for seeing each arithmetic step clearly, but obviously much smaller than what you would use in practice.</p>
            <h4 id="step-by-step-calculation">Step-by-Step Calculation</h4>
            <p><strong>I. Calculate the Means</strong></p>
            <p>$$
                \bar{x} = \frac{2 + 4 + 6 + 8}{4} = 5
                $$</p>
            <p>$$
                \bar{y} = \frac{50 + 60 + 70 + 80}{4} = 65
                $$</p>
            <p>The means serve as the ‚Äúbalance points‚Äù of the $x$ and $y$ distributions. Centering each variable around its mean (by subtracting $\bar{x}$ or $\bar{y}$) makes the subsequent algebra cleaner and is the core of why the least-squares formulas work.</p>
            <p><strong>II. Compute the Sum of Squares and Cross-Products</strong></p>
            <p>Create a table to organize calculations:</p>
            <p>
            <table>
                <tr>
                    <td>$x_i$</td>
                    <td>$y_i$</td>
                    <td>$x_i - \bar{x} $</td>
                    <td>$y_i - \bar{y} $</td>
                    <td>$(x_i - \bar{x})(y_i - \bar{y})$</td>
                    <td>$(x_i - \bar{x})^2 $</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td>50</td>
                    <td>-3</td>
                    <td>-15</td>
                    <td>45</td>
                    <td>9</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td>60</td>
                    <td>-1</td>
                    <td>-5</td>
                    <td>5</td>
                    <td>1</td>
                </tr>
                <tr>
                    <td>6</td>
                    <td>70</td>
                    <td>1</td>
                    <td>5</td>
                    <td>5</td>
                    <td>1</td>
                </tr>
                <tr>
                    <td>8</td>
                    <td>80</td>
                    <td>3</td>
                    <td>15</td>
                    <td>45</td>
                    <td>9</td>
                </tr>
                <tr>
                    <td><strong>Total</strong></td>
                    <td></td>
                    <td></td>
                    <td></td>
                    <td><strong>100</strong></td>
                    <td><strong>20</strong></td>
                </tr>
            </table>
            </p>
            <p>Here, the cross-product column captures how $x$ and $y$ move together. Large positive numbers mean both variables are either above or below their means at the same time; large negative numbers would indicate opposite movements.</p>
            <p><strong>Sum of Cross-Products</strong>:</p>
            <p>$$
                \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) = 100
                $$</p>
            <p><strong>Sum of Squares of $x$</strong>:</p>
            <p>$$
                \sum_{i=1}^{n} (x_i - \bar{x})^2 = 20
                $$</p>
            <p>Think of the first sum as the <strong>numerator</strong> of the slope (how $x$ co-varies with $y$) and the second as the <strong>denominator</strong> (how much $x$ varies by itself).</p>
            <p><strong>III. Calculate the Slope ($\hat{\beta}_1$)</strong></p>
            <p>$$
                \hat{\beta}_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2} = \frac{100}{20} = 5
                $$</p>
            <p>Interpreted literally, every additional hour of study is associated with a 5-point increase in the test score.</p>
            <p><strong>IV. Calculate the Intercept ($\hat{\beta}_0$)</strong></p>
            <p>$$
                \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x} = 65 - (5)(5) = 40
                $$</p>
            <p>Although ‚Äú0 hours studied‚Äù lies outside the observed data, the intercept mathematically pins down where the regression line crosses the $y$-axis.</p>
            <p><strong>V. Formulate the Regression Equation</strong></p>
            <p>$$
                \hat{y} = 40 + 5x
                $$</p>
            <p>This compact equation now lets us predict a test score for <strong>any</strong> study-hours value (within reason) by simple substitution.</p>
            <p><strong>VI. Predict the Test Scores and Calculate Residuals</strong></p>
            <p>
            <table>
                <tr>
                    <td>$x_i$</td>
                    <td>$y_i$</td>
                    <td>$\hat{y}_i = 40 + 5x_i$</td>
                    <td>Residuals ($y_i - \hat{y}_i$)</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td>50</td>
                    <td>50</td>
                    <td>0</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td>60</td>
                    <td>60</td>
                    <td>0</td>
                </tr>
                <tr>
                    <td>6</td>
                    <td>70</td>
                    <td>70</td>
                    <td>0</td>
                </tr>
                <tr>
                    <td>8</td>
                    <td>80</td>
                    <td>80</td>
                    <td>0</td>
                </tr>
            </table>
            </p>
            <p>Every residual is exactly zero‚Äîan outcome that almost never occurs with real-world data and signals a <strong>perfect linear fit</strong> for these four points.</p>
            <p><strong>VII. Calculate the Sum of Squares</strong></p>
            <p><strong>Total Sum of Squares (SST)</strong>:</p>
            <p>$$
                \text{SST} = \sum (y_i - \bar{y})^2 = (-15)^2 + (-5)^2 + 5^2 + 15^2 = 500
                $$</p>
            <p><strong>Sum of Squared Errors (SSE)</strong>:</p>
            <p>$$
                \text{SSE} = \sum (y_i - \hat{y}_i)^2 = 0
                $$</p>
            <p><strong>Regression Sum of Squares (SSR)</strong>:</p>
            <p>$$
                \text{SSR} = \text{SST} - \text{SSE} = 500 - 0 = 500
                $$</p>
            <p>Since the regression explains all variability ($SSE = 0$), SSR equals SST, satisfying the identity $SST = SSR + SSE$.</p>
            <p><strong>VIII. Compute the Coefficient of Determination ($R^2 $)</strong></p>
            <p>$$
                R^2 = \frac{\text{SSR}}{\text{SST}} = \frac{500}{500} = 1
                $$</p>
            <p>An $R^2 $ value of 1 indicates that the model perfectly explains the variability in the test scores.</p>
            <p><img alt="output(18)" src="https://github.com/user-attachments/assets/033767fc-d920-4f36-9827-5e8af81e4760" /></p>
            <p>In practice you should be cautious: a perfect $R^2$ can be a sign of overfitting, data entry errors, or‚Äîmore benignly‚Äîan extremely small, contrived sample like this one.</p>
            <p><strong>IX. Calculate the Standard Error of the Estimate ($s $)</strong></p>
            <p>$$
                s = \sqrt{\frac{\text{SSE}}{n - 2}} = \sqrt{\frac{0}{2}} = 0
                $$</p>
            <p>The standard error measures typical residual size. Here, with all residuals exactly zero, the estimate collapses to zero as well.</p>
            <p>Consequently,</p>
            <blockquote>
                <p><em>the estimated standard errors for both $\hat{\beta}_0$ and $\hat{\beta}_1$ are also zero, implying infinite precision‚Äîagain a mathematical quirk of a perfect fit, not a realistic outcome.</em></p>
            </blockquote>
            <p><strong>X. Hypothesis Testing (Optional)</strong></p>
            <p>In this case, the test statistic for $\hat{\beta}_1$ is undefined due to division by zero in the standard error. However, in practice, with more realistic data where $s &gt; 0$, you would perform a $t $-test to assess the significance of the slope.</p>
            <p>When $s$ is positive the $t$-test asks, ‚ÄúIs the observed slope big compared with the noise?‚Äù Here there is no noise, so the usual inferential machinery breaks down‚Äîwhich actually reinforces the lesson that <strong>perfect fits are the exception, not the rule</strong>.</p>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol><a href="#simple-linear-regression">Simple Linear Regression</a>
                <ol>
                    <li><a href="#the-simple-linear-regression-model">The Simple Linear Regression Model</a>
                        <ol>
                            <li><a href="#assumptions-of-the-model">Assumptions of the Model</a></li>
                        </ol>
                    </li>
                    <li><a href="#estimation-of-coefficients-using-the-least-squares-method">Estimation of Coefficients Using the Least Squares Method</a></li>
                    <li><a href="#calculating-the-slope-hat-beta-_1-and-intercept-hat-beta-_0-">Calculating the Slope ($\hat{\beta}_1$) and Intercept ($\hat{\beta}_0$)</a></li>
                    <li><a href="#interpretation-of-the-coefficients">Interpretation of the Coefficients</a></li>
                    <li><a href="#assessing-the-fit-of-the-model">Assessing the Fit of the Model</a>
                        <ol>
                            <li><a href="#total-sum-of-squares-sst-">Total Sum of Squares (SST)</a></li>
                            <li><a href="#regression-sum-of-squares-ssr-">Regression Sum of Squares (SSR)</a></li>
                            <li><a href="#sum-of-squared-errors-sse-">Sum of Squared Errors (SSE)</a></li>
                            <li><a href="#coefficient-of-determination-r-2-">Coefficient of Determination ($R^2 $)</a></li>
                        </ol>
                    </li>
                    <li><a href="#hypothesis-testing">Hypothesis Testing</a>
                        <ol>
                            <li><a href="#testing-the-significance-of-the-slope">Testing the Significance of the Slope</a></li>
                        </ol>
                    </li>
                    <li><a href="#example">Example</a>
                        <ol>
                            <li><a href="#step-by-step-calculation">Step-by-Step Calculation</a></li>
                        </ol>
                    </li>
                </ol>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li>Basic Concepts<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/axioms_of_probability.html">Axioms of Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/bayes_theorem.html">Bayes Theorem</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/bayesian_vs_frequentist.html">Bayesian vs Frequentist</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/conditional_probability.html">Conditional Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/descriptive_statistics.html">Descriptive Statistics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/geometric_probability.html">Geometric Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/introduction_to_probability.html">Introduction to Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/introduction_to_statistics.html">Introduction to Statistics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/probability_tree.html">Probability Tree</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/standard_error_and_lln.html">Standard Error and Lln</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/total_probability.html">Total Probability</a></li>
                        </ol>
                    </li>
                    <li>Probability Distributions<ol>
                            <li>Continuous Distributions<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/beta_distribution.html">Beta Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/chi_square_distribution.html">Chi Square Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/exponential_distribution.html">Exponential Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/f_distribution.html">F Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/gamma_distribution.html">Gamma Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/log_normal_distribution.html">Log Normal Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/normal_distribution.html">Normal Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/student_t_distribution.html">Student T Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/uniform_distribution.html">Uniform Distribution</a></li>
                                </ol>
                            </li>
                            <li>Discrete Distributions<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/binomial_distribution.html">Binomial Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/geometric_distribution.html">Geometric Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/negative_binomial_distribution.html">Negative Binomial Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/poisson_distribution.html">Poisson Distribution</a></li>
                                </ol>
                            </li>
                            <li>Intro<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/central_limit_theorem.html">Central Limit Theorem</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/introduction_to_distributions.html">Introduction to Distributions</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/normal_curve_and_z_score.html">Normal Curve and z Score</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/statistical_moments.html">Statistical Moments</a></li>
                                </ol>
                            </li>
                        </ol>
                    </li>
                    <li>Correlation and Regression<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/correlation.html">Correlation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/covariance.html">Covariance</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/logistic_regression.html">Logistic Regression</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/metrics.html">Metrics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/multiple_regression.html">Multiple Regression</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/simple_linear_regression.html">Simple Linear Regression</a></li>
                        </ol>
                    </li>
                    <li>Statistical Inference<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/analysis_of_categorical_data.html">Analysis of Categorical Data</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/analysis_of_variance.html">Analysis of Variance</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/confidence_intervals.html">Confidence Intervals</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/hypothesis_testing.html">Hypothesis Testing</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/multiple_comparisons.html">Multiple Comparisons</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/null_hypothesis.html">Null Hypothesis</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/resampling.html">Resampling</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/type_i_and_type_ii_errors.html">Type i and Type Ii Errors</a></li>
                        </ol>
                    </li>
                    <li>Time Series Analysis<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/arima_models.html">Arima Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autocorrelation_function.html">Autocorrelation Function</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autocovariance_function.html">Autocovariance Function</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autoregressive_models.html">Autoregressive Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/backward_shift_operator.html">Backward Shift Operator</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/difference_equations.html">Difference Equations</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/forecasting.html">Forecasting</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/invertibility.html">Invertibility</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/moving_average_models.html">Moving Average Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/random_walk.html">Random Walk</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/seasonality_and_trends.html">Seasonality and Trends</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/series.html">Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/stationarity.html">Stationarity</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/statistical_moments_and_time_series.html">Statistical Moments and Time Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/time_series.html">Time Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/time_series_modeling.html">Time Series Modeling</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/yule_walker_equations.html">Yule Walker Equations</a></li>
                        </ol>
                    </li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All content here is free to use,
                    but please remember to be respectful and avoid any misuse of the site.
                    If you‚Äôd like to get in touch, feel free to reach out via my
                    <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a>
                    or connect with me on
                    <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a>
                    if you have technical questions or ideas to share.
                    Wishing you all the best and a fantastic life ahead!
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                ¬© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../../app.js"></script>
    </footer>
    <div id="pdf-spinner-overlay">
        <div class="spinner"></div>
    </div>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>