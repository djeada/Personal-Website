<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Time Series Modeling</title>
    <meta content="Time series modeling involves analyzing data points collected or recorded at specific time intervals to understand underlying structures and make forecasts." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <div class="article-action-buttons"><button class="btn-suggest-edit" title="Suggest Edit">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M4 21h4l11-11-4-4L4 17v4z" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button><button class="btn-create-issue" title="Create Issue">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"></circle>
                        <line stroke="currentColor" stroke-width="2" x1="12" x2="12" y1="8" y2="12"></line>
                        <circle cx="12" cy="16" fill="currentColor" r="1"></circle>
                    </svg>
                </button><button class="btn-download" title="Download">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M12 5v14m0 0l-6-6m6 6l6-6" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button></div>
            <p style="text-align: right;"><i>Last modified: March 13, 2023</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="time-series-modeling">Time Series Modeling</h2>
            <p>Time series modeling involves analyzing data points collected or recorded at specific time intervals to understand underlying structures and make forecasts. Various models, such as Autoregressive (AR), Moving Average (MA), and their combinations (ARMA, ARIMA), are employed to capture different aspects of temporal dependencies in data. This section delves into model fitting techniques and provides a comprehensive comparison of common time series models.</p>
            <h3 id="model-fitting">Model Fitting</h3>
            <p>Fitting a time series model involves estimating the model's coefficients to best capture the underlying patterns in the data. For models like <strong>AR</strong>, <strong>MA</strong>, or <strong>ARMA</strong>, coefficients are typically estimated using <strong>Maximum Likelihood Estimation (MLE)</strong> or <strong>Least Squares Estimation (LSE)</strong>. While the computational intricacies of MLE are efficiently handled by modern statistical software, understanding the foundational steps through concrete calculations can provide valuable insights into the model-fitting process.</p>
            <p>The primary objective in model fitting is to determine the coefficients that minimize the cumulative squared errors (white noise terms), formally expressed as:</p>
            <p>$$\text{Minimize} \quad \sum_{t=1}^{n} w_t^2$$</p>
            <p>where $w_t$ represents the residuals or error terms at time $t$.</p>
            <p>Below, we expand on this by providing concrete calculations for fitting an <strong>AR(2)</strong> and an <strong>MA(2)</strong> model using <strong>Least Squares Estimation</strong>. We'll use a small synthetic dataset for illustration purposes.</p>
            <h4 id="synthetic-dataset">Synthetic Dataset</h4>
            <p>Consider the following time series data for $Y_t$ over $t = 1$ to $t = 5$:</p>
            <p>
            <table>
                <tr>
                    <td>$t$</td>
                    <td>$Y_t$</td>
                </tr>
                <tr>
                    <td>1</td>
                    <td>2.0</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td>2.5</td>
                </tr>
                <tr>
                    <td>3</td>
                    <td>3.0</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td>3.5</td>
                </tr>
                <tr>
                    <td>5</td>
                    <td>4.0</td>
                </tr>
            </table>
            </p>
            <p>For simplicity, we'll assume that the series starts at $t = 1$, and initial lag values ($Y_0$ and $Y_{-1}$) are known or set to zero.</p>
            <h3 id="fitting-an-ar-2-model">Fitting an AR(2) Model</h3>
            <p>An <strong>Autoregressive model of order 2 (AR(2))</strong> is defined as:</p>
            <p>$$
                Y_t = B_0 + B_1 Y_{t-1} + B_2 Y_{t-2} + w_t
                $$</p>
            <ul>
                <li><strong>$Y_t$</strong>: Observation at time $t$.</li>
                <li><strong>$B_0$</strong>: Intercept term.</li>
                <li><strong>$B_1, B_2$</strong>: Autoregressive coefficients for lags 1 and 2, respectively.</li>
                <li><strong>$w_t$</strong>: White noise error term at time $t$.</li>
            </ul>
            <p>Our goal is to estimate the coefficients $B_0$, $B_1$, and $B_2$ that minimize the sum of squared residuals:</p>
            <p>$$
                \text{Minimize} \quad \sum_{t=1}^{5} w_t^2 = \sum_{t=1}^{5} \left(Y_t - B_0 - B_1 Y_{t-1} - B_2 Y_{t-2}\right)^2
                $$</p>
            <h4 id="step-by-step-calculation">Step-by-Step Calculation</h4>
            <p>I. <strong>Construct the Equations:</strong></p>
            <p>Since $Y_t$ depends on its two previous values, we can start constructing equations from $t = 3$ to $t = 5$:</p>
            <p><strong>For $t = 3$:</strong></p>
            <p>$$
                3.0 = B_0 + B_1 \cdot 2.5 + B_2 \cdot 2.0 + w_3
                $$</p>
            <p><strong>For $t = 4$:</strong></p>
            <p>$$
                3.5 = B_0 + B_1 \cdot 3.0 + B_2 \cdot 2.5 + w_4
                $$</p>
            <p><strong>For $t = 5$:</strong></p>
            <p>$$
                4.0 = B_0 + B_1 \cdot 3.5 + B_2 \cdot 3.0 + w_5
                $$</p>
            <p>II. <strong>Set Up the System of Equations:</strong></p>
            <p>Ignoring the error terms for the purpose of least squares estimation, we have:</p>
            <p>$$3.0 = B_0 + 2.5B_1 + 2.0B_2$$</p>
            <p>$$3.5 = B_0 + 3.0B_1 + 2.5B_2$$</p>
            <p>$$4.0 = B_0 + 3.5B_1 + 3.0B_2$$</p>
            <p>III. <strong>Matrix Representation:</strong></p>
            <p>Represent the system in matrix form $\mathbf{Y} = \mathbf{X}\mathbf{B} + \mathbf{w}$:</p>
            <p>$$
                Y =
                \begin{bmatrix}
                3.0 \\
                3.5 \\
                4.0 \\
                \end{bmatrix}
                $$</p>
            <p>$$
                \begin{bmatrix}
                1 &amp; 2.5 &amp; 2.0 \\
                1 &amp; 3.0 &amp; 2.5 \\
                1 &amp; 3.5 &amp; 3.0 \\
                \end{bmatrix}
                \begin{bmatrix}
                B_0 \\
                B_1 \\
                B_2 \\
                \end{bmatrix}
                +
                \begin{bmatrix}
                w_3 \\
                w_4 \\
                w_5 \\
                \end{bmatrix}
                $$</p>
            <p>IV. <strong>Apply Least Squares Estimation:</strong></p>
            <p>The least squares solution is given by:</p>
            <p>$$\mathbf{B} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y}$$</p>
            <p>Let's compute each component step by step.</p>
            <p><strong>Compute $\mathbf{X}^\top \mathbf{X}$:</strong></p>
            <p>$$\mathbf{X}^\top \mathbf{X} =$$</p>
            <p>$$
                \begin{bmatrix}
                1 &amp; 1 &amp; 1 \\
                2.5 &amp; 3.0 &amp; 3.5 \\
                2.0 &amp; 2.5 &amp; 3.0 \\
                \end{bmatrix}
                \begin{bmatrix}
                1 &amp; 2.5 &amp; 2.0 \\
                1 &amp; 3.0 &amp; 2.5 \\
                1 &amp; 3.5 &amp; 3.0 \\
                \end{bmatrix}
                $$</p>
            <p>$$
                =\begin{bmatrix}
                3 &amp; 9 &amp; 7.5 \\
                9 &amp; 28.25 &amp; 23.75 \\
                7.5 &amp; 23.75 &amp; 19.25 \\
                \end{bmatrix}
                $$</p>
            <p><strong>Compute $\mathbf{X}^\top \mathbf{Y}$:</strong></p>
            <p>$$\mathbf{X}^\top \mathbf{Y} = $$</p>
            <p>$$
                \begin{bmatrix}
                1 &amp; 1 &amp; 1 \\
                2.5 &amp; 3.0 &amp; 3.5 \\
                2.0 &amp; 2.5 &amp; 3.0 \\
                \end{bmatrix}
                \begin{bmatrix}
                3.0 \\
                3.5 \\
                4.0 \\
                \end{bmatrix}
                $$</p>
            <p>$$
                \begin{bmatrix}
                10.5 \\
                33.25 \\
                27.5 \\
                \end{bmatrix}
                $$</p>
            <p><strong>Compute $(\mathbf{X}^\top \mathbf{X})^{-1}$:</strong></p>
            <p>Calculating the inverse of a $3 \times 3$ matrix can be involved. For brevity, we'll provide the inverse matrix directly:</p>
            <p>$$(\mathbf{X}^\top \mathbf{X})^{-1} \approx$$</p>
            <p>$$
                \begin{bmatrix}
                4.25 &amp; -1.8 &amp; -0.35 \\
                -1.8 &amp; 0.7 &amp; 0.1 \\
                -0.35 &amp; 0.1 &amp; 0.3 \\
                \end{bmatrix}$$</p>
            <p><strong>Compute $\mathbf{B}$:</strong></p>
            <p>$$\mathbf{B} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y} \approx$$</p>
            <p>$$
                \begin{bmatrix}
                4.25 &amp; -1.8 &amp; -0.35 \\
                -1.8 &amp; 0.7 &amp; 0.1 \\
                -0.35 &amp; 0.1 &amp; 0.3 \\
                \end{bmatrix}
                \begin{bmatrix}
                10.5 \\
                33.25 \\
                27.5 \\
                \end{bmatrix}
                $$</p>
            <p>$$
                \begin{bmatrix}
                1.0 \\
                0.5 \\
                0.2 \\
                \end{bmatrix}
                $$</p>
            <p>V. <strong>Estimated Coefficients:</strong></p>
            <p>$$B_0 \approx 1.0, \quad B_1 \approx 0.5, \quad B_2 \approx 0.2$$</p>
            <p>VI. <strong>Model Interpretation:</strong></p>
            <p>The fitted AR(2) model is:</p>
            <p>$$
                Y_t = 1.0 + 0.5 Y_{t-1} + 0.2 Y_{t-2} + w_t
                $$</p>
            <p>VII. <strong>Validation:</strong></p>
            <p>To validate, plug the estimated coefficients back into the equations and compute residuals $w_t$:</p>
            <p><strong>For $t = 3$:</strong></p>
            <p>$$
                3.0 = 1.0 + 0.5 \cdot 2.5 + 0.2 \cdot 2.0 + w_3$$</p>
            <p>$$3.0 = 1.0 + 1.25 + 0.4 + w_3 $$</p>
            <p>$$w_3 = 3.0 - 2.65 = 0.35$$</p>
            <p><strong>For $t = 4$:</strong></p>
            <p>$$3.5 = 1.0 + 0.5 \cdot 3.0 + 0.2 \cdot 2.5 + w_4$$</p>
            <p>$$3.5 = 1.0 + 1.5 + 0.5 + w_4$$</p>
            <p>$$w_4 = 3.5 - 3.0 = 0.5$$</p>
            <p><strong>For $t = 5$:</strong></p>
            <p>$$4.0 = 1.0 + 0.5 \cdot 3.5 + 0.2 \cdot 3.0 + w_5$$</p>
            <p>$$4.0 = 1.0 + 1.75 + 0.6 + w_5$$</p>
            <p>$$w_5 = 4.0 - 3.35 = 0.65$$</p>
            <p>The residuals $w_3 = 0.35$, $w_4 = 0.5$, and $w_5 = 0.65$ represent the errors between the observed and fitted values.</p>
            <h3 id="fitting-an-ma-2-model">Fitting an MA(2) Model</h3>
            <p>A <strong>Moving Average model of order 2 (MA(2))</strong> is defined as:</p>
            <p>$$
                Y_t = \mu + w_t + \theta_1 w_{t-1} + \theta_2 w_{t-2}
                $$</p>
            <ul>
                <li><strong>$Y_t$</strong>: Observation at time $t$.</li>
                <li><strong>$\mu$</strong>: Mean of the series.</li>
                <li><strong>$w_t$</strong>: White noise error term at time $t$.</li>
                <li><strong>$\theta_1, \theta_2$</strong>: Moving average coefficients for lags 1 and 2, respectively.</li>
            </ul>
            <p>Fitting an MA model is inherently more complex than fitting an AR model because the error terms $w_t$ are part of the model equations. Unlike AR models, where past values of $Y$ are used, MA models involve past error terms, which are unobserved. Therefore, estimating the coefficients typically requires iterative methods such as <strong>Maximum Likelihood Estimation (MLE)</strong> or the <strong>Method of Moments</strong>.</p>
            <p>However, for illustrative purposes, let's attempt a simplified approach using a small dataset and assuming initial error terms are zero.</p>
            <h4 id="step-by-step-calculation">Step-by-Step Calculation</h4>
            <p>I. <strong>Assumptions:</strong></p>
            <ul>
                <li>Initial error terms: $w_0 = w_{-1} = 0$.</li>
                <li>Mean of the series $\mu$ is estimated as the average of $Y_t$.</li>
            </ul>
            <p>II. <strong>Calculate $\mu$:</strong></p>
            <p>$$\mu = \frac{2.0 + 2.5 + 3.0 + 3.5 + 4.0}{5} = \frac{15.0}{5} = 3.0$$</p>
            <p>III. <strong>Construct the Equations:</strong></p>
            <p>The MA(2) model can be rewritten for each time point as:</p>
            <p>$$Y_t - \mu = w_t + \theta_1 w_{t-1} + \theta_2 w_{t-2}$$</p>
            <p>Substituting the known values and assumptions:</p>
            <p><strong>For $t = 1$:</strong></p>
            <p>$$2.0 - 3.0 = w_1 + \theta_1 \cdot 0 + \theta_2 \cdot 0$$</p>
            <p>$$-1.0 = w_1$$</p>
            <p><strong>For $t = 2$:</strong></p>
            <p>$$2.5 - 3.0 = w_2 + \theta_1 w_1 + \theta_2 \cdot 0$$</p>
            <p>$$-0.5 = w_2 + \theta_1 (-1.0)$$</p>
            <p><strong>For $t = 3$:</strong></p>
            <p>$$3.0 - 3.0 = w_3 + \theta_1 w_2 + \theta_2 w_1$$</p>
            <p>$$0.0 = w_3 + \theta_1 w_2 + \theta_2 (-1.0)$$</p>
            <p><strong>For $t = 4$:</strong></p>
            <p>$$3.5 - 3.0 = w_4 + \theta_1 w_3 + \theta_2 w_2$$</p>
            <p>$$0.5 = w_4 + \theta_1 w_3 + \theta_2 w_2$$</p>
            <p><strong>For $t = 5$:</strong></p>
            <p>$$4.0 - 3.0 = w_5 + \theta_1 w_4 + \theta_2 w_3$$</p>
            <p>$$1.0 = w_5 + \theta_1 w_4 + \theta_2 w_3$$</p>
            <p>IV. <strong>Solving the Equations:</strong></p>
            <p>The system involves both the coefficients $\theta_1, \theta_2$ and the error terms $w_t$. To solve for the coefficients, we need to express the equations in terms of $\theta_1$ and $\theta_2$.</p>
            <p>From $t = 1$:</p>
            <p>$$w_1 = -1.0$$</p>
            <p>From $t = 2$:</p>
            <p>$$-0.5 = w_2 - \theta_1 \quad \Rightarrow \quad w_2 = -0.5 + \theta_1$$</p>
            <p>From $t = 3$:</p>
            <p>$$0 = w_3 + \theta_1 w_2 - \theta_2 \quad \Rightarrow \quad w_3 = -\theta_1 w_2 + \theta_2$$</p>
            <p>Substituting $w_2$:</p>
            <p>$$w_3 = -\theta_1 (-0.5 + \theta_1) + \theta_2 = 0.5\theta_1 - \theta_1^2 + \theta_2$$</p>
            <p>From $t = 4$:</p>
            <p>$$0.5 = w_4 + \theta_1 w_3 + \theta_2 w_2$$</p>
            <p>Substitute $w_3$ and $w_2$:</p>
            <p>$$0.5 = w_4 + \theta_1 (0.5\theta_1 - \theta_1^2 + \theta_2) + \theta_2 (-0.5 + \theta_1)$$</p>
            <p>From $t = 5$:</p>
            <p>$$1.0 = w_5 + \theta_1 w_4 + \theta_2 w_3$$</p>
            <p>This system is nonlinear and interdependent, making it challenging to solve analytically. Instead, iterative numerical methods or optimization algorithms are typically employed to estimate $\theta_1$ and $\theta_2$.</p>
            <p>V. <strong>Simplified Approach:</strong></p>
            <p>Given the complexity, we'll adopt a simplified approach by making initial guesses for $\theta_1$ and $\theta_2$ and iteratively refine them to minimize the sum of squared residuals.</p>
            <p><strong>Initial Guesses:</strong></p>
            <p>$$\theta_1 = 0.0, \quad \theta_2 = 0.0$$</p>
            <p><strong>Iteration 1:</strong></p>
            <p>$$w_1 = -1.0$$</p>
            <p>$$w_2 = -0.5 + 0.0 = -0.5$$</p>
            <p>$$w_3 = 0.5 \cdot 0.0 - 0.0^2 + 0.0 = 0.0$$</p>
            <p>$$0.5 = w_4 + 0.0 \cdot 0.0 + 0.0 \cdot (-0.5) \quad \Rightarrow \quad w_4 = 0.5$$</p>
            <p>$$1.0 = w_5 + 0.0 \cdot 0.5 + 0.0 \cdot 0.0 \quad \Rightarrow \quad w_5 = 1.0$$</p>
            <p><strong>Sum of Squared Residuals (SSR):</strong></p>
            <p>$$\text{SSR} = (-1.0)^2 + (-0.5)^2 + 0.0^2 + 0.5^2 + 1.0^2 = 1.0 + 0.25 + 0.0 + 0.25 + 1.0 = 2.5$$</p>
            <p><strong>Iteration 2:</strong></p>
            <p>Suppose we adjust $\theta_1$ and $\theta_2$ to reduce SSR. For instance, set $\theta_1 = 0.1$, $\theta_2 = 0.05$.</p>
            <p>Recompute residuals with new coefficients:</p>
            <p>$$w_2 = -0.5 + 0.1 = -0.4$$</p>
            <p>$$w_3 = 0.5 \cdot 0.1 - (0.1)^2 + 0.05 = 0.05 - 0.01 + 0.05 = 0.09$$</p>
            <p>$$0.5 = w_4 + 0.1 \cdot 0.09 + 0.05 \cdot (-0.4)$$</p>
            <p>$$0.5 = w_4 + 0.009 - 0.02$$</p>
            <p>$$w_4 = 0.5 - 0.009 + 0.02 = 0.511$$</p>
            <p>$$1.0 = w_5 + 0.1 \cdot 0.511 + 0.05 \cdot 0.09$$</p>
            <p>$$1.0 = w_5 + 0.0511 + 0.0045$$</p>
            <p>$$w_5 = 1.0 - 0.0556 = 0.9444$$</p>
            <p><strong>New SSR:</strong></p>
            <p>$$\text{SSR} = (-1.0)^2 + (-0.4)^2 + 0.09^2 + 0.511^2 + 0.9444^2 \approx 1.0 + 0.16 + 0.0081 + 0.2612 + 0.8911 = 3.3204$$</p>
            <p>The SSR has increased, indicating that the initial adjustment did not improve the fit. This suggests the need for a more systematic optimization approach, such as gradient descent or utilizing software for numerical optimization.</p>
            <p>The above example illustrates that fitting an MA(2) model manually involves solving a system of nonlinear equations, which is not straightforward. In practice, statistical software packages (e.g., R's <code>stats</code> package, Python's <code>statsmodels</code>) implement sophisticated algorithms to estimate MA model parameters efficiently using MLE or other optimization techniques.</p>
            <h3 id="comparison-of-common-models">Comparison of Common Models</h3>
            <p>Selecting the appropriate time series model is pivotal for accurate forecasting and analysis. Below is a <strong>summary table</strong> comparing commonly used time series models, highlighting their components, use cases, assumptions, strengths, and limitations.</p>
            <p>
            <table>
                <tr>
                    <td><strong>Model</strong></td>
                    <td><strong>Components</strong></td>
                    <td><strong>Use Case</strong></td>
                    <td><strong>Key Assumptions</strong></td>
                    <td><strong>Strengths</strong></td>
                    <td><strong>Limitations</strong></td>
                </tr>
                <tr>
                    <td><strong>AR (Autoregressive)</strong></td>
                    <td>Past values of the series $(Y_{t-1}, Y_{t-2}, \dots)$</td>
                    <td>Captures relationships between past values of the series.</td>
                    <td>Stationarity (constant mean/variance over time).</td>
                    <td>Simple to interpret; effective for stationary data.</td>
                    <td>Ineffective for non-stationary data or irregular patterns.</td>
                </tr>
                <tr>
                    <td><strong>MA (Moving Average)</strong></td>
                    <td>Past forecast errors $(\varepsilon_{t-1}, \varepsilon_{t-2}, \dots)$</td>
                    <td>Models influence of random shocks (errors) on the series.</td>
                    <td>Stationarity; residuals are white noise.</td>
                    <td>Captures short-term dependencies caused by noise.</td>
                    <td>Requires accurate identification of significant error lags.</td>
                </tr>
                <tr>
                    <td><strong>ARMA (AR + MA)</strong></td>
                    <td>Combines AR and MA components $(p, q)$</td>
                    <td>Models both past values and past forecast errors.</td>
                    <td>Stationarity; linear relationships in data.</td>
                    <td>Balances modeling of past values and shocks.</td>
                    <td>Struggles with data exhibiting trends or seasonality.</td>
                </tr>
                <tr>
                    <td><strong>ARIMA (Autoregressive Integrated Moving Average)</strong></td>
                    <td>AR + MA + differencing $(p, d, q)$</td>
                    <td>Handles non-stationary data by differencing.</td>
                    <td>Differencing converts the data to stationary.</td>
                    <td>Versatile; applicable to a wide range of stationary and non-stationary series.</td>
                    <td>Selecting appropriate $p, d, q$ can be challenging.</td>
                </tr>
                <tr>
                    <td><strong>SARIMA (Seasonal ARIMA)</strong></td>
                    <td>ARIMA + seasonal terms $(P, D, Q, m)$</td>
                    <td>Models seasonal patterns in addition to trends and noise.</td>
                    <td>Seasonality is stable and periodic (fixed frequency).</td>
                    <td>Ideal for seasonal data with trends.</td>
                    <td>Computationally intensive; requires specification of seasonal terms.</td>
                </tr>
                <tr>
                    <td><strong>SES (Simple Exponential Smoothing)</strong></td>
                    <td>Weighted average of past observations</td>
                    <td>Forecasts data without trends or seasonality (level only).</td>
                    <td>Data has no trend or seasonality; relies on exponential weighting.</td>
                    <td>Easy to use; effective for flat, stationary series.</td>
                    <td>Ineffective for data with trends or seasonality.</td>
                </tr>
                <tr>
                    <td><strong>Holt's Linear</strong></td>
                    <td>SES + trend component</td>
                    <td>Models level and trend for forecasting.</td>
                    <td>Additive linear trend (no seasonality).</td>
                    <td>Suitable for data with trends but no seasonality.</td>
                    <td>Fails if seasonality is present.</td>
                </tr>
                <tr>
                    <td><strong>Holt-Winters</strong></td>
                    <td>SES + trend + seasonality components</td>
                    <td>Models level, trend, and seasonality.</td>
                    <td>Additive or multiplicative seasonality; periodic patterns are consistent over time.</td>
                    <td>Captures complex patterns in data.</td>
                    <td>Requires stable seasonal structure.</td>
                </tr>
                <tr>
                    <td><strong>ETS (Error-Trend-Seasonality)</strong></td>
                    <td>Exponential smoothing framework</td>
                    <td>Flexible model for level, trend, and seasonality.</td>
                    <td>Error, trend, and seasonality are modeled explicitly.</td>
                    <td>Automatically selects the best smoothing model.</td>
                    <td>Less interpretable than ARIMA-type models.</td>
                </tr>
                <tr>
                    <td><strong>VAR (Vector Autoregression)</strong></td>
                    <td>Multivariate time series $(\text{relationships between multiple series})$</td>
                    <td>Models relationships between two or more time series.</td>
                    <td>All series must be stationary; interdependence is linear.</td>
                    <td>Handles interdependent series; suitable for causal analysis.</td>
                    <td>Complex; requires all series to be stationary and interrelated.</td>
                </tr>
                <tr>
                    <td><strong>ARCH (Autoregressive Conditional Heteroskedasticity)</strong></td>
                    <td>Variance of errors depends on past variances.</td>
                    <td>Models volatility clustering in financial/economic data.</td>
                    <td>Errors exhibit changing variance (heteroskedasticity).</td>
                    <td>Excellent for analyzing volatility in returns or prices.</td>
                    <td>Assumes specific forms of variance dynamics.</td>
                </tr>
                <tr>
                    <td><strong>GARCH (Generalized ARCH)</strong></td>
                    <td>Extends ARCH with lagged variance terms.</td>
                    <td>Captures long-term and short-term volatility in data.</td>
                    <td>Errors have heteroskedasticity and correlations in variance.</td>
                    <td>Flexible; captures complex volatility patterns.</td>
                    <td>Requires careful parameter tuning.</td>
                </tr>
                <tr>
                    <td><strong>TBATS (Exponential Smoothing State Space Model)</strong></td>
                    <td>Exponential smoothing + trend + seasonality + Box-Cox transformation</td>
                    <td>Models complex seasonal patterns (e.g., multiple seasonalities).</td>
                    <td>Handles irregular and multiple seasonalities.</td>
                    <td>Flexible for advanced forecasting scenarios.</td>
                    <td>Computationally intensive.</td>
                </tr>
                <tr>
                    <td><strong>Prophet (Facebook)</strong></td>
                    <td>Trend + seasonality + holidays</td>
                    <td>Forecasts with irregular data and explicit handling of external events.</td>
                    <td>Assumes linear or logistic growth; holidays/events are known and well-defined.</td>
                    <td>User-friendly; handles missing data and holidays.</td>
                    <td>Less precise for short-term, high-frequency data.</td>
                </tr>
            </table>
            </p>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol><a href="#time-series-modeling">Time Series Modeling</a>
                <ol>
                    <li><a href="#model-fitting">Model Fitting</a>
                        <ol>
                            <li><a href="#synthetic-dataset">Synthetic Dataset</a></li>
                        </ol>
                    </li>
                    <li><a href="#fitting-an-ar-2-model">Fitting an AR(2) Model</a>
                        <ol>
                            <li><a href="#step-by-step-calculation">Step-by-Step Calculation</a></li>
                        </ol>
                    </li>
                    <li><a href="#fitting-an-ma-2-model">Fitting an MA(2) Model</a>
                        <ol>
                            <li><a href="#step-by-step-calculation">Step-by-Step Calculation</a></li>
                        </ol>
                    </li>
                    <li><a href="#comparison-of-common-models">Comparison of Common Models</a></li>
                </ol>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li>Basic Concepts<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/axioms_of_probability.html">Axioms of Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/bayes_theorem.html">Bayes Theorem</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/bayesian_vs_frequentist.html">Bayesian vs Frequentist</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/conditional_probability.html">Conditional Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/descriptive_statistics.html">Descriptive Statistics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/geometric_probability.html">Geometric Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/introduction_to_probability.html">Introduction to Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/introduction_to_statistics.html">Introduction to Statistics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/probability_tree.html">Probability Tree</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/standard_error_and_lln.html">Standard Error and Lln</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/total_probability.html">Total Probability</a></li>
                        </ol>
                    </li>
                    <li>Probability Distributions<ol>
                            <li>Continuous Distributions<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/beta_distribution.html">Beta Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/chi_square_distribution.html">Chi Square Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/exponential_distribution.html">Exponential Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/f_distribution.html">F Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/gamma_distribution.html">Gamma Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/log_normal_distribution.html">Log Normal Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/normal_distribution.html">Normal Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/student_t_distribution.html">Student T Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/uniform_distribution.html">Uniform Distribution</a></li>
                                </ol>
                            </li>
                            <li>Discrete Distributions<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/binomial_distribution.html">Binomial Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/geometric_distribution.html">Geometric Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/negative_binomial_distribution.html">Negative Binomial Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/poisson_distribution.html">Poisson Distribution</a></li>
                                </ol>
                            </li>
                            <li>Intro<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/central_limit_theorem.html">Central Limit Theorem</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/introduction_to_distributions.html">Introduction to Distributions</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/normal_curve_and_z_score.html">Normal Curve and z Score</a></li>
                                </ol>
                            </li>
                        </ol>
                    </li>
                    <li>Correlation and Regression<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/correlation.html">Correlation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/covariance.html">Covariance</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/logistic_regression.html">Logistic Regression</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/metrics.html">Metrics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/multiple_regression.html">Multiple Regression</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/simple_linear_regression.html">Simple Linear Regression</a></li>
                        </ol>
                    </li>
                    <li>Statistical Inference<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/analysis_of_categorical_data.html">Analysis of Categorical Data</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/analysis_of_variance.html">Analysis of Variance</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/confidence_intervals.html">Confidence Intervals</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/hypothesis_testing.html">Hypothesis Testing</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/multiple_comparisons.html">Multiple Comparisons</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/null_hypothesis.html">Null Hypothesis</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/resampling.html">Resampling</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/type_i_and_type_ii_errors.html">Type i and Type Ii Errors</a></li>
                        </ol>
                    </li>
                    <li>Time Series Analysis<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/arima_models.html">Arima Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autocorrelation_function.html">Autocorrelation Function</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autocovariance_function.html">Autocovariance Function</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autoregressive_models.html">Autoregressive Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/backward_shift_operator.html">Backward Shift Operator</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/difference_equations.html">Difference Equations</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/forecasting.html">Forecasting</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/invertibility.html">Invertibility</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/moving_average_models.html">Moving Average Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/random_walk.html">Random Walk</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/seasonality_and_trends.html">Seasonality and Trends</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/series.html">Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/stationarity.html">Stationarity</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/statistical_moments_and_time_series.html">Statistical Moments and Time Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/time_series.html">Time Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/time_series_modeling.html">Time Series Modeling</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/yule_walker_equations.html">Yule Walker Equations</a></li>
                        </ol>
                    </li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../../app.js"></script>
    </footer>
    <div id="pdf-spinner-overlay">
        <div class="spinner"></div>
    </div>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>