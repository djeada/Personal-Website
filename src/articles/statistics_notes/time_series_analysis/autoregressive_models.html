<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Autoregressive (AR) Models in Time Series Analysis</title>
    <meta content="Autoregressive (AR) models are fundamental tools in time series analysis, used to describe and forecast time-dependent data." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>Last modified: September 16, 2024</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="autoregressive-ar-models-in-time-series-analysis">Autoregressive (AR) Models in Time Series Analysis</h2>
            <p>Autoregressive (AR) models are fundamental tools in time series analysis, used to describe and forecast time-dependent data. An AR model predicts future values based on a linear combination of past observations. The order of an AR model, denoted as ( p ), indicates how many lagged past values are used.</p>
            <h3 id="mathematical-definition-of-ar-models">Mathematical Definition of AR Models</h3>
            <p>An <strong>Autoregressive model of order ( p )</strong>, denoted as AR(( p )), is defined by the equation:</p>
            <p>[
                X_t = c + \sum_{i=1}^{p} \phi_i X_{t-i} + \epsilon_t
                ]</p>
            <p>where:</p>
            <ul>
                <li>( X_t ): The value of the time series at time ( t ).</li>
                <li>( c ): A constant term (intercept).</li>
                <li>( \phi_i ): Autoregressive coefficients for lag ( i ).</li>
                <li>( X_{t-i} ): The value of the time series at lag ( i ).</li>
                <li>( \epsilon_t ): White noise error term at time ( t ), assumed to be independently and identically distributed with mean zero and constant variance ( \sigma^2 ).</li>
            </ul>
            <h3 id="properties-of-ar-models">Properties of AR Models</h3>
            <ul>
                <li><strong>Stationarity</strong>: An AR model is stationary if the roots of its characteristic equation lie outside the unit circle in the complex plane. For an AR(1) model, stationarity requires (|\phi_1| &lt; 1).</li>
                <li><strong>Autocorrelation Function (ACF)</strong>: The ACF of an AR model declines exponentially or in a damped sinusoidal pattern.</li>
                <li><strong>Partial Autocorrelation Function (PACF)</strong>: The PACF of an AR(( p )) model cuts off after lag ( p ).</li>
            </ul>
            <h3 id="estimation-of-parameters">Estimation of Parameters</h3>
            <p>Parameters ( c ) and ( \phi_i ) can be estimated using methods such as:</p>
            <ul>
                <li><strong>Least Squares Estimation</strong>: Minimizes the sum of squared residuals.</li>
                <li><strong>Yule-Walker Equations</strong>: Utilizes autocorrelations to solve for parameters.</li>
                <li><strong>Maximum Likelihood Estimation (MLE)</strong>: Based on the likelihood function of the observed data.</li>
            </ul>
            <h3 id="model-selection">Model Selection</h3>
            <p>Selecting the appropriate order ( p ) is crucial. Common criteria include:</p>
            <ul>
                <li><strong>Akaike Information Criterion (AIC)</strong>:</li>
            </ul>
            <p>[
                \text{AIC} = -2 \ln(L) + 2k
                ]</p>
            <ul>
                <li><strong>Bayesian Information Criterion (BIC)</strong>:</li>
            </ul>
            <p>[
                \text{BIC} = -2 \ln(L) + k \ln(n)
                ]</p>
            <p>where:</p>
            <ul>
                <li>( L ): The maximized value of the likelihood function.</li>
                <li>( k ): The number of estimated parameters.</li>
                <li>( n ): The number of observations.</li>
            </ul>
            <p>Lower values of AIC or BIC indicate a better model, balancing goodness of fit and model complexity.</p>
            <hr />
            <h2 id="example-ar-2-model-in-time-series-forecasting">Example: AR(2) Model in Time Series Forecasting</h2>
            <p>Consider an AR model of order 2, denoted as AR(2). This model assumes that the value at a particular time point is a linear function of the two preceding values, plus some error term.</p>
            <h3 id="ar-2-model-equation">AR(2) Model Equation</h3>
            <p>The AR(2) model is given by:</p>
            <p>[
                X_t = c + \phi_1 X_{t-1} + \phi_2 X_{t-2} + \epsilon_t
                ]</p>
            <p>Suppose we have:</p>
            <ul>
                <li>( c = 3 )</li>
                <li>( \phi_1 = 0.6 )</li>
                <li>( \phi_2 = -0.2 )</li>
                <li>Past observations: ( X_{t-1} = 10 ), ( X_{t-2} = 5 )</li>
            </ul>
            <h4 id="calculation">Calculation</h4>
            <p>Plugging the values into the AR(2) equation:</p>
            <p>[
                \begin{align<em>}
                    X_t &amp;= 3 + 0.6 \times 10 - 0.2 \times 5 + \epsilon_t \
                    &amp;= 3 + 6 - 1 + \epsilon_t \
                    &amp;= 8 + \epsilon_t
                    \end{align</em>}
                ]</p>
            <h4 id="interpretation">Interpretation</h4>
            <ul>
                <li><strong>Deterministic Part</strong>: The expected value of ( X_t ) given past observations is 8.</li>
                <li><strong>Stochastic Part</strong>: ( \epsilon_t ) represents random fluctuations (white noise).</li>
                <li><strong>Prediction</strong>: The forecasted value of ( X_t ) is 8, with variability introduced by ( \epsilon_t ).</li>
            </ul>
            <h3 id="visualization">Visualization</h3>
            <p><img alt="ar2_model" src="https://github.com/djeada/Statistics-Notes/assets/37275728/9cc88c5a-174a-4503-a9cb-a20c43e26ab7" /></p>
            <ul>
                <li>The blue line represents the original mock data generated to simulate a time series following an AR(2) process.</li>
                <li>The red dashed line shows the predictions made by the fitted AR(2) model.</li>
            </ul>
            <p>This visualization illustrates how the AR(2) model captures the underlying pattern of the time series. The model uses the two most recent values to make predictions, adjusting to the trends and fluctuations in the data. </p>
            <h2 id="autoregressive-moving-average-arma-models">Autoregressive Moving Average (ARMA) Models</h2>
            <p>ARMA models combine autoregressive (AR) and moving average (MA) components to model time series data exhibiting both autocorrelation and serial dependence.</p>
            <h3 id="mathematical-definition-of-arma-models">Mathematical Definition of ARMA Models</h3>
            <p>An <strong>ARMA(( p, q ))</strong> model is defined by:</p>
            <p>[
                X_t = c + \sum_{i=1}^{p} \phi_i X_{t-i} + \epsilon_t + \sum_{j=1}^{q} \theta_j \epsilon_{t-j}
                ]</p>
            <p>or, equivalently, using the backshift operator ( B ):</p>
            <p>[
                \phi(B) X_t = c + \theta(B) \epsilon_t
                ]</p>
            <p>where:</p>
            <ul>
                <li>( \phi(B) = 1 - \phi_1 B - \phi_2 B^2 - \dots - \phi_p B^p )</li>
                <li>( \theta(B) = 1 + \theta_1 B + \theta_2 B^2 + \dots + \theta_q B^q )</li>
            </ul>
            <h3 id="key-concepts">Key Concepts</h3>
            <h4 id="stationarity-of-ar-processes">Stationarity of AR Processes</h4>
            <p>An AR(( p )) process is <strong>stationary</strong> if all the roots of the characteristic polynomial ( \phi(B) = 0 ) lie outside the unit circle in the complex plane. This condition ensures that the time series has a constant mean and variance over time.</p>
            <h4 id="invertibility-of-ma-processes">Invertibility of MA Processes</h4>
            <p>An MA(( q )) process is <strong>invertible</strong> if all the roots of ( \theta(B) = 0 ) lie outside the unit circle. Invertibility allows the MA process to be expressed as an infinite AR process, ensuring a unique representation and facilitating parameter estimation.</p>
            <h4 id="infinite-order-representations">Infinite Order Representations</h4>
            <ul>
                <li><strong>AR(âˆž) Representation of MA Processes</strong>:</li>
            </ul>
            <p>An MA process can be expressed as an infinite-order AR process:</p>
            <p>[
                X_t = \sum_{k=1}^{\infty} \pi_k X_{t-k} + \epsilon_t
                ]</p>
            <ul>
                <li><strong>MA(âˆž) Representation of AR Processes</strong>:</li>
            </ul>
            <p>An AR process can be expressed as an infinite-order MA process:</p>
            <p>[
                X_t = \sum_{k=0}^{\infty} \psi_k \epsilon_{t-k}
                ]</p>
            <h3 id="example-arma-1-1-process">Example: ARMA(1,1) Process</h3>
            <p>Consider the ARMA(1,1) model:</p>
            <p>[
                X_t = \phi X_{t-1} + \epsilon_t + \theta \epsilon_{t-1}
                ]</p>
            <p>Let ( \phi = 0.7 ), ( \theta = 0.2 ), and ( \epsilon_t ) is white noise.</p>
            <h4 id="simulation">Simulation</h4>
            <p>To analyze this process, we simulate a large number of observations using statistical software (e.g., R or Python) to approximate its properties.</p>
            <p>
            <div>
                <pre><code class="language-r">set.seed(500)
data &lt;- arima.sim(n = 1e6, list(ar = 0.7, ma = 0.2))</code></pre>
            </div>
            </p>
            <h4 id="converting-arma-to-infinite-order-processes">Converting ARMA to Infinite Order Processes</h4>
            <ul>
                <li><strong>AR(âˆž) Representation</strong>:</li>
            </ul>
            <p>[
                \begin{align<em>}
                    (1 - \phi B) X_t &amp;= (1 + \theta B) \epsilon_t \
                    X_t &amp;= (1 - \phi B)^{-1} (1 + \theta B) \epsilon_t \
                    X_t &amp;= [1 + \phi B + \phi^2 B^2 + \dots] (1 + \theta B) \epsilon_t \
                    \end{align</em>}
                ]</p>
            <p>Multiplying the series:</p>
            <p>[
                X_t = [1 + (\phi + \theta) B + (\phi^2 + \phi \theta) B^2 + \dots] \epsilon_t
                ]</p>
            <ul>
                <li><strong>MA(âˆž) Representation</strong>:</li>
            </ul>
            <p>[
                X_t = \frac{1 + \theta B}{1 - \phi B} \epsilon_t = [1 + \psi_1 B + \psi_2 B^2 + \dots] \epsilon_t
                ]</p>
            <p>Calculating ( \psi ) coefficients:</p>
            <p>[
                \psi_k = \phi^k + \theta \phi^{k-1}
                ]</p>
            <h4 id="theoretical-autocorrelations">Theoretical Autocorrelations</h4>
            <p>The autocorrelation function (ACF) for an ARMA(1,1) process is:</p>
            <p>[
                \rho_k = \phi^k \left( \frac{1 + \phi \theta}{1 + 2 \phi \theta + \theta^2} \right)
                ]</p>
            <p>Calculations:</p>
            <p>[
                \begin{align<em>}
                    \rho_1 &amp;= 0.7 \left( \frac{1 + 0.7 \times 0.2}{1 + 2 \times 0.7 \times 0.2 + 0.2^2} \right) \approx 0.777 \
                    \rho_2 &amp;= 0.7 \times \rho_1 \approx 0.544 \
                    \rho_3 &amp;= 0.7 \times \rho_2 \approx 0.381 \
                    \end{align</em>}
                ]</p>
            <h4 id="results-and-interpretation">Results and Interpretation</h4>
            <ul>
                <li><strong>Empirical ACF</strong>: The autocorrelations computed from the simulated data closely match the theoretical values, validating the model.</li>
                <li><strong>Model Behavior</strong>: The ARMA(1,1) model captures both the short-term dependencies (MA component) and the longer-term autocorrelation (AR component).</li>
            </ul>
            <hr />
            <h2 id="autoregressive-integrated-moving-average-arima-models">Autoregressive Integrated Moving Average (ARIMA) Models</h2>
            <p>ARIMA models generalize ARMA models to include differencing, allowing them to model non-stationary time series data.</p>
            <h3 id="mathematical-definition-of-arima-models">Mathematical Definition of ARIMA Models</h3>
            <p>An <strong>ARIMA(( p, d, q ))</strong> model is defined by:</p>
            <p>[
                \phi(B) (1 - B)^d X_t = c + \theta(B) \epsilon_t
                ]</p>
            <p>where:</p>
            <ul>
                <li>( \phi(B) ): Autoregressive polynomial.</li>
                <li>( \theta(B) ): Moving average polynomial.</li>
                <li>( d ): Order of differencing required to achieve stationarity.</li>
            </ul>
            <h3 id="determining-differencing-order">Determining Differencing Order</h3>
            <ul>
                <li><strong>Unit Root Tests</strong>: Tests like the Augmented Dickey-Fuller (ADF) test assess whether differencing is needed.</li>
                <li><strong>Visual Inspection</strong>: Time series plots reveal trends or changing variance.</li>
                <li><strong>ACF Analysis</strong>: A slow decay in the ACF suggests non-stationarity.</li>
            </ul>
            <h3 id="fitting-arima-models-numerical-example">Fitting ARIMA Models: Numerical Example</h3>
            <p>Suppose we have a time series ( X_t ) exhibiting an upward trend.</p>
            <h4 id="step-1-differencing">Step 1: Differencing</h4>
            <p>First-order differencing is applied to achieve stationarity:</p>
            <p>[
                Y_t = (1 - B) X_t = X_t - X_{t-1}
                ]</p>
            <h4 id="step-2-model-identification">Step 2: Model Identification</h4>
            <p>Analyzing the differenced series ( Y_t ):</p>
            <ul>
                <li><strong>ACF</strong>: Significant spikes at lag ( q ) suggest an MA(( q )) component.</li>
                <li><strong>PACF</strong>: Significant spikes at lag ( p ) suggest an AR(( p )) component.</li>
            </ul>
            <p>Assume ACF suggests MA(1) and PACF suggests AR(1).</p>
            <h4 id="step-3-parameter-estimation">Step 3: Parameter Estimation</h4>
            <p>Fit an ARIMA(1,1,1) model:</p>
            <p>[
                (1 - \phi B)(1 - B) X_t = c + (1 + \theta B) \epsilon_t
                ]</p>
            <p>Estimate ( \phi ), ( \theta ), and ( c ) using MLE.</p>
            <h4 id="step-4-model-diagnostics">Step 4: Model Diagnostics</h4>
            <ul>
                <li><strong>Residual Analysis</strong>: Plot residuals to check for randomness.</li>
                <li><strong>Ljung-Box Test</strong>: Confirm absence of autocorrelation in residuals.</li>
                <li><strong>Information Criteria</strong>: Compare AIC and BIC values for different models.</li>
            </ul>
            <h4 id="step-5-forecasting">Step 5: Forecasting</h4>
            <p>Use the fitted model to forecast future values:</p>
            <p>[
                \hat{X}<em>{t+h} = c + \phi \hat{X}</em>{t+h-1} + \theta \hat{\epsilon}_{t+h-1}
                ]</p>
            <h2 id="limitations-of-ar-and-arima-models">Limitations of AR and ARIMA Models</h2>
            <ul>
                <li><strong>Linearity Assumption</strong>: AR and ARIMA models assume linear relationships, which may not capture nonlinear dynamics.</li>
                <li><strong>Stationarity Requirement</strong>: Accurate modeling requires stationarity; real-world data often violate this assumption.</li>
                <li><strong>Parameter Estimation Sensitivity</strong>: Estimates can be sensitive to outliers and model specification.</li>
                <li><strong>Model Selection Complexity</strong>: Choosing appropriate ( p ), ( d ), and ( q ) values is non-trivial and may require expert judgment.</li>
                <li><strong>Overfitting Risk</strong>: Overly complex models may fit the noise rather than the underlying process, reducing predictive performance.</li>
            </ul>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#autoregressive-ar-models-in-time-series-analysis">Autoregressive (AR) Models in Time Series Analysis</a>
                    <ol>
                        <li><a href="#mathematical-definition-of-ar-models">Mathematical Definition of AR Models</a></li>
                        <li><a href="#properties-of-ar-models">Properties of AR Models</a></li>
                        <li><a href="#estimation-of-parameters">Estimation of Parameters</a></li>
                        <li><a href="#model-selection">Model Selection</a></li>
                    </ol>
                </li>
                <li><a href="#example-ar-2-model-in-time-series-forecasting">Example: AR(2) Model in Time Series Forecasting</a>
                    <ol>
                        <li><a href="#ar-2-model-equation">AR(2) Model Equation</a>
                            <ol>
                                <li><a href="#calculation">Calculation</a></li>
                                <li><a href="#interpretation">Interpretation</a></li>
                            </ol>
                        </li>
                        <li><a href="#visualization">Visualization</a></li>
                    </ol>
                </li>
                <li><a href="#autoregressive-moving-average-arma-models">Autoregressive Moving Average (ARMA) Models</a>
                    <ol>
                        <li><a href="#mathematical-definition-of-arma-models">Mathematical Definition of ARMA Models</a></li>
                        <li><a href="#key-concepts">Key Concepts</a>
                            <ol>
                                <li><a href="#stationarity-of-ar-processes">Stationarity of AR Processes</a></li>
                                <li><a href="#invertibility-of-ma-processes">Invertibility of MA Processes</a></li>
                                <li><a href="#infinite-order-representations">Infinite Order Representations</a></li>
                            </ol>
                        </li>
                        <li><a href="#example-arma-1-1-process">Example: ARMA(1,1) Process</a>
                            <ol>
                                <li><a href="#simulation">Simulation</a></li>
                                <li><a href="#converting-arma-to-infinite-order-processes">Converting ARMA to Infinite Order Processes</a></li>
                                <li><a href="#theoretical-autocorrelations">Theoretical Autocorrelations</a></li>
                                <li><a href="#results-and-interpretation">Results and Interpretation</a></li>
                            </ol>
                        </li>
                    </ol>
                </li>
                <li><a href="#autoregressive-integrated-moving-average-arima-models">Autoregressive Integrated Moving Average (ARIMA) Models</a>
                    <ol>
                        <li><a href="#mathematical-definition-of-arima-models">Mathematical Definition of ARIMA Models</a></li>
                        <li><a href="#determining-differencing-order">Determining Differencing Order</a></li>
                        <li><a href="#fitting-arima-models-numerical-example">Fitting ARIMA Models: Numerical Example</a>
                            <ol>
                                <li><a href="#step-1-differencing">Step 1: Differencing</a></li>
                                <li><a href="#step-2-model-identification">Step 2: Model Identification</a></li>
                                <li><a href="#step-3-parameter-estimation">Step 3: Parameter Estimation</a></li>
                                <li><a href="#step-4-model-diagnostics">Step 4: Model Diagnostics</a></li>
                                <li><a href="#step-5-forecasting">Step 5: Forecasting</a></li>
                            </ol>
                        </li>
                    </ol>
                </li>
                <li><a href="#limitations-of-ar-and-arima-models">Limitations of AR and ARIMA Models</a></li>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li>Basic Concepts<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/axioms_of_probability.html">Axioms of Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/bayes_theorem.html">Bayes Theorem</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/bayesian_vs_frequentist.html">Bayesian vs Frequentist</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/conditional_probability.html">Conditional Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/descriptive_statistics.html">Descriptive Statistics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/geometric_probability.html">Geometric Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/introduction_to_probability.html">Introduction to Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/introduction_to_statistics.html">Introduction to Statistics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/probability_tree.html">Probability Tree</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/standard_error_and_lln.html">Standard Error and Lln</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/total_probability.html">Total Probability</a></li>
                        </ol>
                    </li>
                    <li>Probability Distributions<ol>
                            <li>Continuous Distributions<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/beta_distribution.html">Beta Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/chi_square_distribution.html">Chi Square Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/exponential_distribution.html">Exponential Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/f_distribution.html">F Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/gamma_distribution.html">Gamma Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/log_normal_distribution.html">Log Normal Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/normal_distribution.html">Normal Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/student_t_distribution.html">Student T Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/uniform_distribution.html">Uniform Distribution</a></li>
                                </ol>
                            </li>
                            <li>Discrete Distributions<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/binomial_distribution.html">Binomial Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/geometric_distribution.html">Geometric Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/negative_binomial_distribution.html">Negative Binomial Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/poisson_distribution.html">Poisson Distribution</a></li>
                                </ol>
                            </li>
                            <li>Intro<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/central_limit_theorem.html">Central Limit Theorem</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/introduction_to_distributions.html">Introduction to Distributions</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/normal_curve_and_z_score.html">Normal Curve and z Score</a></li>
                                </ol>
                            </li>
                        </ol>
                    </li>
                    <li>Correlation and Regression<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/correlation.html">Correlation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/covariance.html">Covariance</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/logistic_regression.html">Logistic Regression</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/metrics.html">Metrics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/multiple_regression.html">Multiple Regression</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/simple_linear_regression.html">Simple Linear Regression</a></li>
                        </ol>
                    </li>
                    <li>Time Series Analysis<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autocorrelation_function.html">Autocorrelation Function</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autocovariance_function.html">Autocovariance Function</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autoregressive_models.html">Autoregressive Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/backward_shift_operator.html">Backward Shift Operator</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/difference_equations.html">Difference Equations</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/forecasting.html">Forecasting</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/invertibility.html">Invertibility</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/moving_average_models.html">Moving Average Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/random_walk.html">Random Walk</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/seasonality_and_trends.html">Seasonality and Trends</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/series.html">Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/stationarity.html">Stationarity</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/time_series.html">Time Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/yule_walker_equations.html">Yule Walker Equations</a></li>
                        </ol>
                    </li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../../app.js"></script>
    </footer>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>