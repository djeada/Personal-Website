<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Stationarity in Time Series</title>
    <meta content="Stationarity is a fundamental concept in time series analysis." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>Last modified: December 03, 2024</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="stationarity-in-time-series">Stationarity in Time Series</h2>
            <p>Stationarity is a fundamental concept in time series analysis. A time series is considered <strong>stationary</strong> if its statistical propertiesâ€”such as mean, variance, and autocovarianceâ€”remain constant over time. Stationary processes are crucial in time series modeling because many methods, such as ARIMA and ARMA models, assume stationarity (we need to check if data is stationar before applying those models).</p>
            <p>Stationarity can be classified into two types:</p>
            <ol>
                <li>Strict stationarity implies that the <strong>entire distribution of the process remains the same over time</strong>. </li>
                <li>Weak stationarity, also known as second-order stationarity, requires only that the <strong>mean, variance, and autocovariance remain time-invariant</strong> over time. </li>
            </ol>
            <h3 id="intuition-for-stationary-time-series">Intuition for Stationary Time Series</h3>
            <p>A <strong>stationary time series</strong> behaves similarly over time, meaning:</p>
            <ul>
                <li>The mean of the series shows <strong>no trend</strong> and does not systematically change over time.</li>
                <li>The variability around the mean has a <strong>constant variance</strong>, remaining stable throughout.</li>
                <li>There are <strong>no periodic fluctuations</strong> such as seasonality or cyclic behavior, unless explicitly modeled.</li>
            </ul>
            <p>This means that the statistical properties of one segment of the series are similar to those of any other segment, allowing us to predict future behavior based on past data.</p>
            <h3 id="strict-stationarity">Strict Stationarity</h3>
            <p>A process is said to be <strong>strictly stationary</strong> if the joint distribution of any subset of observations $X_{t_1}, X_{t_2}, \dots, X_{t_k}$ is the same as the distribution of $X_{t_1 + \tau}, X_{t_2 + \tau}, \dots, X_{t_k + \tau}$ for all $\tau$. </p>
            <p>In simple terms, the process looks the same no matter how we shift it in time. Strict stationarity implies that:</p>
            <ul>
                <li>The distribution of $X_t$ does not change over time.</li>
                <li>All moments of the distribution (mean, variance, higher moments) are constant over time.</li>
            </ul>
            <h3 id="weak-second-order-stationarity">Weak (Second-Order) Stationarity</h3>
            <p>Weak stationarity, also known as <strong>second-order stationarity</strong>, requires only that the <strong>first two moments</strong> (mean and variance) and the <strong>autocovariance</strong> depend solely on the lag between observations, not on time itself.</p>
            <p>A time series ${X_t}$ is weakly stationary if:</p>
            <ol>
                <li>The <strong>mean</strong> of the series is constant: $E[X_t] = \mu$ for all $t$.</li>
                <li>The <strong>variance</strong> is constant: $\text{Var}(X_t) = \sigma^2$ for all $t$.</li>
                <li>The <strong>autocovariance</strong> between $X_t$ and $X_{t+k}$ depends only on the lag $k$, not on $t$:</li>
            </ol>
            <p>$$
                \text{Cov}(X_t, X_{t+k}) = \gamma(k)
                $$</p>
            <p>Weak stationarity is often sufficient for most time series models, as it focuses on ensuring that the mean and variance remain stable over time, making the process easier to model and analyze.</p>
            <h3 id="properties-of-stationary-processes">Properties of Stationary Processes</h3>
            <h4 id="mean-variance-and-autocovariance-functions">Mean, Variance, and Autocovariance Functions</h4>
            <p>To analyze a stationary process, we focus on three key functions:</p>
            <ul>
                <li>The <strong>mean function</strong> $\mu(t) = E[X_t]$ represents the expected value of the process at time $t$, and for a stationary process, this should remain constant.</li>
                <li>The <strong>variance function</strong> $\sigma^2(t) = \text{Var}(X_t)$ gives the variance at time $t$, which must also be constant for stationarity.</li>
                <li>The <strong>autocovariance function</strong> $\gamma(k) = \text{Cov}(X_t, X_{t+k})$ measures how the process correlates with itself at different time lags $k$, and for a stationary process, it depends only on the lag $k$, not on time $t$.</li>
            </ul>
            <h4 id="autocorrelation-and-bounds">Autocorrelation and Bounds</h4>
            <p>For a weakly stationary process, the <strong>autocorrelation function</strong> $\rho(k)$, which measures the correlation between two points in the series separated by lag $k$, is bounded by -1 and 1:</p>
            <p>$$
                -1 \leq \rho(k) \leq 1
                $$</p>
            <p>This bound can be derived from basic linear algebra principles that apply to correlations between random variables.</p>
            <h3 id="examples-of-stationary-processes">Examples of Stationary Processes</h3>
            <h4 id="white-noise"><strong>White Noise</strong></h4>
            <p>White noise is the simplest example of a stationary process. It is defined as a sequence of uncorrelated, identically distributed random variables:</p>
            <p>$$
                X_t \sim \mathcal{N}(0, \sigma^2)
                $$</p>
            <p>Properties of white noise:</p>
            <ul>
                <li>The <strong>mean</strong> is constant: $E[X_t] = 0$.</li>
                <li>The <strong>variance</strong> is constant: $\text{Var}(X_t) = \sigma^2$.</li>
                <li>The <strong>autocovariance</strong> function is:</li>
            </ul>
            <p>$$
                \gamma(k) =
                \begin{cases}
                \sigma^2 &amp; \text{if } k = 0 \\
                0 &amp; \text{if } k \neq 0
                \end{cases}
                $$</p>
            <ul>
                <li>The <strong>autocorrelation</strong> function is:</li>
            </ul>
            <p>$$
                \rho(k) =
                \begin{cases}
                1 &amp; \text{if } k = 0 \\
                0 &amp; \text{if } k \neq 0
                \end{cases}
                $$</p>
            <p>Thus, white noise is a stationary process because its mean and variance are constant, and its autocovariance depends only on the lag.</p>
            <h4 id="moving-average-ma-process">Moving Average (MA) Process</h4>
            <p>A <strong>moving average (MA) process</strong> of order $q$, denoted as MA(q), is another example of a weakly stationary process. It is defined as:</p>
            <p>$$
                X_t = \beta_0 Z_t + \beta_1 Z_{t-1} + \dots + \beta_q Z_{t-q}
                $$</p>
            <p>where $Z_t \sim \mathcal{N}(0, \sigma_Z^2)$ are independent white noise terms.</p>
            <p>For an MA(q) process:</p>
            <ul>
                <li>The <strong>mean</strong> is zero: $E[X_t] = 0$.</li>
                <li>The <strong>variance</strong> is constant:</li>
            </ul>
            <p>$$
                \text{Var}(X_t) = \sigma_Z^2 \sum_{i=0}^{q} \beta_i^2
                $$</p>
            <ul>
                <li>The <strong>autocovariance</strong> function $\gamma(k)$ depends on the lag $k$:</li>
            </ul>
            <p>$$
                \gamma(k) =
                \begin{cases}
                \sigma_Z^2 \sum_{i=0}^{q-k} \beta_i \beta_{i+k} &amp; \text{if } k \leq q \\
                0 &amp; \text{if } k &gt; q
                \end{cases}
                $$</p>
            <p>The autocorrelation function $\rho(k)$ is obtained by normalizing the autocovariance by the variance:</p>
            <p>$$
                \rho(k) = \frac{\gamma(k)}{\gamma(0)}
                $$</p>
            <p>The MA(q) process is weakly stationary because its mean and variance are constant, and the autocovariance depends only on the lag.</p>
            <h3 id="non-stationary-processes">Non-Stationary Processes</h3>
            <h4 id="random-walk">Random Walk</h4>
            <p>A <strong>random walk</strong> is an example of a non-stationary process. A random walk can be written as:</p>
            <p>$$
                X_t = X_{t-1} + Z_t
                $$</p>
            <p>where $Z_t$ is white noise.</p>
            <p>For a random walk:</p>
            <p>The <strong>mean</strong> grows over time:</p>
            <p>$$
                E[X_t] = t \cdot \mu
                $$</p>
            <p>The <strong>variance</strong> increases with time:</p>
            <p>$$
                \text{Var}(X_t) = t \cdot \sigma^2
                $$</p>
            <p>Since the variance and mean depend on time, the random walk is <strong>not stationary</strong>. However, applying a <strong>difference operator</strong> can transform a random walk into a stationary series.</p>
            <h4 id="differencing-to-remove-non-stationarity">Differencing to Remove Non-Stationarity</h4>
            <p>To handle non-stationary processes like random walks, we can apply the <strong>difference operator</strong> $\Delta$, which removes trends and transforms the process into a stationary one.</p>
            <p>The difference operator is defined as:</p>
            <p>$$
                \Delta X_t = X_t - X_{t-1} = Z_t
                $$</p>
            <p>By differencing the series, we convert a random walk into white noise, which is stationary. This technique is essential for models like ARIMA that require the data to be stationary before modeling.</p>
            <h3 id="dealing-with-non-stationary-time-series">Dealing with Non-Stationary Time Series</h3>
            <p>In real-world applications, many time series are non-stationary. To apply statistical models that require stationarity, we often use <strong>transformations</strong> such as:</p>
            <ul>
                <li>Applying <strong>differencing</strong> helps remove trends and makes the series stationary. </li>
                <li>Using <strong>logarithmic transformations</strong> can stabilize the variance in the series. </li>
                <li>The process of <strong>detrending</strong> removes long-term trends, allowing a focus on short-term fluctuations.</li>
            </ul>
            <h4 id="example-of-differencing-in-python">Example of Differencing in Python</h4>
            <p>We can use Python to difference a non-stationary series like a random walk:</p>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# Simulate a random walk
np.random.seed(42)
N = 1000
Z = np.random.normal(0, 1, N)
X = np.cumsum(Z)  #

 Random walk as cumulative sum of white noise

# Apply differencing to make it stationary
diff_X = np.diff(X)

# Plot the original random walk and the differenced series
plt.figure(figsize=(10, 6))
plt.subplot(2, 1, 1)
plt.plot(X, label='Random Walk')
plt.title('Random Walk (Non-Stationary)')
plt.grid(True)

plt.subplot(2, 1, 2)
plt.plot(diff_X, label='Differenced Series')
plt.title('Differenced Series (Stationary)')
plt.grid(True)

plt.tight_layout()
plt.show()</code></pre>
            </div>
            </p>
            <p>I. Simulating a Random Walk:</p>
            <ul>
                <li>A random walk is generated by taking the cumulative sum of normally distributed random numbers. This produces a series where each value depends on the previous one plus some random noise.</li>
                <li>The random walk is non-stationary because it lacks a constant mean and variance over timeâ€”it drifts unpredictably.</li>
            </ul>
            <p>II. Differencing:</p>
            <ul>
                <li>Differencing transforms the non-stationary series into a stationary one by subtracting the previous observation from the current one. This removes any trend or long-term structure in the data.</li>
                <li>In Python, this is done using <code>np.diff()</code>, which takes the difference between consecutive elements of the series.</li>
            </ul>
            <p>The result plot would look like the following:</p>
            <p><img alt="output(19)" src="https://github.com/user-attachments/assets/2cebba56-7d3b-470c-9511-56d617be7159" /></p>
            <p>In this plot, the upper section shows the random walk (non-stationary), while the lower section shows the differenced series (stationary). Differencing removes the trend from the original series, making it easier to model and predict future values.</p>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol><a href="#stationarity-in-time-series">Stationarity in Time Series</a>
                <ol>
                    <li><a href="#intuition-for-stationary-time-series">Intuition for Stationary Time Series</a></li>
                    <li><a href="#strict-stationarity">Strict Stationarity</a></li>
                    <li><a href="#weak-second-order-stationarity">Weak (Second-Order) Stationarity</a></li>
                    <li><a href="#properties-of-stationary-processes">Properties of Stationary Processes</a>
                        <ol>
                            <li><a href="#mean-variance-and-autocovariance-functions">Mean, Variance, and Autocovariance Functions</a></li>
                            <li><a href="#autocorrelation-and-bounds">Autocorrelation and Bounds</a></li>
                        </ol>
                    </li>
                    <li><a href="#examples-of-stationary-processes">Examples of Stationary Processes</a>
                        <ol>
                            <li><a href="#white-noise">White Noise</a></li>
                            <li><a href="#moving-average-ma-process">Moving Average (MA) Process</a></li>
                        </ol>
                    </li>
                    <li><a href="#non-stationary-processes">Non-Stationary Processes</a>
                        <ol>
                            <li><a href="#random-walk">Random Walk</a></li>
                            <li><a href="#differencing-to-remove-non-stationarity">Differencing to Remove Non-Stationarity</a></li>
                        </ol>
                    </li>
                    <li><a href="#dealing-with-non-stationary-time-series">Dealing with Non-Stationary Time Series</a>
                        <ol>
                            <li><a href="#example-of-differencing-in-python">Example of Differencing in Python</a></li>
                        </ol>
                    </li>
                </ol>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li>Basic Concepts<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/axioms_of_probability.html">Axioms of Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/bayes_theorem.html">Bayes Theorem</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/bayesian_vs_frequentist.html">Bayesian vs Frequentist</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/conditional_probability.html">Conditional Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/descriptive_statistics.html">Descriptive Statistics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/geometric_probability.html">Geometric Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/introduction_to_probability.html">Introduction to Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/introduction_to_statistics.html">Introduction to Statistics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/probability_tree.html">Probability Tree</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/standard_error_and_lln.html">Standard Error and Lln</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/total_probability.html">Total Probability</a></li>
                        </ol>
                    </li>
                    <li>Probability Distributions<ol>
                            <li>Continuous Distributions<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/beta_distribution.html">Beta Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/chi_square_distribution.html">Chi Square Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/exponential_distribution.html">Exponential Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/f_distribution.html">F Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/gamma_distribution.html">Gamma Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/log_normal_distribution.html">Log Normal Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/normal_distribution.html">Normal Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/student_t_distribution.html">Student T Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/uniform_distribution.html">Uniform Distribution</a></li>
                                </ol>
                            </li>
                            <li>Discrete Distributions<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/binomial_distribution.html">Binomial Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/geometric_distribution.html">Geometric Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/negative_binomial_distribution.html">Negative Binomial Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/poisson_distribution.html">Poisson Distribution</a></li>
                                </ol>
                            </li>
                            <li>Intro<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/central_limit_theorem.html">Central Limit Theorem</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/introduction_to_distributions.html">Introduction to Distributions</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/normal_curve_and_z_score.html">Normal Curve and z Score</a></li>
                                </ol>
                            </li>
                        </ol>
                    </li>
                    <li>Correlation and Regression<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/correlation.html">Correlation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/covariance.html">Covariance</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/logistic_regression.html">Logistic Regression</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/metrics.html">Metrics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/multiple_regression.html">Multiple Regression</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/simple_linear_regression.html">Simple Linear Regression</a></li>
                        </ol>
                    </li>
                    <li>Time Series Analysis<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autocorrelation_function.html">Autocorrelation Function</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autocovariance_function.html">Autocovariance Function</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autoregressive_models.html">Autoregressive Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/backward_shift_operator.html">Backward Shift Operator</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/difference_equations.html">Difference Equations</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/forecasting.html">Forecasting</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/invertibility.html">Invertibility</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/moving_average_models.html">Moving Average Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/random_walk.html">Random Walk</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/seasonality_and_trends.html">Seasonality and Trends</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/series.html">Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/stationarity.html">Stationarity</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/time_series.html">Time Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/yule_walker_equations.html">Yule Walker Equations</a></li>
                        </ol>
                    </li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../../app.js"></script>
    </footer>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>