<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Stationarity in Time Series</title>
    <meta content="Stationarity is an important idea in time series analysis." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper"><article-section id="article-body">
            <div class="article-action-buttons"><button class="btn-suggest-edit" title="Suggest Edit">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M4 21h4l11-11-4-4L4 17v4z" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button><button class="btn-create-issue" title="Create Issue">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"></circle>
                        <line stroke="currentColor" stroke-width="2" x1="12" x2="12" y1="8" y2="12"></line>
                        <circle cx="12" cy="16" fill="currentColor" r="1"></circle>
                    </svg>
                </button><button class="btn-download" title="Download">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M12 5v14m0 0l-6-6m6 6l6-6" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button></div>
            <p style="text-align: right;"><i>Last modified: December 26, 2022</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="stationarity-in-time-series">Stationarity in Time Series</h2>
            <p>Stationarity is an important idea in time series analysis. A time series is considered stationary if its statistical propertiesâ€”like the mean, variance, and autocovarianceâ€”stay constant over time. This matters because methods like ARIMA and ARMA are designed to work with stationary data, so itâ€™s a good idea to check for stationarity before using these models.</p>
            <p>Stationarity can be classified into two types:</p>
            <ol>
                <li>Strict stationarity implies that the <strong>entire distribution of the process remains the same over time</strong>. </li>
                <li>Weak stationarity, also known as second-order stationarity, requires only that the <strong>mean, variance, and autocovariance remain time-invariant</strong> over time. </li>
            </ol>
            <h3 id="why-do-we-care-about-stationarity-">Why Do We Care About Stationarity?</h3>
            <p>Imagine we are analyzing synthetic stock prices that exhibit both an upward trend and random fluctuations over time. Suppose our goal is to study the seasonal variations in these stock prices throughout the year. However, the data we have spans several years, during which the stock prices have been steadily increasing. This steady increase introduces non-stationarity, complicating our analysis of seasonal patterns.</p>
            <p><strong>Addressing Non-Stationarity: Detrending the Data</strong></p>
            <p>To effectively analyze the seasonal component, we first need to remove the non-stationary trend. This can be accomplished by fitting a linear model to the data using least squares regression:</p>
            <p>$$\text{Stock Price} = b \times \text{Time} + z$$</p>
            <p>Where:</p>
            <ul>
                <li>$b$ is the slope representing the trend over time.</li>
                <li>$z$ is the intercept.</li>
            </ul>
            <p>By subtracting this linear trend from the original stock price data, we obtain "detrended" stock prices. This process isolates the seasonal fluctuations by eliminating the underlying upward trend.</p>
            <p><strong>Visualization</strong></p>
            <p><img alt="Synthetic Stock Prices Analysis" src="https://github.com/user-attachments/assets/db1f2c1c-f1ce-4aa8-b3e0-34d821a4a001" /></p>
            <ul>
                <li>Top panel displays the synthetic stock prices over time with a fitted linear trend (red dashed line) using least squares regression. This trend captures the overall upward movement in stock prices.</li>
                <li>Bottom panel shows the detrended stock prices (residuals), highlighting the seasonal variations after removing the linear trend. These residuals are more suitable for analyzing periodic patterns without the confounding effect of the trend.</li>
            </ul>
            <h3 id="intuition-for-stationary-time-series">Intuition for Stationary Time Series</h3>
            <p>A <strong>stationary time series</strong> behaves similarly over time, meaning:</p>
            <ul>
                <li>The mean of the series shows <strong>no trend</strong> and does not systematically change over time.</li>
                <li>The variability around the mean has a <strong>constant variance</strong>, remaining stable throughout.</li>
                <li>There are <strong>no periodic fluctuations</strong> such as seasonality or cyclic behavior, unless explicitly modeled.</li>
            </ul>
            <p>This means that the statistical properties of one segment of the series are similar to those of any other segment, allowing us to predict future behavior based on past data.</p>
            <h3 id="strict-stationarity">Strict Stationarity</h3>
            <p>A process is said to be <strong>strictly stationary</strong> if the joint distribution of any subset of observations $X_{t_1}, X_{t_2}, \dots, X_{t_k}$ is the same as the distribution of $X_{t_1 + \tau}, X_{t_2 + \tau}, \dots, X_{t_k + \tau}$ for all $\tau$. </p>
            <p>In simple terms, the process looks the same no matter how we shift it in time. Strict stationarity implies that:</p>
            <ul>
                <li>The distribution of $X_t$ does not change over time.</li>
                <li>All moments of the distribution (mean, variance, higher moments) are constant over time.</li>
            </ul>
            <h3 id="weak-second-order-stationarity">Weak (Second-Order) Stationarity</h3>
            <p>Weak stationarity, also known as <strong>second-order stationarity</strong>, requires only that the <strong>first two moments</strong> (mean and variance) and the <strong>autocovariance</strong> depend solely on the lag between observations, not on time itself.</p>
            <p>A time series ${X_t}$ is weakly stationary if:</p>
            <ol>
                <li>The <strong>mean</strong> of the series is constant: $E[X_t] = \mu$ for all $t$.</li>
                <li>The <strong>variance</strong> is constant: $\text{Var}(X_t) = \sigma^2$ for all $t$.</li>
                <li>The <strong>autocovariance</strong> between $X_t$ and $X_{t+k}$ depends only on the lag $k$, not on $t$:</li>
            </ol>
            <p>$$
                \text{Cov}(X_t, X_{t+k}) = \gamma(k)
                $$</p>
            <p>Weak stationarity is often sufficient for most time series models, as it focuses on ensuring that the mean and variance remain stable over time, making the process easier to model and analyze.</p>
            <h3 id="properties-of-stationary-processes">Properties of Stationary Processes</h3>
            <h4 id="mean-variance-and-autocovariance-functions">Mean, Variance, and Autocovariance Functions</h4>
            <p>To analyze a stationary process, we focus on three key functions:</p>
            <ul>
                <li>The <strong>mean function</strong> $\mu(t) = E[X_t]$ represents the expected value of the process at time $t$, and for a stationary process, this should remain constant.</li>
                <li>The <strong>variance function</strong> $\sigma^2(t) = \text{Var}(X_t)$ gives the variance at time $t$, which must also be constant for stationarity.</li>
                <li>The <strong>autocovariance function</strong> $\gamma(k) = \text{Cov}(X_t, X_{t+k})$ measures how the process correlates with itself at different time lags $k$, and for a stationary process, it depends only on the lag $k$, not on time $t$.</li>
            </ul>
            <h4 id="autocorrelation-and-bounds">Autocorrelation and Bounds</h4>
            <p>For a weakly stationary process, the <strong>autocorrelation function</strong> $\rho(k)$, which measures the correlation between two points in the series separated by lag $k$, is bounded by -1 and 1:</p>
            <p>$$
                -1 \leq \rho(k) \leq 1
                $$</p>
            <p>This bound can be derived from basic linear algebra principles that apply to correlations between random variables.</p>
            <h3 id="examples-of-stationary-processes">Examples of Stationary Processes</h3>
            <h4 id="white-noise"><strong>White Noise</strong></h4>
            <p>White noise is the simplest example of a stationary process. It is defined as a sequence of uncorrelated, identically distributed random variables:</p>
            <p>$$
                X_t \sim \mathcal{N}(0, \sigma^2)
                $$</p>
            <p>Properties of white noise:</p>
            <ul>
                <li>The <strong>mean</strong> is constant: $E[X_t] = 0$.</li>
                <li>The <strong>variance</strong> is constant: $\text{Var}(X_t) = \sigma^2$.</li>
                <li>The <strong>autocovariance</strong> function is:</li>
            </ul>
            <p>$$
                \gamma(k) =
                \begin{cases}
                \sigma^2 &amp; \text{if } k = 0 \\
                0 &amp; \text{if } k \neq 0
                \end{cases}
                $$</p>
            <ul>
                <li>The <strong>autocorrelation</strong> function is:</li>
            </ul>
            <p>$$
                \rho(k) =
                \begin{cases}
                1 &amp; \text{if } k = 0 \\
                0 &amp; \text{if } k \neq 0
                \end{cases}
                $$</p>
            <p>Below is a plot of synthetically generated white noise:</p>
            <p><img alt="white_noise" src="https://github.com/user-attachments/assets/a24cc561-f34a-431e-bac0-2f9cd5e62a49" /></p>
            <p>Thus, white noise is a stationary process because its mean and variance are constant, and its autocovariance depends only on the lag.</p>
            <h4 id="moving-average-ma-process">Moving Average (MA) Process</h4>
            <p>A <strong>moving average (MA) process</strong> of order $q$, denoted as MA(q), is another example of a weakly stationary process. It is defined as:</p>
            <p>$$
                X_t = \beta_0 Z_t + \beta_1 Z_{t-1} + \dots + \beta_q Z_{t-q}
                $$</p>
            <p>where $Z_t \sim \mathcal{N}(0, \sigma_Z^2)$ are independent white noise terms.</p>
            <p>For an MA(q) process:</p>
            <ul>
                <li>The <strong>mean</strong> is zero: $E[X_t] = 0$.</li>
                <li>The <strong>variance</strong> is constant:</li>
            </ul>
            <p>$$
                \text{Var}(X_t) = \sigma_Z^2 \sum_{i=0}^{q} \beta_i^2
                $$</p>
            <ul>
                <li>The <strong>autocovariance</strong> function $\gamma(k)$ depends on the lag $k$:</li>
            </ul>
            <p>$$
                \gamma(k) =
                \begin{cases}
                \sigma_Z^2 \sum_{i=0}^{q-k} \beta_i \beta_{i+k} &amp; \text{if } k \leq q \\
                0 &amp; \text{if } k &gt; q
                \end{cases}
                $$</p>
            <p>The autocorrelation function $\rho(k)$ is obtained by normalizing the autocovariance by the variance:</p>
            <p>$$
                \rho(k) = \frac{\gamma(k)}{\gamma(0)}
                $$</p>
            <p>Below is a plot of the Moving Average (MA) process of order $q=2$:</p>
            <p><img alt="moving_average" src="https://github.com/user-attachments/assets/28e1d7fa-243b-4841-afba-afd2188ac400" /></p>
            <p>The MA(q) process is weakly stationary because its mean and variance are constant, and the autocovariance depends only on the lag.</p>
            <h3 id="non-stationary-processes">Non-Stationary Processes</h3>
            <p>A <strong>non-stationary process</strong> is a time series whose statistical propertiesâ€”such as mean, variance, and autocorrelationâ€”change over time. Unlike stationary processes, which have consistent statistical characteristics, non-stationary series can exhibit trends, seasonal effects, or other forms of structural changes that complicate analysis.</p>
            <p>Non-stationary processes can generally be categorized into two types:</p>
            <ol>
                <li><strong>Trend-Stationary Processes</strong></li>
                <li><strong>Difference-Stationary Processes</strong></li>
            </ol>
            <h4 id="trend-stationary-processes">Trend-Stationary Processes</h4>
            <p>A <strong>trend-stationary</strong> series has a stable long-term trend around which the data fluctuates. If a time series follows a trend-stationary process, it tends to revert to its trend line after experiencing a disturbance.</p>
            <p><strong>De-trending:</strong> </p>
            <p>To achieve stationarity in such series, one can remove the trend component. This is typically done by fitting a trend line (e.g., linear or polynomial) to the data and subtracting it from the original series. The resulting series, with the trend removed, should exhibit stationary behavior.</p>
            <h4 id="difference-stationary-processes">Difference-Stationary Processes</h4>
            <p>When de-trending is insufficient to stabilize the mean and variance of a series, the process might be <strong>difference-stationary</strong>. In these cases, even after removing the trend, the series still exhibits non-constant statistical properties.</p>
            <p><strong>Differencing:</strong> </p>
            <p>To achieve stationarity, we transform the series by taking differences between consecutive observations (period-to-period) or between observations separated by a season (season-to-season). This transformation often stabilizes the mean and variance, resulting in a stationary series of changes rather than levels.</p>
            <h4 id="transformations-to-achieve-stationarity">Transformations to Achieve Stationarity</h4>
            <p>To prepare a non-stationary time series for modeling with techniques that require stationarity (like ARIMA), various <strong>transformations</strong> can be applied:</p>
            <p><strong>Differencing</strong></p>
            <ul>
                <li>Differencing removes trends from time series data and stabilizes the mean. </li>
                <li>It involves computing the difference between consecutive observations in the series. </li>
                <li>The first difference of a series $Y_t$ is defined as $\Delta Y_t = Y_t - Y_{t-1}$. </li>
                <li>Higher-order differencing can be applied if trends persist, such as the second difference $\Delta^2 Y_t = \Delta Y_t - \Delta Y_{t-1}$. </li>
                <li>Differencing is especially useful for converting non-stationary data into a stationary form. </li>
            </ul>
            <p><strong>Logarithmic Transformations</strong></p>
            <ul>
                <li>Logarithmic transformations stabilize variance when variability increases with the magnitude of the data. </li>
                <li>The natural logarithm (or another logarithm base) is applied to each data point in the series. </li>
                <li>For a series $Y_t$, the transformed series becomes $\log(Y_t)$. </li>
                <li>This method is particularly effective for data exhibiting exponential growth or multiplicative seasonality. </li>
                <li>It also compresses the scale of large values, making trends easier to identify visually. </li>
            </ul>
            <p><strong>Detrending</strong></p>
            <ul>
                <li>Detrending removes long-term trends from data to highlight short-term fluctuations. </li>
                <li>A trend line is fitted to the data and then subtracted from the original series. </li>
                <li>For a linear trend $Y_t = \alpha + \beta t + \epsilon_t$, the detrended series is computed as $Y_t - (\alpha + \beta t)$. </li>
                <li>Non-linear trends can be removed by fitting polynomial or exponential functions. </li>
                <li>Detrending helps in isolating cyclical or seasonal patterns from broader trends. </li>
            </ul>
            <h4 id="the-random-walk-model">The Random Walk Model</h4>
            <p>A <strong>random walk</strong> is a classic example of a non-stationary process. It is characterized by each value being a random step away from the previous value, making its statistical properties dependent on time.</p>
            <h4 id="definition">Definition</h4>
            <p>A random walk can be mathematically expressed as:</p>
            <p>$$
                X_t = X_{t-1} + Z_t
                $$</p>
            <p>where:</p>
            <ul>
                <li>$X_t$ is the value of the series at time $t$.</li>
                <li>$Z_t$ is a white noise term (a sequence of uncorrelated random variables with mean zero and constant variance).</li>
            </ul>
            <h4 id="properties-of-a-random-walk">Properties of a Random Walk</h4>
            <p>I. <strong>Mean</strong></p>
            <p>The expected value (mean) of $X_t$ grows over time:</p>
            <p>$$
                E[X_t] = t \cdot \mu
                $$</p>
            <p>where $\mu$ is the mean of $Z_t$.</p>
            <p>II. <strong>Variance</strong></p>
            <p>The variance of $X_t$ increases linearly with time:</p>
            <p>$$
                \text{Var}(X_t) = t \cdot \sigma^2
                $$</p>
            <p>where $\sigma^2$ is the variance of $Z_t$.</p>
            <p>Because both the mean and variance depend on time, a random walk is inherently <strong>non-stationary</strong>.</p>
            <h4 id="transforming-a-random-walk-into-a-stationary-series">Transforming a Random Walk into a Stationary Series</h4>
            <p>To utilize statistical models that require stationarity, it's necessary to transform a random walk into a stationary series. This is achieved through <strong>differencing</strong>.</p>
            <h5>Differencing Operator</h5>
            <p>The <strong>difference operator</strong> ( \Delta ) is used to remove trends and stabilize the mean of a time series. It is defined as:</p>
            <p>$$
                \Delta X_t = X_t - X_{t-1} = Z_t
                $$</p>
            <p>Applying the difference operator to a random walk:</p>
            <p>I. <strong>First Difference:</strong></p>
            <p>$$
                \Delta X_t = X_t - X_{t-1} = Z_t
                $$</p>
            <p>Since $Z_t$ is white noise, the differenced series $\Delta X_t$ is <strong>stationary</strong>.</p>
            <p>II. <strong>Resulting Series:</strong></p>
            <ul>
                <li>The differenced series has a constant mean (assuming $Z_t$ has mean zero).</li>
                <li>The variance is constant over time.</li>
                <li>There is no autocorrelation in the differenced series if $Z_t$ is truly white noise.</li>
            </ul>
            <p>Thus, differencing a random walk transforms it into a <strong>stationary white noise</strong> series, which is suitable for modeling with techniques that assume stationarity.</p>
            <h5>Example of Differencing in Python</h5>
            <p>We can use Python to difference a non-stationary series like a random walk:</p>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# Simulate a random walk
np.random.seed(42)
N = 1000
Z = np.random.normal(0, 1, N)
X = np.cumsum(Z)  #

 Random walk as cumulative sum of white noise

# Apply differencing to make it stationary
diff_X = np.diff(X)

# Plot the original random walk and the differenced series
plt.figure(figsize=(10, 6))
plt.subplot(2, 1, 1)
plt.plot(X, label='Random Walk')
plt.title('Random Walk (Non-Stationary)')
plt.grid(True)

plt.subplot(2, 1, 2)
plt.plot(diff_X, label='Differenced Series')
plt.title('Differenced Series (Stationary)')
plt.grid(True)

plt.tight_layout()
plt.show()</code></pre>
            </div>
            </p>
            <p>I. Simulating a Random Walk:</p>
            <ul>
                <li>A random walk is generated by taking the cumulative sum of normally distributed random numbers. This produces a series where each value depends on the previous one plus some random noise.</li>
                <li>The random walk is non-stationary because it lacks a constant mean and variance over timeâ€”it drifts unpredictably.</li>
            </ul>
            <p>II. Differencing:</p>
            <ul>
                <li>Differencing transforms the non-stationary series into a stationary one by subtracting the previous observation from the current one. This removes any trend or long-term structure in the data.</li>
                <li>In Python, this is done using <code>np.diff()</code>, which takes the difference between consecutive elements of the series.</li>
            </ul>
            <p>The result plot would look like the following:</p>
            <p><img alt="differenced_random_walk" src="https://github.com/user-attachments/assets/2cebba56-7d3b-470c-9511-56d617be7159" /></p>
            <p>In this plot, the upper section shows the random walk (non-stationary), while the lower section shows the differenced series (stationary). Differencing removes the trend from the original series, making it easier to model and predict future values.</p>
        </article-section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol><a href="#stationarity-in-time-series">Stationarity in Time Series</a>
                <ol>
                    <li><a href="#why-do-we-care-about-stationarity-">Why Do We Care About Stationarity?</a></li>
                    <li><a href="#intuition-for-stationary-time-series">Intuition for Stationary Time Series</a></li>
                    <li><a href="#strict-stationarity">Strict Stationarity</a></li>
                    <li><a href="#weak-second-order-stationarity">Weak (Second-Order) Stationarity</a></li>
                    <li><a href="#properties-of-stationary-processes">Properties of Stationary Processes</a>
                        <ol>
                            <li><a href="#mean-variance-and-autocovariance-functions">Mean, Variance, and Autocovariance Functions</a></li>
                            <li><a href="#autocorrelation-and-bounds">Autocorrelation and Bounds</a></li>
                        </ol>
                    </li>
                    <li><a href="#examples-of-stationary-processes">Examples of Stationary Processes</a>
                        <ol>
                            <li><a href="#white-noise">White Noise</a></li>
                            <li><a href="#moving-average-ma-process">Moving Average (MA) Process</a></li>
                        </ol>
                    </li>
                    <li><a href="#non-stationary-processes">Non-Stationary Processes</a>
                        <ol>
                            <li><a href="#trend-stationary-processes">Trend-Stationary Processes</a></li>
                            <li><a href="#difference-stationary-processes">Difference-Stationary Processes</a></li>
                            <li><a href="#transformations-to-achieve-stationarity">Transformations to Achieve Stationarity</a></li>
                            <li><a href="#the-random-walk-model">The Random Walk Model</a></li>
                            <li><a href="#definition">Definition</a></li>
                            <li><a href="#properties-of-a-random-walk">Properties of a Random Walk</a></li>
                            <li><a href="#transforming-a-random-walk-into-a-stationary-series">Transforming a Random Walk into a Stationary Series</a></li>
                        </ol>
                    </li>
                </ol>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li>Basic Concepts<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/axioms_of_probability.html">Axioms of Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/bayes_theorem.html">Bayes Theorem</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/bayesian_vs_frequentist.html">Bayesian vs Frequentist</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/conditional_probability.html">Conditional Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/descriptive_statistics.html">Descriptive Statistics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/geometric_probability.html">Geometric Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/introduction_to_probability.html">Introduction to Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/introduction_to_statistics.html">Introduction to Statistics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/probability_tree.html">Probability Tree</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/standard_error_and_lln.html">Standard Error and Lln</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/total_probability.html">Total Probability</a></li>
                        </ol>
                    </li>
                    <li>Probability Distributions<ol>
                            <li>Continuous Distributions<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/beta_distribution.html">Beta Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/chi_square_distribution.html">Chi Square Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/exponential_distribution.html">Exponential Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/f_distribution.html">F Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/gamma_distribution.html">Gamma Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/log_normal_distribution.html">Log Normal Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/normal_distribution.html">Normal Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/student_t_distribution.html">Student T Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/uniform_distribution.html">Uniform Distribution</a></li>
                                </ol>
                            </li>
                            <li>Discrete Distributions<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/binomial_distribution.html">Binomial Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/geometric_distribution.html">Geometric Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/negative_binomial_distribution.html">Negative Binomial Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/poisson_distribution.html">Poisson Distribution</a></li>
                                </ol>
                            </li>
                            <li>Intro<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/central_limit_theorem.html">Central Limit Theorem</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/introduction_to_distributions.html">Introduction to Distributions</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/normal_curve_and_z_score.html">Normal Curve and z Score</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/statistical_moments.html">Statistical Moments</a></li>
                                </ol>
                            </li>
                        </ol>
                    </li>
                    <li>Correlation and Regression<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/correlation.html">Correlation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/covariance.html">Covariance</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/logistic_regression.html">Logistic Regression</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/metrics.html">Metrics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/multiple_regression.html">Multiple Regression</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/simple_linear_regression.html">Simple Linear Regression</a></li>
                        </ol>
                    </li>
                    <li>Statistical Inference<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/analysis_of_categorical_data.html">Analysis of Categorical Data</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/analysis_of_variance.html">Analysis of Variance</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/confidence_intervals.html">Confidence Intervals</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/hypothesis_testing.html">Hypothesis Testing</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/multiple_comparisons.html">Multiple Comparisons</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/null_hypothesis.html">Null Hypothesis</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/resampling.html">Resampling</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/type_i_and_type_ii_errors.html">Type i and Type Ii Errors</a></li>
                        </ol>
                    </li>
                    <li>Time Series Analysis<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/arima_models.html">Arima Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autocorrelation_function.html">Autocorrelation Function</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autocovariance_function.html">Autocovariance Function</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autoregressive_models.html">Autoregressive Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/backward_shift_operator.html">Backward Shift Operator</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/difference_equations.html">Difference Equations</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/forecasting.html">Forecasting</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/invertibility.html">Invertibility</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/moving_average_models.html">Moving Average Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/random_walk.html">Random Walk</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/seasonality_and_trends.html">Seasonality and Trends</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/series.html">Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/stationarity.html">Stationarity</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/statistical_moments_and_time_series.html">Statistical Moments and Time Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/time_series.html">Time Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/time_series_modeling.html">Time Series Modeling</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/yule_walker_equations.html">Yule Walker Equations</a></li>
                        </ol>
                    </li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>Thank you for visiting my personal website. All content here is free to use, but please remember to be respectful and avoid any misuse of the site. If youâ€™d like to get in touch, feel free to reach out via my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or connect with me on <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have technical questions or ideas to share. Wishing you all the best and a fantastic life ahead!</p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../../app.js"></script>
    </footer>
    <div id="pdf-spinner-overlay">
        <div class="spinner"></div>
    </div>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>