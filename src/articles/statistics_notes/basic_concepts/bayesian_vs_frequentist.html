<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Bayesian vs Frequentist Statistics</title>
    <meta content="Bayesian and frequentist statistics are two distinct approaches to statistical inference." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper"><article-section id="article-body">
            <div class="article-action-buttons"><button class="btn-suggest-edit" title="Suggest Edit">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M4 21h4l11-11-4-4L4 17v4z" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button><button class="btn-create-issue" title="Create Issue">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"></circle>
                        <line stroke="currentColor" stroke-width="2" x1="12" x2="12" y1="8" y2="12"></line>
                        <circle cx="12" cy="16" fill="currentColor" r="1"></circle>
                    </svg>
                </button><button class="btn-download" title="Download">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M12 5v14m0 0l-6-6m6 6l6-6" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button></div>
            <p style="text-align: right;"><i>Last modified: May 03, 2025</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="bayesian-vs-frequentist-statistics">Bayesian vs Frequentist Statistics</h2>
            <p>Bayesian and frequentist statistics are two distinct approaches to statistical inference. Both approaches aim to make inferences about an underlying population based on sample data. However, the way they interpret probability and handle uncertainty is fundamentally different.</p>
            <h3 id="frequentist-statistics">Frequentist Statistics</h3>
            <ul>
                <li>Frequentist statistics operates under the assumption that parameters in a population are fixed but unknown, such as the true mean, which remains constant even though its value is not directly observed. </li>
                <li>Confidence intervals are constructed by procedures that <em>cover</em> the true parameter value a known proportion of the time in repeated sampling. For example, a 95 % confidence interval means that if an experiment were repeated many times, 95 % of the intervals produced by the same method would capture the true parameter value. (The parameter itself is fixed; it is the interval that varies from sample to sample.) </li>
                <li>Null hypothesis testing is a core component of frequentist analysis. It tests observed data against a null hypothesis, which typically asserts no effect or no difference, such as the hypothesis that two groups do not differ significantly. </li>
            </ul>
            <h4 id="mathematical-foundations">Mathematical Foundations</h4>
            <ul>
                <li>Frequentist probability is defined in terms of long-run frequency. This interpretation suggests that, in a large number of trials, the probability of an eventâ€”like getting heads in a coin tossâ€”reflects its relative frequency over time. </li>
                <li>Test statistics are used to measure how unusual the observed data are compared to what would be expected under the null hypothesis. The extremeness of the data is quantified by the p-value, which indicates the probability of obtaining such data if the null hypothesis is true. A small p-value suggests the observed data are inconsistent with the null hypothesis. </li>
            </ul>
            <h4 id="advantages">Advantages</h4>
            <ul>
                <li>Frequentist methods are known for their simplicity and accessibility, making them easier for non-experts to understand and apply in practical settings. </li>
                <li>These methods are well-suited for large sample sizes, often yielding highly reliable results when ample data are available. </li>
                <li>Frequentist statistics offer standardized methods, supported by extensive tables and procedures, which have been widely adopted in fields like medicine and social sciences. </li>
            </ul>
            <h4 id="limitations">Limitations</h4>
            <ul>
                <li>One major limitation is that frequentist methods can produce misleading results when applied to small sample sizes or complex data structures, potentially leading to inaccurate conclusions. </li>
                <li>Frequentist statistics do not incorporate prior knowledge or beliefs about the parameters; they rely solely on the data at hand, potentially missing valuable context. </li>
                <li>The binary decision-making process, where one either rejects or fails to reject the null hypothesis, can be overly simplistic, often ignoring the nuance and depth of the data, leading to a lack of deeper interpretation. </li>
            </ul>
            <h4 id="example">Example</h4>
            <p>Let's assume we have a population of ten items, where <strong>X</strong> represents the attribute we are looking for and <strong>O</strong> represents the absence of this attribute.</p>
            <p>Population:</p>
            <p>
            <div>
                <pre><code class="language-shell">O O X O O O X X O X</code></pre>
            </div>
            </p>
            <p>We take a sample of 4 randomly from this population:</p>
            <p>
            <div>
                <pre><code class="language-shell">Sample:
X O O X</code></pre>
            </div>
            </p>
            <p>A frequentist would report the <strong>sample proportion</strong> of the attribute, which is 50 % (2 out of 4), as the maximum-likelihood <strong>point estimate</strong> of the population proportion. They might also calculate its standard error and construct a confidence interval before applying the resulting estimate to future inference.</p>
            <p>
            <table>
                <tr>
                    <td>Step</td>
                    <td>Equation</td>
                    <td>Plugging the numbers</td>
                </tr>
                <tr>
                    <td><strong>Point estimate</strong></td>
                    <td>$\hat p = x/n$</td>
                    <td>$\hat p = 2/4 = 0.50$</td>
                </tr>
                <tr>
                    <td><strong>Standard error</strong></td>
                    <td>$SE(\hat p) = \sqrt{\hat p(1-\hat p)/n}$</td>
                    <td>$\sqrt{0.5,(1-0.5)/4} = \sqrt{0.0625} = 0.25$</td>
                </tr>
                <tr>
                    <td><strong>95 % confidence interval</strong><br />(Wald large-sample)</td>
                    <td>$\hat p \pm z_{0.975},SE(\hat p)$, $z_{0.975}=1.96$</td>
                    <td>$0.50 \pm 1.96\times0.25 = 0.50 \pm 0.49$ â‡’ <strong>CI â‰ˆ [0.01, 0.99]</strong></td>
                </tr>
                <tr>
                    <td><strong>Null-hypothesis test</strong><br />($H_0: p = p_0$)</td>
                    <td>$z = (\hat p - p_0)/\sqrt{p_0(1-p_0)/n}$</td>
                    <td>For $p_0=0.5$:<br />$z = (0.50-0.50)/\sqrt{0.5\cdot0.5/4} = 0$<br />p-value = 1 (fail to reject)</td>
                </tr>
            </table>
            </p>
            <h3 id="bayesian-statistics">Bayesian Statistics</h3>
            <ul>
                <li>Bayesian statistics treats parameters as random variables with associated probability distributions, reflecting the uncertainty about their true values rather than considering them fixed. </li>
                <li>A <strong>prior distribution</strong> represents pre-existing knowledge or beliefs about a parameter, formulated as a probability distribution that captures this initial understanding before observing new data. </li>
                <li>The <strong>likelihood function</strong> expresses the probability of observing the data given different parameter values. This function plays a key role in updating beliefs as new data become available. </li>
                <li>The <strong>posterior distribution</strong> is the updated probability distribution after combining the prior distribution and the likelihood. It forms the core of Bayesian inference, offering a new perspective on the parameter based on the data. </li>
            </ul>
            <h4 id="mathematical-framework">Mathematical Framework</h4>
            <p><strong>Bayes' Theorem</strong> serves as the foundation of Bayesian analysis. It mathematically updates the prior belief in light of new evidence by using the formula: </p>
            <p>$\text{Posterior} \propto \text{Likelihood} \times \text{Prior}$</p>
            <p>reflecting how new data influence prior knowledge. </p>
            <p>In Bayesian statistics, probability is interpreted as a <strong>degree of belief</strong> or certainty about an event or parameter, rather than as a long-run frequency of occurrence as in frequentist statistics. </p>
            <h4 id="incorporating-prior-knowledge">Incorporating Prior Knowledge</h4>
            <ul>
                <li>Bayesian methods allow for the use of general-knowledge priors, even without specific domain expertise. For example, in a study on snake lifespans, a prior could favor a lifespan around 10 years, rather than something implausible like 1000 years, based on biological understanding. </li>
                <li>As new data are collected, Bayesian inference updates the prior to form the posterior, which reflects the combined influence of prior knowledge and new observations. For instance, if new research suggests that certain snakes live longer than expected, the posterior distribution adjusts to reflect this new evidence alongside previous beliefs. </li>
            </ul>
            <h4 id="advantages">Advantages</h4>
            <ul>
                <li>One of the key strengths of Bayesian statistics is its ability to incorporate prior knowledge or expertise into the analysis, making it particularly valuable in situations where data are limited or difficult to obtain. </li>
                <li>Bayesian methods often lead to more intuitive and direct interpretations, especially in scenarios where the data are complex or the sample size is small, as the results are framed in terms of probabilities and uncertainties. </li>
                <li>Bayesian analysis provides a <strong>probabilistic</strong> understanding of estimates, allowing for a more nuanced and detailed interpretation of uncertainty around the parameter estimates (credible intervals give the probability that the parameter lies in a specified range). </li>
                <li>These methods offer great flexibility in handling complex models and uncertainty, making them well-suited for sophisticated models and smaller datasets. </li>
            </ul>
            <h4 id="limitations">Limitations</h4>
            <ul>
                <li>Bayesian analysis is computationally intensive, requiring significant resources to calculate posterior distributionsâ€”particularly when dealing with complex hierarchical models or large datasetsâ€”although Markov-chain Monte Carlo and variational inference have made many such analyses practical. </li>
                <li>The choice of prior can heavily influence the results, potentially introducing bias if the prior is not carefully selected or justified based on solid reasoning. </li>
                <li>Implementing Bayesian methods requires a more advanced understanding of statistical principles, making them harder to apply and interpret without specialized knowledge. </li>
            </ul>
            <h4 id="example">Example</h4>
            <p>Assume we have a <strong>Beta(1, 1)</strong> prior, which is uniform on the interval $[0,1]$, expressing equal belief in any value of the probability of a coin landing heads (H) or tails (T).</p>
            <p>
            <div>
                <pre><code class="language-shell">Prior:
H: 0.5, T: 0.5</code></pre>
            </div>
            </p>
            <p>Now we flip the coin 3 times and observe all heads:</p>
            <p>
            <div>
                <pre><code class="language-shell">Data:
H H H</code></pre>
            </div>
            </p>
            <p>Updating the Beta(1, 1) prior with these data yields the posterior <strong>Beta(4, 1)</strong>. The posterior mean is $4/5 = 0.8$:</p>
            <p>
            <div>
                <pre><code class="language-shell">Posterior:
H: 0.8, T: 0.2</code></pre>
            </div>
            </p>
            <p>This demonstrates how the Bayesian approach systematically updates beliefs (probabilities) based on new data.</p>
            <p>
            <table>
                <tr>
                    <td>Step</td>
                    <td>Equation</td>
                    <td>Plugging the numbers</td>
                </tr>
                <tr>
                    <td><strong>Prior density</strong></td>
                    <td>$f(p)=dfrac{\Gamma(a+b)}{\Gamma(a),\Gamma(b)},p^{a-1}(1-p)^{b-1}$</td>
                    <td>$a=b=1\implies f(p)=1$ for $p\in[0,1]$</td>
                </tr>
                <tr>
                    <td><strong>Likelihood</strong></td>
                    <td>$L(p)=\binom{n}{x}p^x(1-p)^{n-x}$</td>
                    <td>$\binom{3}{3}p^3(1-p)^0=p^3$</td>
                </tr>
                <tr>
                    <td><strong>Posterior</strong></td>
                    <td>$p\mid\mathrm{data}\sim\mathrm{Beta}(a+x,b+n-x)$</td>
                    <td>$\mathrm{Beta}(1+3,1+0)=\mathrm{Beta}(4,1)$</td>
                </tr>
                <tr>
                    <td><strong>Posterior mean</strong></td>
                    <td>$\mathbb{E}[p\mid\mathrm{data}]=\frac{a+x}{a+b+n}$</td>
                    <td>$\frac{4}{5}=0.8$</td>
                </tr>
                <tr>
                    <td><strong>95 % credible interval</strong></td>
                    <td>Central interval between the 0.025 and 0.975 quantiles of $\mathrm{Beta}(4,1)$</td>
                    <td>$\approx[0.50,0.97]$</td>
                </tr>
            </table>
            </p>
            <h3 id="bayesian-vs-frequentist-convergence">Bayesian vs Frequentist Convergence</h3>
            <p>As the sample size increases, Bayesian and frequentist methods often produce similar numerical resultsâ€”<em>provided the prior is non-informative or weakly informative</em>. When using uninformed or non-informative priors (indicating a lack of strong prior knowledge), the results from Bayesian and frequentist approaches are frequently comparable, if not identical. However, the <strong>interpretation</strong> of these results can still differ between the two frameworks: a frequentist 95 % confidence interval has 95 % coverage in repeated sampling, whereas a Bayesian 95 % credible interval contains the parameter with 95 % posterior probability.</p>
            <h4 id="when-do-they-diverge-">When Do They Diverge?</h4>
            <ul>
                <li>In cases involving complex models or smaller sample sizes, Bayesian and frequentist methods may produce significantly different outcomes. Bayesian approaches may perform better in these scenarios because they can incorporate prior information, which helps when data are limited or the model is sophisticated. </li>
                <li>The specific context of the problemâ€”such as the presence of strong prior information or a complicated data structureâ€”can also lead to divergence between the two methods. Bayesian methods might yield more nuanced results in certain situations where frequentist methods may struggle.</li>
            </ul>
            <h4 id="example-frequentist-vs-bayesian-mean-estimation">Example: Frequentist vs. Bayesian Mean Estimation</h4>
            <ol>
                <li>We generated synthetic data consisting of 100 random values drawn from a normal distribution with a mean of 5 and a standard deviation of 2. This dataset simulates real-world measurements with inherent variability around the central value of 5. The goal was to compare how the frequentist and Bayesian approaches estimate the mean and uncertainty of this data. </li>
                <li>Using the <strong>frequentist approach</strong>, we calculated the sample mean and constructed a 95 % confidence interval (CI). The mean came out to be approximately 4.79, and the confidence interval was between 4.44 and 5.15. This interval suggests that, if we repeated this experiment many times, 95 % of the calculated intervals would contain the true population mean. </li>
                <li>In the <strong>Bayesian approach</strong>, we incorporated prior knowledge about the data by assuming a prior mean of 5 and a prior variance of 1. Combining this prior belief with the observed data, we calculated a posterior mean of 4.80. The 95 % credible interval, which reflects where the true mean is likely to lie <em>given both the prior and observed data</em>, ranged from 4.42 to 5.18. This interval accounts for both the prior information and the variability in the data. </li>
            </ol>
            <p><img alt="output(11)" src="https://github.com/user-attachments/assets/4ba1be0a-21d3-4627-ad7e-f357f5453487" /></p>
            <p>The analysis results are as follows:</p>
            <ul>
                <li><strong>Frequentist Mean:</strong> 4.79, with a 95 % confidence interval of (4.44, 5.15). </li>
                <li><strong>Bayesian Mean:</strong> 4.80, with a 95 % credible interval of (4.42, 5.18). </li>
            </ul>
        </article-section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol><a href="#bayesian-vs-frequentist-statistics">Bayesian vs Frequentist Statistics</a>
                <ol>
                    <li><a href="#frequentist-statistics">Frequentist Statistics</a>
                        <ol>
                            <li><a href="#mathematical-foundations">Mathematical Foundations</a></li>
                            <li><a href="#advantages">Advantages</a></li>
                            <li><a href="#limitations">Limitations</a></li>
                            <li><a href="#example">Example</a></li>
                        </ol>
                    </li>
                    <li><a href="#bayesian-statistics">Bayesian Statistics</a>
                        <ol>
                            <li><a href="#mathematical-framework">Mathematical Framework</a></li>
                            <li><a href="#incorporating-prior-knowledge">Incorporating Prior Knowledge</a></li>
                            <li><a href="#advantages">Advantages</a></li>
                            <li><a href="#limitations">Limitations</a></li>
                            <li><a href="#example">Example</a></li>
                        </ol>
                    </li>
                    <li><a href="#bayesian-vs-frequentist-convergence">Bayesian vs Frequentist Convergence</a>
                        <ol>
                            <li><a href="#when-do-they-diverge-">When Do They Diverge?</a></li>
                            <li><a href="#example-frequentist-vs-bayesian-mean-estimation">Example: Frequentist vs. Bayesian Mean Estimation</a></li>
                        </ol>
                    </li>
                </ol>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li>Basic Concepts<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/axioms_of_probability.html">Axioms of Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/bayes_theorem.html">Bayes Theorem</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/bayesian_vs_frequentist.html">Bayesian vs Frequentist</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/conditional_probability.html">Conditional Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/descriptive_statistics.html">Descriptive Statistics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/geometric_probability.html">Geometric Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/introduction_to_probability.html">Introduction to Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/introduction_to_statistics.html">Introduction to Statistics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/probability_tree.html">Probability Tree</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/standard_error_and_lln.html">Standard Error and Lln</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/total_probability.html">Total Probability</a></li>
                        </ol>
                    </li>
                    <li>Probability Distributions<ol>
                            <li>Continuous Distributions<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/beta_distribution.html">Beta Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/chi_square_distribution.html">Chi Square Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/exponential_distribution.html">Exponential Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/f_distribution.html">F Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/gamma_distribution.html">Gamma Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/log_normal_distribution.html">Log Normal Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/normal_distribution.html">Normal Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/student_t_distribution.html">Student T Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/uniform_distribution.html">Uniform Distribution</a></li>
                                </ol>
                            </li>
                            <li>Discrete Distributions<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/binomial_distribution.html">Binomial Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/geometric_distribution.html">Geometric Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/negative_binomial_distribution.html">Negative Binomial Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/poisson_distribution.html">Poisson Distribution</a></li>
                                </ol>
                            </li>
                            <li>Intro<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/central_limit_theorem.html">Central Limit Theorem</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/introduction_to_distributions.html">Introduction to Distributions</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/normal_curve_and_z_score.html">Normal Curve and z Score</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/statistical_moments.html">Statistical Moments</a></li>
                                </ol>
                            </li>
                        </ol>
                    </li>
                    <li>Correlation and Regression<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/correlation.html">Correlation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/covariance.html">Covariance</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/logistic_regression.html">Logistic Regression</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/metrics.html">Metrics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/multiple_regression.html">Multiple Regression</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/simple_linear_regression.html">Simple Linear Regression</a></li>
                        </ol>
                    </li>
                    <li>Statistical Inference<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/analysis_of_categorical_data.html">Analysis of Categorical Data</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/analysis_of_variance.html">Analysis of Variance</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/confidence_intervals.html">Confidence Intervals</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/hypothesis_testing.html">Hypothesis Testing</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/multiple_comparisons.html">Multiple Comparisons</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/null_hypothesis.html">Null Hypothesis</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/resampling.html">Resampling</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/type_i_and_type_ii_errors.html">Type i and Type Ii Errors</a></li>
                        </ol>
                    </li>
                    <li>Time Series Analysis<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/arima_models.html">Arima Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autocorrelation_function.html">Autocorrelation Function</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autocovariance_function.html">Autocovariance Function</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autoregressive_models.html">Autoregressive Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/backward_shift_operator.html">Backward Shift Operator</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/difference_equations.html">Difference Equations</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/forecasting.html">Forecasting</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/invertibility.html">Invertibility</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/moving_average_models.html">Moving Average Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/random_walk.html">Random Walk</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/seasonality_and_trends.html">Seasonality and Trends</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/series.html">Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/stationarity.html">Stationarity</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/statistical_moments_and_time_series.html">Statistical Moments and Time Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/time_series.html">Time Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/time_series_modeling.html">Time Series Modeling</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/yule_walker_equations.html">Yule Walker Equations</a></li>
                        </ol>
                    </li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>Thank you for visiting my personal website. All content here is free to use, but please remember to be respectful and avoid any misuse of the site. If youâ€™d like to get in touch, feel free to reach out via my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or connect with me on <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have technical questions or ideas to share. Wishing you all the best and a fantastic life ahead!</p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../../app.js"></script>
    </footer>
    <div id="pdf-spinner-overlay">
        <div class="spinner"></div>
    </div>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>