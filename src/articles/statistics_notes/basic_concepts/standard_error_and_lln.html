<!DOCTYPE html>

<html lang="en">
<head>
<script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
<meta charset="utf-8"/>
<title>Standard Error and Law of Large Numbers</title>
<meta content="Expected Value (E), also known as the mean, is the long-run average of a random variable, representing the value we anticipate on average from repeated random draws from a population." name="description"/>
<meta content="Adam Djellouli" name="author"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet"/>
<link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon"/>
<link href="../../../resources/style.css" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>
<body><nav aria-label="Main navigation">
<a class="logo" href="https://adamdjellouli.com">
<img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG"/>
</a>
<input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox"/>
<ul aria-labelledby="navbar-toggle" role="menu">
<li role="menuitem">
<a href="../../../index.html" title="Go to Home Page"> Home </a>
</li>
<li role="menuitem">
<a class="active" href="../../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
</li>
<li role="menuitem">
<a href="../../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
</li>
<li role="menuitem">
<a href="../../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
</li>
<li role="menuitem">
<a href="../../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
</li>
<li>
<script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
<div class="gcse-search"></div>
</li>
<li>
<button aria-label="Toggle dark mode" id="dark-mode-button"></button>
</li>
</ul>
</nav>
<div id="article-wrapper"><article-section id="article-body">
<p style="text-align: right;"><i>Last modified: December 25, 2025</i></p>
<p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
<h2 id="standard-error-and-law-of-large-numbers">Standard Error and Law of Large Numbers</h2>
<h3 id="expected-value">Expected Value</h3>
<p><strong>Expected Value (E)</strong>, also known as the mean, is the long-run average of a random variable, representing the value we anticipate on average from repeated random draws from a population.</p>
<p>I. For a single random draw from a population with mean $\mu$, the expected value is given by:</p>
<p>$$
E(X) = \mu
$$</p>
<p>II. For a sample of size $n$, the expected value of the sample mean $\bar{x}_n$ is equal to the population mean:</p>
<p>$$
E(\bar{x}_n) = \mu
$$</p>
<p>This follows from the property that the sample mean is an unbiased estimator of the population mean.</p>
<h3 id="standard-error">Standard Error</h3>
<p><strong>Standard Error (SE)</strong> quantifies the dispersion or variability of a sample statistic, such as the sample mean, from its expected value. It reflects the precision of the sample statistic as an estimator of the population parameter.</p>
<p>The standard error of the sample mean is defined as:</p>
<p>$$
SE(\bar{x}_n) = \frac{\sigma}{\sqrt{n}}
$$</p>
<p>where:</p>
<ul>
<li>$\sigma$ is the population standard deviation, and</li>
<li>$n$ is the sample size.</li>
</ul>
<p>As $n$ increases, $SE(\bar{x}_n)$ decreases, implying that larger samples yield more precise estimates of the population mean. The relationship between the standard error and the sample size follows an inverse square root law, meaning that quadrupling the sample size halves the standard error. This emphasizes the diminishing returns effect in terms of accuracy gained by increasing the sample size.</p>
<h4 id="standard-error-for-the-sum-and-percentages">Standard Error for the Sum and Percentages</h4>
<p>When dealing with the sum of random draws rather than the sample mean, the expected value and standard error follow these formulas:</p>
<p>I. <strong>Expected Value for the Sum</strong></p>
<p>The expected value of the sum is the total number of draws $n$ multiplied by the population mean $\mu$:</p>
<p>$$
E(S_n) = n \mu
$$</p>
<p>For example, if $\mu = 5$ and $n = 100$, then:</p>
<p>$$
E(S_n) = 100 \times 5 = 500
$$</p>
<p>This means that, on average, the sum of 100 random draws would be 500.</p>
<p>II. <strong>Standard Error for the Sum</strong></p>
<p>The standard error of the sum increases with the square root of the sample size and is calculated as:</p>
<p>$$
SE(S_n) = \sqrt{n} \sigma
$$</p>
<p>where $\sigma$ is the population standard deviation. For example, if $\sigma = 2$ and $n = 100$, then:</p>
<p>$$
SE(S_n) = \sqrt{100} \times 2 = 10 \times 2 = 20
$$</p>
<p>This means that the standard deviation of the sum of 100 draws is 20, reflecting the expected variability in the sum.</p>
<p>Together, the expected sum and standard error give us a range of plausible values for the sum:</p>
<p>$$
S_n \approx 500 \pm 20
$$</p>
<h4 id="standard-error-for-percentages">Standard Error for Percentages</h4>
<p>Percentages are often used in scenarios where the variable is binary, such as 1 for success and 0 for failure. In this case, you are interested in the percentage of 1s (successes) in a sample. The formulas for the expected value and standard error of percentages are closely related to those for the mean and sum but scaled to represent percentages.</p>
<p>I. <strong>Expected Value for Percentages</strong></p>
<p>The expected value of the percentage of successes is simply the population proportion $\mu$ multiplied by 100 to convert it to a percentage:</p>
<p>$$
E(\text{percentage of 1s}) = \mu \times 100\%
$$</p>
<p>For example, if $\mu = 0.6$ (i.e., 60% of the population are successes), then:</p>
<p>$$
E(\text{percentage of 1s}) = 0.6 \times 100\% = 60\%
$$</p>
<p>This means we expect 60% of the sample to be successes.</p>
<p>II. <strong>Standard Error for Percentages</strong></p>
<p>The standard error for the percentage of 1s is calculated as:</p>
<p>$$
SE(\text{percentage of 1s}) = \frac{\sigma}{\sqrt{n}} \times 100\%
$$</p>
<p>where $\sigma$ is the population standard deviation for the binary outcome. In the case of a binary variable, the population standard deviation is given by:</p>
<p>$$
\sigma = \sqrt{\mu(1 - \mu)}
$$</p>
<p>For example, if $\mu = 0.6$, then:</p>
<p>$$
\sigma = \sqrt{0.6 \times (1 - 0.6)} = \sqrt{0.6 \times 0.4} = \sqrt{0.24} \approx 0.49
$$</p>
<p>Now, if we take a sample of size $n = 100$, the standard error is:</p>
<p>$$
SE(\text{percentage of 1s}) = \frac{0.49}{\sqrt{100}} \times 100\% = \frac{0.49}{10} \times 100\% = 4.9\%
$$</p>
<p>This means that the percentage of successes will vary by approximately $\pm 4.9\%$ from the expected value of 60%. Therefore, the percentage of successes will typically fall within the range:</p>
<p>$$
\text{Percentage of 1s} \approx 60\% \pm 4.9\%
$$</p>
<h4 id="law-of-large-numbers-lln-">Law of Large Numbers (LLN)</h4>
<p>The Law of Large Numbers (LLN) states that as the sample size $n$ increases, the sample mean $\bar{x}_n$ will tend to converge towards the population mean $\mu$. In other words, the larger the sample size, the closer the sample mean gets to the true mean of the population.</p>
<p>Mathematically, as $n \to \infty$:</p>
<p>$$
\bar{x}_n \to \mu
$$</p>
<p>This principle ensures that with a sufficiently large sample, the sample mean becomes a reliable estimator of the population mean.</p>
<ul>
<li><strong>Impact on Standard Error</strong>:</li>
</ul>
<p>The <strong>Standard Error (SE)</strong> of the sample mean $\bar{x}_n$, which measures the variability of the sample mean around the population mean, decreases as the sample size increases. The SE is given by:</p>
<p>$$
SE(\bar{x}_n) = \frac{\sigma}{\sqrt{n}}
$$</p>
<p>where:</p>
<ul>
<li>$\sigma$ is the population standard deviation, and</li>
<li>$n$ is the sample size.</li>
</ul>
<p>As $n$ increases, $\sqrt{n}$ increases, and thus the SE decreases. This means that larger samples produce more precise and reliable estimates of the population mean.</p>
<p>For example, if $\sigma = 10$ and:</p>
<ul>
<li>For $n = 25$, the standard error is:</li>
</ul>
<p>$$
SE(\bar{x}_{25}) = \frac{10}{\sqrt{25}} = \frac{10}{5} = 2
$$</p>
<ul>
<li>For $n = 100$, the standard error is:</li>
</ul>
<p>$$
SE(\bar{x}_{100}) = \frac{10}{\sqrt{100}} = \frac{10}{10} = 1
$$</p>
<ul>
<li>For $n = 400$, the standard error becomes:</li>
</ul>
<p>$$
SE(\bar{x}_{400}) = \frac{10}{\sqrt{400}} = \frac{10}{20} = 0.5
$$</p>
<p>As the sample size increases from 25 to 400, the standard error decreases from 2 to 0.5, showing that the estimates of the sample mean become more precise with larger samples.</p>
<h4 id="application-to-averages-and-percentages">Application to Averages and Percentages</h4>
<p>The Law of Large Numbers applies to both sample means (averages) and sample proportions (percentages).</p>
<p>I. <strong>For Averages</strong>:
The larger the sample size, the closer the sample mean $\bar{x}_n$ gets to the population mean $\mu$, as shown by the decreasing standard error formula:</p>
<p>$$
SE(\bar{x}_n) = \frac{\sigma}{\sqrt{n}}
$$</p>
<p>II. <strong>For Percentages</strong>:<br/>
The LLN also applies to sample percentages. As the sample size increases, the percentage of successes (or 1s) in the sample gets closer to the true population percentage. The standard error for the sample percentage is given by:</p>
<p>$$
SE(\text{percentage of 1s}) = \frac{\sigma}{\sqrt{n}} \times 100\%
$$</p>
<p>where $\sigma = \sqrt{\mu(1 - \mu)}$ is the standard deviation for binary outcomes.</p>
<p>For example, if $\mu = 0.6$ (i.e., 60% success rate), then $\sigma = \sqrt{0.6 \times 0.4} \approx 0.49$. Now, for different sample sizes, the standard error for percentages becomes:</p>
<ul>
<li>For $n = 25$:</li>
</ul>
<p>$$
SE(\text{percentage}) = \frac{0.49}{\sqrt{25}} \times 100\% = \frac{0.49}{5} \times 100\% = 9.8\%
$$</p>
<ul>
<li>For $n = 100$:</li>
</ul>
<p>$$
SE(\text{percentage}) = \frac{0.49}{\sqrt{100}} \times 100\% = \frac{0.49}{10} \times 100\% = 4.9\%
$$</p>
<ul>
<li>For $n = 400$:</li>
</ul>
<p>$$
SE(\text{percentage}) = \frac{0.49}{\sqrt{400}} \times 100\% = \frac{0.49}{20} \times 100\% = 2.45\%
$$</p>
<p>The standard error decreases as the sample size increases, indicating that larger samples yield more reliable estimates of the population percentage.</p>
<p>III. <strong>Important Note on Sums</strong>:</p>
<p>The Law of Large Numbers <strong>does not apply to sums</strong> in the same way it applies to means and percentages. While the standard error of the sample mean decreases with increasing sample size, the standard error of the sum increases with sample size.</p>
<p>The standard error for the sum $S_n$ is given by:</p>
<p>$$
SE(S_n) = \sqrt{n} \sigma
$$</p>
<p>As $n$ increases, $\sqrt{n}$ increases, which means the variability of the sum increases with sample size. This is because the sum grows proportionally with $n$, and its variability does as well.</p>
<p>For example:</p>
<ul>
<li>If $\sigma = 10$ and $n = 25$:</li>
</ul>
<p>$$
SE(S_{25}) = \sqrt{25} \times 10 = 5 \times 10 = 50
$$</p>
<ul>
<li>If $n = 100$:</li>
</ul>
<p>$$
SE(S_{100}) = \sqrt{100} \times 10 = 10 \times 10 = 100
$$</p>
<ul>
<li>If $n = 400$:</li>
</ul>
<p>$$
SE(S_{400}) = \sqrt{400} \times 10 = 20 \times 10 = 200
$$</p>
<p>The standard error for the sum increases with the square root of the sample size, which reflects the growing total sum's variability.</p>
</article-section><div id="table-of-contents"><h2>Table of Contents</h2><ol><a href="#standard-error-and-law-of-large-numbers">Standard Error and Law of Large Numbers</a><ol><li><a href="#expected-value">Expected Value</a></li><li><a href="#standard-error">Standard Error</a><ol><li><a href="#standard-error-for-the-sum-and-percentages">Standard Error for the Sum and Percentages</a></li><li><a href="#standard-error-for-percentages">Standard Error for Percentages</a></li><li><a href="#law-of-large-numbers-lln-">Law of Large Numbers (LLN)</a></li><li><a href="#application-to-averages-and-percentages">Application to Averages and Percentages</a></li></ol></li></ol></ol><div id="related-articles"><h2>Related Articles</h2><ol><li>Basic Concepts<ol><li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/axioms_of_probability.html">Axioms of Probability</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/bayes_theorem.html">Bayes Theorem</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/bayesian_vs_frequentist.html">Bayesian vs Frequentist</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/conditional_probability.html">Conditional Probability</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/descriptive_statistics.html">Descriptive Statistics</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/geometric_probability.html">Geometric Probability</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/introduction_to_probability.html">Introduction to Probability</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/introduction_to_statistics.html">Introduction to Statistics</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/probability_tree.html">Probability Tree</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/standard_error_and_lln.html">Standard Error and Lln</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/total_probability.html">Total Probability</a></li></ol></li><li>Probability Distributions<ol><li>Continuous Distributions<ol><li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/beta_distribution.html">Beta Distribution</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/chi_square_distribution.html">Chi Square Distribution</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/exponential_distribution.html">Exponential Distribution</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/f_distribution.html">F Distribution</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/gamma_distribution.html">Gamma Distribution</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/log_normal_distribution.html">Log Normal Distribution</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/normal_distribution.html">Normal Distribution</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/student_t_distribution.html">Student T Distribution</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/uniform_distribution.html">Uniform Distribution</a></li></ol></li><li>Discrete Distributions<ol><li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/binomial_distribution.html">Binomial Distribution</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/geometric_distribution.html">Geometric Distribution</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/negative_binomial_distribution.html">Negative Binomial Distribution</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/poisson_distribution.html">Poisson Distribution</a></li></ol></li><li>Intro<ol><li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/central_limit_theorem.html">Central Limit Theorem</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/introduction_to_distributions.html">Introduction to Distributions</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/normal_curve_and_z_score.html">Normal Curve and z Score</a></li></ol></li></ol></li><li>Correlation and Regression<ol><li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/correlation.html">Correlation</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/covariance.html">Covariance</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/logistic_regression.html">Logistic Regression</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/metrics.html">Metrics</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/multiple_regression.html">Multiple Regression</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/simple_linear_regression.html">Simple Linear Regression</a></li></ol></li><li>Time Series Analysis<ol><li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/arima_models.html">Arima Models</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autocorrelation_function.html">Autocorrelation Function</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autocovariance_function.html">Autocovariance Function</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autoregressive_models.html">Autoregressive Models</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/backward_shift_operator.html">Backward Shift Operator</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/difference_equations.html">Difference Equations</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/forecasting.html">Forecasting</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/invertibility.html">Invertibility</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/moving_average_models.html">Moving Average Models</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/random_walk.html">Random Walk</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/seasonality_and_trends.html">Seasonality and Trends</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/series.html">Series</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/stationarity.html">Stationarity</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/statistical_moments_and_time_series.html">Statistical Moments and Time Series</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/time_series.html">Time Series</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/time_series_modeling.html">Time Series Modeling</a></li><li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/yule_walker_equations.html">Yule Walker Equations</a></li></ol></li></ol></div></div></div><footer>
<div class="footer-columns">
<div class="footer-column">
<img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png"/>
</div>
<div class="footer-column">
<h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
<p>Thank you for visiting my personal website. All content here is free to use, but please remember to be respectful and avoid any misuse of the site. If youâ€™d like to get in touch, feel free to reach out via my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or connect with me on <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have technical questions or ideas to share. Wishing you all the best and a fantastic life ahead!</p>
</div>
<div class="footer-column">
<h2>Follow me</h2>
<ul class="social-media">
<li>
<a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
</a>YouTube
                </li>
<li>
<a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
</a>LinkedIn
                </li>
<li>
<a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
</a>Instagram
                </li>
<li>
<a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
</a>Github
                </li>
</ul>
</div>
</div>
<div>
<p id="copyright">
            Â© Adam Djellouli. All rights reserved.
        </p>
</div>
<script>
        document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
    </script>
<script src="../../../app.js"></script>
</footer></body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script></html>