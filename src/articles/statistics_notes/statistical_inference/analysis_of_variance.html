<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>ANOVA and the Analysis of Variance for Multiple Groups</title>
    <meta content="Does peer assessment enhance student learning?" name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper"><article-section id="article-body">
            <div class="article-action-buttons"><button class="btn-suggest-edit" title="Suggest Edit">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M4 21h4l11-11-4-4L4 17v4z" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button><button class="btn-create-issue" title="Create Issue">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"></circle>
                        <line stroke="currentColor" stroke-width="2" x1="12" x2="12" y1="8" y2="12"></line>
                        <circle cx="12" cy="16" fill="currentColor" r="1"></circle>
                    </svg>
                </button><button class="btn-download" title="Download">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M12 5v14m0 0l-6-6m6 6l6-6" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button></div>
            <p style="text-align: right;"><i>Last modified: April 30, 2022</i></p>
            <p style="text-align: right;"><i>This article is written in: üá∫üá∏</i></p>
            <h2 id="anova-and-the-analysis-of-variance-for-multiple-groups">ANOVA and the Analysis of Variance for Multiple Groups</h2>
            <h3 id="scenario-peer-assessment-and-student-learning">Scenario: Peer Assessment and Student Learning</h3>
            <p><strong>Does peer assessment enhance student learning?</strong></p>
            <ul>
                <li>A study <strong>investigated</strong> whether different instructional methods lead to varying learning outcomes, measured by final exam scores.</li>
                <li>Students were <strong>randomly assigned</strong> to one of three treatment groups to analyze performance.</li>
                <li>One group <strong>completed homework</strong> assignments independently, without any peer assessment.</li>
                <li>Another group completed homework and <strong>participated</strong> in peer assessment, where students evaluated each other‚Äôs work.</li>
                <li>A third group <strong>studied independently</strong> without doing homework.</li>
                <li>Final exam scores for students in each group were <strong>collected</strong> for comparison.</li>
                <li>The final exam scores were <strong>summarized</strong> and visualized using boxplots.</li>
                <li>The main objective of the study was to determine if different <strong>instructional methods</strong> significantly affected learning outcomes, as reflected in final exam results.</li>
            </ul>
            <h4 id="formulating-the-hypotheses">Formulating the Hypotheses</h4>
            <p>To statistically assess the impact of the different treatments, we set up the following hypotheses:</p>
            <p>I. <strong>Null Hypothesis ($H_0$)</strong>: All group means are equal. That is, there is no significant difference in final exam scores between the three treatment groups.</p>
            <p>$$
                H_0: \mu_1 = \mu_2 = \mu_3
                $$</p>
            <p>II. <strong>Alternative Hypothesis ($H_A$)</strong>: At least one group mean is different, indicating that the treatments have different effects on student learning.</p>
            <p>$$
                H_A: \text{At least one } \mu_j \text{ differs}
                $$</p>
            <h4 id="comparing-two-groups-the-two-sample-t-test">Comparing Two Groups: The Two-Sample t-Test</h4>
            <p>When comparing only two groups, the <strong>two-sample t-test</strong> is appropriate. It tests whether there is a significant difference between the means of two independent groups.</p>
            <p>I. <strong>t-Test Formula</strong></p>
            <p>The t-statistic is calculated as:</p>
            <p>$$
                t = \frac{y_1 - y_2}{SE_{y_1 - y_2}}
                $$</p>
            <p>where:</p>
            <ul>
                <li>$\bar{y}_1, \bar{y}_2$: Sample means of groups 1 and 2.</li>
                <li>$SE_{\bar{y}_1 - \bar{y}_2}$: Standard error of the difference between the two sample means.</li>
            </ul>
            <p>II. <strong>Standard Error Calculation</strong></p>
            <p>Assuming equal variances:</p>
            <p>$$
                SE_{\bar{y}_1 - \bar{y}_2} = \sqrt{\frac{s^2}{n_1} + \frac{s^2}{n_2}}
                $$</p>
            <p>where:</p>
            <ul>
                <li>$s^2$: Pooled sample variance.</li>
                <li>$n_1, n_2$: Sample sizes of groups 1 and 2.</li>
            </ul>
            <p>III. <strong>Limitations</strong></p>
            <ul>
                <li>The t-test is limited to comparing <strong>two</strong> groups.</li>
                <li>When dealing with more than two groups, the risk of Type I error increases with multiple t-tests.</li>
            </ul>
            <h3 id="anova-analysis-of-variance-for-multiple-groups">ANOVA: Analysis of Variance for Multiple Groups</h3>
            <p>To compare <strong>three or more groups</strong>, we use <strong>Analysis of Variance (ANOVA)</strong>. ANOVA tests for significant differences among group means by analyzing variances.</p>
            <p>Key Concepts:</p>
            <ul>
                <li><strong>Between-groups variance</strong> refers to the variability caused by the interaction between different treatment groups.</li>
                <li><strong>Within-groups variance</strong> refers to the variability within each group that arises from individual differences.</li>
            </ul>
            <p>ANOVA Hypotheses:</p>
            <ul>
                <li>The <strong>null hypothesis (H‚ÇÄ)</strong> in ANOVA states that all group means are equal.</li>
                <li>The <strong>alternative hypothesis (H‚Çê)</strong> suggests that not all group means are equal.</li>
            </ul>
            <h4 id="the-anova-methodology">The ANOVA Methodology</h4>
            <p>ANOVA partitions the total variability in the data into components attributable to various sources.</p>
            <h3 id="total-sum-of-squares-sstotal-">Total Sum of Squares (SSTotal)</h3>
            <p>The <strong>Total Sum of Squares</strong> measures the total variability in the data:</p>
            <p>$$
                SSTotal = \sum_{i=1}^{N} (y_i - \bar{y})^2
                $$</p>
            <p>where:</p>
            <ul>
                <li>$y_i$: Individual observations.</li>
                <li>$\bar{y}$: Overall mean of all observations.</li>
                <li>$N$: Total number of observations.</li>
            </ul>
            <h4 id="1-treatment-variance-between-groups-variability-">1. Treatment Variance (Between-Groups Variability)</h4>
            <p><strong>Sum of Squares for Treatments (SST)</strong> measures the variability between the group means and the overall mean:</p>
            <p>$$
                SST = \sum_{j=1}^{k} n_j (\bar{y}_j - \bar{y})^2
                $$</p>
            <p>where:</p>
            <ul>
                <li>$n_j$: Sample size of group $j$.</li>
                <li>$\bar{y}_j$: Mean of group $j$.</li>
                <li>$k$: Number of groups.</li>
            </ul>
            <p><strong>Treatment Mean Square (MST)</strong> is the average treatment variability:</p>
            <p>$$
                MST = \frac{SST}{k - 1}
                $$</p>
            <p>Degrees of freedom for treatments: $df_{\text{Treatment}} = k - 1$.</p>
            <h4 id="2-error-variance-within-groups-variability-">2. Error Variance (Within-Groups Variability)</h4>
            <p><strong>Sum of Squares for Error (SSE)</strong> measures the variability within the groups:</p>
            <p>$$
                SSE = \sum_{j=1}^{k} \sum_{i=1}^{n_j} (y_{ij} - \bar{y}_j)^2
                $$</p>
            <p>where:</p>
            <ul>
                <li>$y_{ij}$: Individual observation in group $j$.</li>
                <li>$\bar{y}_j$: Mean of group $j$.</li>
            </ul>
            <p><strong>Error Mean Square (MSE)</strong> is the average within-group variability:</p>
            <p>$$
                MSE = \frac{SSE}{N - k}
                $$</p>
            <p>where:</p>
            <ul>
                <li>Degrees of freedom for error: $df_{\text{Error}} = N - k$.</li>
            </ul>
            <h4 id="3-calculating-the-f-statistic">3. Calculating the F-Statistic</h4>
            <p>The <strong>F-statistic</strong> compares the between-groups variance to the within-groups variance:</p>
            <p>$$
                F = \frac{MST}{MSE}
                $$</p>
            <p>where:</p>
            <ul>
                <li>Under $H_0$, $MST$ and $MSE$ estimate the same variance, so $F \approx 1$.</li>
                <li>A large $F$ value suggests that between-group variability is greater than within-group variability, indicating significant differences among group means.</li>
            </ul>
            <p><strong>F-Distribution</strong></p>
            <ul>
                <li>The F-statistic follows an F-distribution with $df_1 = k - 1$ (numerator degrees of freedom) and $df_2 = N - k$ (denominator degrees of freedom).</li>
                <li>The p-value is determined based on the F-distribution.</li>
            </ul>
            <h3 id="example-peer-assessment-data">Example: Peer Assessment Data</h3>
            <h4 id="data-summary">Data Summary</h4>
            <p>Suppose we have the following data:</p>
            <ul>
                <li>The <strong>group sizes</strong> are defined as: Homework Only ($n_1$), Homework with Peer Assessment ($n_2$), and Study Without Homework ($n_3$).</li>
                <li>The <strong>group means</strong> ($\bar{y}_j$) and <strong>variances</strong> ($s_j^2$) are calculated based on the collected data.</li>
                <li>The <strong>total number of observations</strong> is represented as $N = n_1 + n_2 + n_3$.</li>
            </ul>
            <h4 id="anova-table-construction">ANOVA Table Construction</h4>
            <p><strong>ANOVA Table</strong> summarizes the calculations:</p>
            <p>
            <table>
                <tr>
                    <td>Source</td>
                    <td>df</td>
                    <td>Sum of Squares (SS)</td>
                    <td>Mean Square (MS)</td>
                    <td>F</td>
                </tr>
                <tr>
                    <td>Treatment</td>
                    <td>$k - 1$</td>
                    <td>SST</td>
                    <td>$MST = \frac{SST}{k - 1}$</td>
                    <td>$F = \frac{MST}{MSE}$</td>
                </tr>
                <tr>
                    <td>Error</td>
                    <td>$N - k$</td>
                    <td>SSE</td>
                    <td>$MSE = \frac{SSE}{N - k}$</td>
                    <td></td>
                </tr>
                <tr>
                    <td>Total</td>
                    <td>$N - 1$</td>
                    <td>SSTotal</td>
                    <td></td>
                    <td></td>
                </tr>
            </table>
            </p>
            <p><strong>Given Example</strong>:</p>
            <p>
            <table>
                <tr>
                    <td>Source</td>
                    <td>df</td>
                    <td>Sum of Squares</td>
                    <td>Mean Square</td>
                    <td>F</td>
                </tr>
                <tr>
                    <td>Treatment</td>
                    <td>2</td>
                    <td>98.4</td>
                    <td>49.2</td>
                    <td>2.57</td>
                </tr>
                <tr>
                    <td>Error</td>
                    <td>38</td>
                    <td>723.8</td>
                    <td>19.05</td>
                    <td></td>
                </tr>
                <tr>
                    <td>Total</td>
                    <td>40</td>
                    <td>822.2</td>
                    <td></td>
                    <td></td>
                </tr>
            </table>
            </p>
            <h4 id="calculations">Calculations</h4>
            <p><strong>SSTotal</strong>:</p>
            <p>$$
                SSTotal = SST + SSE = 98.4 + 723.8 = 822.2
                $$</p>
            <p><strong>Degrees of Freedom</strong>:</p>
            <ul>
                <li>$df_{\text{Treatment}} = k - 1 = 3 - 1 = 2$</li>
                <li>$df_{\text{Error}} = N - k = 41 - 3 = 38$</li>
                <li>$df_{\text{Total}} = N - 1 = 41 - 1 = 40$</li>
            </ul>
            <p><strong>Mean Squares</strong>:</p>
            <ul>
                <li>$MST = \frac{SST}{df_{\text{Treatment}}} = \frac{98.4}{2} = 49.2$</li>
                <li>$MSE = \frac{SSE}{df_{\text{Error}}} = \frac{723.8}{38} \approx 19.05$</li>
            </ul>
            <p><strong>F-Statistic</strong>:</p>
            <p>$$
                F = \frac{MST}{MSE} = \frac{49.2}{19.05} \approx 2.58
                $$</p>
            <p><strong>P-Value</strong>:</p>
            <ul>
                <li>Using F-distribution tables or software, with $df_1 = 2$ and $df_2 = 38$, we find:</li>
                <li><strong>p-value</strong> ‚âà 0.087</li>
            </ul>
            <h4 id="decision">Decision</h4>
            <ul>
                <li>At a significance level of <strong>$\alpha = 0.05$</strong>, since <strong>$p = 0.087 &gt; 0.05$</strong>, we <strong>fail to reject $H_0$</strong>.</li>
                <li>There is not enough evidence to conclude that the <strong>treatments</strong> have different effects on student learning.</li>
            </ul>
            <h3 id="interpretation-of-anova-results">Interpretation of ANOVA Results</h3>
            <h4 id="understanding-the-f-statistic">Understanding the F-Statistic</h4>
            <ul>
                <li>A <strong>small F-value</strong> (around 1) suggests that the variability between group means is similar to the variability within groups, implying <strong>no significant difference</strong> among group means.</li>
                <li>A <strong>large F-value</strong> indicates that the variability between group means is greater than the variability within groups, suggesting <strong>significant differences</strong> among group means.</li>
            </ul>
            <h4 id="p-value-interpretation">P-Value Interpretation</h4>
            <ul>
                <li>If the <strong>p-value is less than $\alpha$</strong>, we reject <strong>$H_0$</strong> and conclude that at least one group mean is different.</li>
                <li>If the <strong>p-value is greater than or equal to $\alpha$</strong>, we fail to reject <strong>$H_0$</strong> and cannot conclude that there are differences among group means.</li>
            </ul>
            <h4 id="conclusion-for-the-example">Conclusion for the Example</h4>
            <ul>
                <li>With a <strong>p-value of 0.087</strong>, there is not sufficient evidence at the 5% significance level to claim that peer assessment improves student learning over other methods.</li>
                <li>However, the <strong>p-value is close</strong> to 0.05, suggesting a potential trend that might reach significance with a larger sample size.</li>
            </ul>
            <h3 id="the-one-way-anova-model">The One-Way ANOVA Model</h3>
            <h4 id="statistical-model">Statistical Model</h4>
            <p>The <strong>one-way ANOVA model</strong> expresses each observation as:</p>
            <p>$$
                y_{ij} = \mu_j + \epsilon_{ij}
                $$</p>
            <p>where:</p>
            <ul>
                <li>$y_{ij}$: Observation $i$ in group $j$.</li>
                <li>$\mu_j$: Mean of group $j$.</li>
                <li>$\epsilon_{ij}$: Random error term, assumed to be independently and identically distributed (i.i.d.) with:</li>
                <li>Mean $E[\epsilon_{ij}] = 0$</li>
                <li>Variance $Var(\epsilon_{ij}) = \sigma^2$</li>
            </ul>
            <h4 id="alternative-representation">Alternative Representation</h4>
            <p>We can express the model in terms of the overall mean $\mu$ and group effects $\tau_j$:</p>
            <p>$$
                y_{ij} = \mu + \tau_j + \epsilon_{ij}
                $$</p>
            <p>where:</p>
            <ul>
                <li>$\mu$: Overall mean.</li>
                <li>$\tau_j = \mu_j - \mu$: Effect of treatment $j$.</li>
            </ul>
            <h4 id="assumptions-of-the-model">Assumptions of the Model</h4>
            <ol>
                <li><strong>Normality</strong>: The residuals $\epsilon_{ij}$ are normally distributed.</li>
                <li><strong>Independence</strong>: Observations are independent.</li>
                <li><strong>Homogeneity of Variance</strong>: The variances within each group are equal ($\sigma^2$ is constant across groups).</li>
            </ol>
            <h3 id="assumptions-of-the-f-test">Assumptions of the F-Test</h3>
            <p>For the results of the ANOVA F-test to be valid, certain assumptions must be met:</p>
            <h4 id="1-equal-variances-homogeneity-of-variance-">1. Equal Variances (Homogeneity of Variance)</h4>
            <ul>
                <li>The <strong>assumption</strong> is that the variances within each group are equal.</li>
                <li>This can be <strong>checked</strong> using boxplots for visual inspection of similar spread among groups.</li>
                <li><strong>Levene's Test</strong> can be used as a statistical method to test for equal variances.</li>
                <li>A <strong>rule of thumb</strong> states that if the largest sample standard deviation is no more than twice the smallest, the assumption is reasonable.</li>
            </ul>
            <h4 id="2-independence-of-observations">2. Independence of Observations</h4>
            <ul>
                <li>The <strong>assumption</strong> is that observations are independent both within and across groups.</li>
                <li>This is <strong>ensured</strong> by random assignment in experimental studies or random sampling in observational studies.</li>
            </ul>
            <h4 id="3-normality-of-residuals">3. Normality of Residuals</h4>
            <ul>
                <li>The <strong>assumption</strong> is that the residuals ($\epsilon_{ij}$) are normally distributed.</li>
                <li>This can be <strong>checked</strong> by using normal probability plots to assess the normality of residuals.</li>
                <li>The <strong>Shapiro-Wilk Test</strong> is a formal statistical test for normality.</li>
            </ul>
            <h3 id="post-anova-testing">Post-ANOVA Testing</h3>
            <p>If the ANOVA F-test leads us to reject $H_0$, indicating that not all group means are equal, we may want to identify <strong>which groups differ</strong>.</p>
            <p>Performing multiple t-tests increases the risk of Type I error (false positives). To control for this, we use:</p>
            <h4 id="1-bonferroni-correction">1. Bonferroni Correction</h4>
            <p>Adjusts the significance level:</p>
            <p>$$
                \alpha' = \frac{\alpha}{\text{Number of Comparisons}}
                $$</p>
            <p>Example: For 3 groups, there are $\frac{3 \times 2}{2} = 3$ comparisons.</p>
            <h4 id="2-tukey-s-honest-significant-difference-hsd-test">2. Tukey's Honest Significant Difference (HSD) Test</h4>
            <ul>
                <li>Controls the family-wise error rate.</li>
                <li>Computes confidence intervals for all pairwise differences.</li>
            </ul>
            <h4 id="steps-for-pairwise-comparisons">Steps for Pairwise Comparisons</h4>
            <p>I. <strong>Calculate the Standard Error for Differences</strong>:</p>
            <p>$$
                SE_{\bar{y}_i - \bar{y}_j} = \sqrt{MSE \left( \frac{1}{n_i} + \frac{1}{n_j} \right)}
                $$</p>
            <p>II. <strong>Compute the t-Statistic for Each Pair</strong>:</p>
            <p>$$
                t = \frac{y_i - y_j}{SE_{y_i - y_j}}
                $$</p>
            <p>III. <strong>Determine the Adjusted p-Values</strong>:</p>
            <p>Use the adjusted significance level or critical values from Tukey's distribution.</p>
            <h3 id="practical-application-steps">Practical Application Steps</h3>
            <h4 id="1-conduct-anova">1. Conduct ANOVA</h4>
            <ul>
                <li>Compute SST, SSE, MST, MSE, and F-statistic.</li>
                <li>Determine the p-value using the F-distribution.</li>
                <li>Decide whether to reject $H_0$ based on the p-value.</li>
            </ul>
            <h4 id="2-check-assumptions">2. Check Assumptions</h4>
            <ul>
                <li>To check for <strong>equal variances</strong>, use boxplots or formal statistical tests.</li>
                <li><strong>Normality</strong> is assessed by examining the residuals.</li>
                <li>Ensure that the study design supports <strong>independence</strong> of observations.</li>
            </ul>
            <h4 id="3-interpret-results">3. Interpret Results</h4>
            <ul>
                <li>If <strong>$H_0$ is not rejected</strong>, conclude that there is no significant difference among group means.</li>
                <li>If <strong>$H_0$ is rejected</strong>, proceed to post-hoc tests to identify specific group differences.</li>
            </ul>
        </article-section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol><a href="#anova-and-the-analysis-of-variance-for-multiple-groups">ANOVA and the Analysis of Variance for Multiple Groups</a>
                <ol>
                    <li><a href="#scenario-peer-assessment-and-student-learning">Scenario: Peer Assessment and Student Learning</a>
                        <ol>
                            <li><a href="#formulating-the-hypotheses">Formulating the Hypotheses</a></li>
                            <li><a href="#comparing-two-groups-the-two-sample-t-test">Comparing Two Groups: The Two-Sample t-Test</a></li>
                        </ol>
                    </li>
                    <li><a href="#anova-analysis-of-variance-for-multiple-groups">ANOVA: Analysis of Variance for Multiple Groups</a>
                        <ol>
                            <li><a href="#the-anova-methodology">The ANOVA Methodology</a></li>
                        </ol>
                    </li>
                    <li><a href="#total-sum-of-squares-sstotal-">Total Sum of Squares (SSTotal)</a>
                        <ol>
                            <li><a href="#1-treatment-variance-between-groups-variability-">1. Treatment Variance (Between-Groups Variability)</a></li>
                            <li><a href="#2-error-variance-within-groups-variability-">2. Error Variance (Within-Groups Variability)</a></li>
                            <li><a href="#3-calculating-the-f-statistic">3. Calculating the F-Statistic</a></li>
                        </ol>
                    </li>
                    <li><a href="#example-peer-assessment-data">Example: Peer Assessment Data</a>
                        <ol>
                            <li><a href="#data-summary">Data Summary</a></li>
                            <li><a href="#anova-table-construction">ANOVA Table Construction</a></li>
                            <li><a href="#calculations">Calculations</a></li>
                            <li><a href="#decision">Decision</a></li>
                        </ol>
                    </li>
                    <li><a href="#interpretation-of-anova-results">Interpretation of ANOVA Results</a>
                        <ol>
                            <li><a href="#understanding-the-f-statistic">Understanding the F-Statistic</a></li>
                            <li><a href="#p-value-interpretation">P-Value Interpretation</a></li>
                            <li><a href="#conclusion-for-the-example">Conclusion for the Example</a></li>
                        </ol>
                    </li>
                    <li><a href="#the-one-way-anova-model">The One-Way ANOVA Model</a>
                        <ol>
                            <li><a href="#statistical-model">Statistical Model</a></li>
                            <li><a href="#alternative-representation">Alternative Representation</a></li>
                            <li><a href="#assumptions-of-the-model">Assumptions of the Model</a></li>
                        </ol>
                    </li>
                    <li><a href="#assumptions-of-the-f-test">Assumptions of the F-Test</a>
                        <ol>
                            <li><a href="#1-equal-variances-homogeneity-of-variance-">1. Equal Variances (Homogeneity of Variance)</a></li>
                            <li><a href="#2-independence-of-observations">2. Independence of Observations</a></li>
                            <li><a href="#3-normality-of-residuals">3. Normality of Residuals</a></li>
                        </ol>
                    </li>
                    <li><a href="#post-anova-testing">Post-ANOVA Testing</a>
                        <ol>
                            <li><a href="#1-bonferroni-correction">1. Bonferroni Correction</a></li>
                            <li><a href="#2-tukey-s-honest-significant-difference-hsd-test">2. Tukey's Honest Significant Difference (HSD) Test</a></li>
                            <li><a href="#steps-for-pairwise-comparisons">Steps for Pairwise Comparisons</a></li>
                        </ol>
                    </li>
                    <li><a href="#practical-application-steps">Practical Application Steps</a>
                        <ol>
                            <li><a href="#1-conduct-anova">1. Conduct ANOVA</a></li>
                            <li><a href="#2-check-assumptions">2. Check Assumptions</a></li>
                            <li><a href="#3-interpret-results">3. Interpret Results</a></li>
                        </ol>
                    </li>
                </ol>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li>Basic Concepts<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/axioms_of_probability.html">Axioms of Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/bayes_theorem.html">Bayes Theorem</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/bayesian_vs_frequentist.html">Bayesian vs Frequentist</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/conditional_probability.html">Conditional Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/descriptive_statistics.html">Descriptive Statistics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/geometric_probability.html">Geometric Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/introduction_to_probability.html">Introduction to Probability</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/introduction_to_statistics.html">Introduction to Statistics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/probability_tree.html">Probability Tree</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/standard_error_and_lln.html">Standard Error and Lln</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/basic_concepts/total_probability.html">Total Probability</a></li>
                        </ol>
                    </li>
                    <li>Probability Distributions<ol>
                            <li>Continuous Distributions<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/beta_distribution.html">Beta Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/chi_square_distribution.html">Chi Square Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/exponential_distribution.html">Exponential Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/f_distribution.html">F Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/gamma_distribution.html">Gamma Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/log_normal_distribution.html">Log Normal Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/normal_distribution.html">Normal Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/student_t_distribution.html">Student T Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/continuous_distributions/uniform_distribution.html">Uniform Distribution</a></li>
                                </ol>
                            </li>
                            <li>Discrete Distributions<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/binomial_distribution.html">Binomial Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/geometric_distribution.html">Geometric Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/negative_binomial_distribution.html">Negative Binomial Distribution</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/discrete_distributions/poisson_distribution.html">Poisson Distribution</a></li>
                                </ol>
                            </li>
                            <li>Intro<ol>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/central_limit_theorem.html">Central Limit Theorem</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/introduction_to_distributions.html">Introduction to Distributions</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/normal_curve_and_z_score.html">Normal Curve and z Score</a></li>
                                    <li><a href="https://adamdjellouli.com/articles/statistics_notes/probability_distributions/intro/statistical_moments.html">Statistical Moments</a></li>
                                </ol>
                            </li>
                        </ol>
                    </li>
                    <li>Correlation and Regression<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/correlation.html">Correlation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/covariance.html">Covariance</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/logistic_regression.html">Logistic Regression</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/metrics.html">Metrics</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/multiple_regression.html">Multiple Regression</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/correlation_and_regression/simple_linear_regression.html">Simple Linear Regression</a></li>
                        </ol>
                    </li>
                    <li>Statistical Inference<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/analysis_of_categorical_data.html">Analysis of Categorical Data</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/analysis_of_variance.html">Analysis of Variance</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/confidence_intervals.html">Confidence Intervals</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/hypothesis_testing.html">Hypothesis Testing</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/multiple_comparisons.html">Multiple Comparisons</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/null_hypothesis.html">Null Hypothesis</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/resampling.html">Resampling</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/statistical_inference/type_i_and_type_ii_errors.html">Type i and Type Ii Errors</a></li>
                        </ol>
                    </li>
                    <li>Time Series Analysis<ol>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/arima_models.html">Arima Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autocorrelation_function.html">Autocorrelation Function</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autocovariance_function.html">Autocovariance Function</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/autoregressive_models.html">Autoregressive Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/backward_shift_operator.html">Backward Shift Operator</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/difference_equations.html">Difference Equations</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/forecasting.html">Forecasting</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/invertibility.html">Invertibility</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/moving_average_models.html">Moving Average Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/random_walk.html">Random Walk</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/seasonality_and_trends.html">Seasonality and Trends</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/series.html">Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/stationarity.html">Stationarity</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/statistical_moments_and_time_series.html">Statistical Moments and Time Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/time_series.html">Time Series</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/time_series_modeling.html">Time Series Modeling</a></li>
                            <li><a href="https://adamdjellouli.com/articles/statistics_notes/time_series_analysis/yule_walker_equations.html">Yule Walker Equations</a></li>
                        </ol>
                    </li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>Thank you for visiting my personal website. All content here is free to use, but please remember to be respectful and avoid any misuse of the site. If you‚Äôd like to get in touch, feel free to reach out via my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or connect with me on <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have technical questions or ideas to share. Wishing you all the best and a fantastic life ahead!</p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                ¬© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../../app.js"></script>
    </footer>
    <div id="pdf-spinner-overlay">
        <div class="spinner"></div>
    </div>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>