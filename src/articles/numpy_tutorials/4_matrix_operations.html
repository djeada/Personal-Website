<!DOCTYPE html>

<html lang="en">
<head>
<script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
<meta charset="utf-8"/>
<title>Matrices</title>
<meta content="A matrix is a systematic arrangement of numbers (or elements) in rows and columns." name="description"/>
<meta content="Adam Djellouli" name="author"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet"/>
<link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon"/>
<link href="../../resources/style.css" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>
<body><nav aria-label="Main navigation">
<a class="logo" href="https://adamdjellouli.com">
<img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG"/>
</a>
<input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox"/>
<ul aria-labelledby="navbar-toggle" role="menu">
<li role="menuitem">
<a href="../../index.html" title="Go to Home Page"> Home </a>
</li>
<li role="menuitem">
<a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
</li>
<li role="menuitem">
<a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
</li>
<li role="menuitem">
<a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
</li>
<li role="menuitem">
<a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
</li>
<li>
<script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
<div class="gcse-search"></div>
</li>
<li>
<button aria-label="Toggle dark mode" id="dark-mode-button"></button>
</li>
</ul>
</nav>
<div id="article-wrapper"><article-section id="article-body">
<p style="text-align: right;"><i>Last modified: December 25, 2025</i></p>
<p style="text-align: right;"><i>This article is written in: üá∫üá∏</i></p>
<h2 id="matrices">Matrices</h2>
<p>A matrix is a systematic arrangement of numbers (or elements) in rows and columns. An m √ó n matrix has <code>m</code> rows and <code>n</code> columns. The dimensions of the matrix are represented as m √ó n.</p>
<h3 id="vector-norms-and-matrix-norms">Vector Norms and Matrix Norms</h3>
<h4 id="vector-norms">Vector Norms</h4>
<p>A vector norm is a function that assigns a non-negative value to a vector in an $n$-dimensional space, providing a quantitative measure of the vector‚Äôs length or magnitude. One commonly used vector norm is the <strong>Euclidean norm</strong>, also known as the $L^2$ norm, defined for a vector $\vec{x}$ in $\mathbb{R}^n$ as:</p>
<p>$$
\lVert \vec{x} \rVert_2 = \sqrt{\sum_{i=1}^n x_i^2}
$$</p>
<p>where $x_i$ represents the components of the vector $\vec{x}$.</p>
<h4 id="matrix-norms">Matrix Norms</h4>
<p>Matrix norms extend the concept of vector norms to matrices. A widely used matrix norm is the <strong>Frobenius norm</strong>, analogous to the Euclidean norm for vectors. For a matrix $M$ with dimensions $m \times n$, the Frobenius norm is defined as:</p>
<p>$$
\lVert M \rVert_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n (M_{ij})^2}
$$</p>
<p>where $M_{ij}$ represents the elements of the matrix $M$.</p>
<h4 id="types-of-matrix-norms">Types of Matrix Norms</h4>
<p>Several matrix norms are commonly used in practice, each with unique properties and applications:</p>
<p>I. <strong>Frobenius Norm</strong> </p>
<p>Measures the ‚Äúsize‚Äù of a matrix in terms of the sum of the squares of its entries.  </p>
<p>$$
\lVert M \rVert_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n (M_{ij})^2}
$$</p>
<p>II. <strong>Spectral Norm</strong> </p>
<p>Also known as the operator 2-norm, it is the largest singular value of the matrix, which corresponds to the square root of the largest eigenvalue of $M^T M$.  </p>
<p>$$
\lVert M \rVert_2 = \sigma_{\max}(M)
$$</p>
<p>III. <strong>1-Norm (Maximum Column Sum Norm)</strong> </p>
<p>The maximum absolute column sum of the matrix.  </p>
<p>$$
\lVert M \rVert_1 = \max_{1 \le j \le n} \sum_{i=1}^m \lvert M_{ij} \rvert
$$</p>
<p>IV. <strong>Infinity Norm (Maximum Row Sum Norm)</strong> </p>
<p>The maximum absolute row sum of the matrix.  </p>
<p>$$
\lVert M \rVert_\infty = \max_{1 \le i \le m} \sum_{j=1}^n \lvert M_{ij} \rvert
$$</p>
<h4 id="norms-in-numpy">Norms in NumPy</h4>
<p>NumPy provides functions to compute various matrix norms, making it easy to work with these concepts in Python.</p>
<h5>Frobenius Norm</h5>
<p>The Frobenius norm can be computed using the <code>numpy.linalg.norm</code> function with the <code>'fro'</code> argument:</p>
<p><div><pre><code class="language-python">import numpy as np

A = np.array([[1, 2], [3, 4]])
frobenius_norm = np.linalg.norm(A, 'fro')
print("Frobenius Norm:", frobenius_norm)</code></pre></div></p>
<p>Expected Output:</p>
<p><div><pre><code class="language-shell">Frobenius Norm: 5.477225575051661</code></pre></div></p>
<h5>Spectral Norm</h5>
<p>The spectral norm, or 2-norm, can be computed using the <code>numpy.linalg.norm</code> function with the <code>2</code> argument:</p>
<p><div><pre><code class="language-python">spectral_norm = np.linalg.norm(A, 2)
print("Spectral Norm:", spectral_norm)</code></pre></div></p>
<p>Expected Output:</p>
<p><div><pre><code class="language-shell">Spectral Norm: 5.464985704219043</code></pre></div></p>
<h5>1-Norm</h5>
<p>The 1-norm can be computed by specifying the <code>1</code> argument in the <code>numpy.linalg.norm</code> function:</p>
<p><div><pre><code class="language-python">one_norm = np.linalg.norm(A, 1)
print("1-Norm:", one_norm)</code></pre></div></p>
<p>Expected Output:</p>
<p><div><pre><code class="language-shell">1-Norm: 6.0</code></pre></div></p>
<h5>Infinity Norm</h5>
<p>The infinity norm can be computed using the <code>np.inf</code> argument in the <code>numpy.linalg.norm</code> function:</p>
<p><div><pre><code class="language-python">infinity_norm = np.linalg.norm(A, np.inf)
print("Infinity Norm:", infinity_norm)</code></pre></div></p>
<p>Expected Output:</p>
<p><div><pre><code class="language-shell">Infinity Norm: 7.0</code></pre></div></p>
<h3 id="practical-applications-of-matrix-norms">Practical Applications of Matrix Norms</h3>
<p>Matrix norms are used in a variety of applications, including:</p>
<ul>
<li>In the field of <strong>Error Analysis</strong>, norms serve as a tool to quantify errors, offering a measure of the discrepancy between the computed and exact solutions in numerical methods.</li>
<li>When conducting <strong>Stability Analysis</strong>, norms provide insight into how perturbations in the data or initial conditions can affect the results, thereby helping to assess the robustness of algorithms and the conditioning of matrices.</li>
<li>In <strong>Optimization</strong>, the minimization of matrix norms often plays a crucial role, as many such problems involve constraints or objectives that can be expressed in terms of norms, allowing for the selection of optimal solutions.</li>
<li>In <strong>Machine Learning</strong>, regularization techniques utilize norms to penalize large coefficients, thereby preventing overfitting and encouraging simpler models that generalize better to new data.</li>
</ul>
<h3 id="example-using-matrix-norms-in-optimization">Example: Using Matrix Norms in Optimization</h3>
<p>Consider a problem where we want to find a matrix $X$ that approximates another matrix $A$ while minimizing the Frobenius norm of the difference:</p>
<p><div><pre><code class="language-python">A = np.array([[1, 2], [3, 4]])
X = np.array([[0.9, 2.1], [3.1, 3.9]])

# Calculate the Frobenius norm of the difference
approx_error = np.linalg.norm(A - X, 'fro')
print("Approximation Error (Frobenius Norm):", approx_error)</code></pre></div></p>
<p>Expected Output:</p>
<p><div><pre><code class="language-shell">Approximation Error (Frobenius Norm): 0.22360679774997896</code></pre></div></p>
<h3 id="sub-multiplicative-property">Sub-multiplicative Property</h3>
<p>Matrix norms exhibit the sub-multiplicative property, a crucial characteristic in linear algebra and numerical analysis. This property is defined as follows:</p>
<p>$$
||AB|| \leq ||A|| \times ||B||
$$</p>
<h4 id="understanding-the-sub-multiplicative-property">Understanding the Sub-multiplicative Property</h4>
<p>The sub-multiplicative property implies that the norm of the product of two matrices $A$ and $B$ is at most the product of the norms of the individual matrices. This property is significant because it helps in understanding the behavior of matrix operations and provides bounds for the results of these operations. </p>
<h4 id="implications-of-the-sub-multiplicative-property">Implications of the Sub-multiplicative Property</h4>
<ul>
<li>The sub-multiplicative property is essential for ensuring the <strong>Stability in Numerical Computations</strong>. When dealing with products of matrices, this property helps in controlling the growth of errors.</li>
<li>Using matrix norms, we can define a distance metric between matrices. This <strong>Norm as a Measure of Distance</strong> is useful in various applications such as optimization, approximation, and machine learning.</li>
</ul>
<p>The distance between two matrices $A$ and $B$ can be defined as:</p>
<p>$$
d(A, B) = ||A - B||
$$</p>
<p>This metric measures how "far apart" two matrices are, which is particularly useful in iterative methods where convergence to a particular matrix is desired.</p>
<h4 id="example-verifying-the-sub-multiplicative-property">Example: Verifying the Sub-multiplicative Property</h4>
<p>Let's verify the sub-multiplicative property with a practical example using NumPy.</p>
<p><div><pre><code class="language-python">A = np.array([[1, 2], [3, 4]])
B = np.array([[2, 0], [1, 2]])

# Calculate the norms
norm_A = np.linalg.norm(A, 'fro')
norm_B = np.linalg.norm(B, 'fro')

# Calculate the product of the norms
product_of_norms = norm_A * norm_B

# Calculate the norm of the matrix product
product_matrix = np.dot(A, B)
norm_product_matrix = np.linalg.norm(product_matrix, 'fro')

print("Norm of A:", norm_A)
print("Norm of B:", norm_B)
print("Product of Norms:", product_of_norms)
print("Norm of Product Matrix:", norm_product_matrix)

# Verify the sub-multiplicative property
assert norm_product_matrix &lt;= product_of_norms, "Sub-multiplicative property violated!"
print("Sub-multiplicative property holds!")</code></pre></div></p>
<p>Output:</p>
<p><div><pre><code class="language-shell">Norm of A: 5.477225575051661
Norm of B: 2.449489742783178
Product of Norms: 13.416407864998739
Norm of Product Matrix: 12.083045973594572
Sub-multiplicative property holds!</code></pre></div></p>
<h2 id="matrix-multiplication">Matrix Multiplication</h2>
<p>Matrix multiplication is a fundamental operation in linear algebra, essential for various applications in science, engineering, computer graphics, and machine learning. The operation involves two matrices, where the number of columns in the first matrix must match the number of rows in the second matrix. The resulting matrix has dimensions determined by the rows of the first matrix and the columns of the second matrix.</p>
<h3 id="definition-and-computation">Definition and Computation</h3>
<p>Given two matrices $M$ and $N$:</p>
<ul>
<li>$M$ is an $m \times n$ matrix</li>
<li>$N$ is an $n \times p$ matrix</li>
</ul>
<p>The product $P = M \times N$ will be an $m \times p$ matrix. The elements of the resulting matrix $P$ are computed as follows:</p>
<p>$$
P_{ij} = \sum_{k=1}^n{M_{ik} N_{kj}}
$$</p>
<p>Where:</p>
<ul>
<li>$(P)_{ij}$ is the element in the $i$-th row and $j$-th column of the resulting matrix $P$.</li>
<li>$M_{ik}$ is the element in the $i$-th row and $k$-th column of matrix $M$.</li>
<li>$N_{kj}$ is the element in the $k$-th row and $j$-th column of matrix $N$.</li>
</ul>
<h3 id="properties-of-matrix-multiplication">Properties of Matrix Multiplication</h3>
<ul>
<li>The property of being <strong>non-commutative</strong> indicates that, in general, $MN \neq NM$ for matrices.</li>
<li>Matrix multiplication is <strong>associative</strong>, meaning that $(MN)P = M(NP)$.</li>
<li>The <strong>distributive</strong> property states that $M(N + P) = MN + MP$ and $(M + N)P = MP + NP$.</li>
<li>The <strong>identity matrix</strong> property ensures that multiplying any matrix $M$ by an identity matrix $I$ (with appropriately matched dimensions) leaves $M$ unchanged, such that $MI = IM = M$.</li>
</ul>
<h3 id="matrix-multiplication">Matrix Multiplication</h3>
<p>NumPy provides several methods to perform matrix multiplication:</p>
<h4 id="using-np-dot-">Using <code>np.dot()</code></h4>
<p>The <code>np.dot()</code> function computes the dot product of two arrays. For 2-D arrays, it is equivalent to matrix multiplication.</p>
<p><div><pre><code class="language-python">import numpy as np

M = np.array([[-4, 5], [1, 7], [8, 3]])
N = np.array([[3, -5, 2, 7], [-5, 1, -4, -3]])
product = np.dot(M, N)
print(product)</code></pre></div></p>
<p>Expected Output:</p>
<p><div><pre><code class="language-shell">[[-37  25 -28 -43]
 [-32   2 -26 -14]
 [  9 -37   4  47]]</code></pre></div></p>
<h4 id="using-operator">Using <code>@</code> Operator</h4>
<p>The <code>@</code> operator is another way to perform matrix multiplication in Python 3.5+.</p>
<p><div><pre><code class="language-python">product = M @ N
print(product)</code></pre></div></p>
<p>Expected Output:</p>
<p><div><pre><code class="language-shell">[[-37  25 -28 -43]
 [-32   2 -26 -14]
 [  9 -37   4  47]]</code></pre></div></p>
<h4 id="using-np-matmul-">Using <code>np.matmul()</code></h4>
<p>The <code>np.matmul()</code> function performs matrix multiplication for two arrays.</p>
<p><div><pre><code class="language-python">product = np.matmul(M, N)
print(product)</code></pre></div></p>
<p>Expected Output:</p>
<p><div><pre><code class="language-shell">[[-37  25 -28 -43]
 [-32   2 -26 -14]
 [  9 -37   4  47]]</code></pre></div></p>
<h3 id="examples-of-applications">Examples of Applications</h3>
<ul>
<li>In <strong>computer graphics</strong>, transformations like rotation, scaling, and translation are represented through matrix multiplications.</li>
<li><strong>Machine learning</strong> frequently involves matrix multiplications for operations such as transforming features and applying weights to data.</li>
<li><strong>Scientific simulations</strong> often require solving systems of linear equations, a task commonly handled using matrix multiplication.</li>
</ul>
<h3 id="performance-considerations">Performance Considerations</h3>
<p>Matrix multiplication can be computationally intensive, especially for large matrices. NumPy uses optimized libraries such as BLAS and LAPACK to perform efficient matrix multiplications. For very large datasets, leveraging these optimizations is crucial.</p>
<h4 id="example-of-large-matrix-multiplication">Example of Large Matrix Multiplication</h4>
<p><div><pre><code class="language-python">import numpy as np

# Create large random matrices
A = np.random.rand(1000, 500)
B = np.random.rand(500, 1000)

# Multiply using np.dot
result = np.dot(A, B)
print(result.shape)</code></pre></div></p>
<p>Expected Output:</p>
<p><div><pre><code class="language-shell">(1000, 1000)</code></pre></div></p>
<h3 id="strassen-s-algorithm-for-matrix-multiplication">Strassen's Algorithm for Matrix Multiplication</h3>
<p>For extremely large matrices, Strassen's algorithm can be used to reduce the computational complexity. Although NumPy does not implement Strassen's algorithm directly, understanding it can be beneficial for theoretical insights.</p>
<h3 id="matrix-transpose">Matrix Transpose</h3>
<p>Transposing a matrix involves interchanging its rows and columns. The transpose of an $m \times n$ matrix results in an $n \times m$ matrix. This operation is fundamental in various applications, including solving linear equations, optimization problems, and transforming data.</p>
<h4 id="definition">Definition</h4>
<p>For an $m \times n$ matrix $M$, the transpose of $M$, denoted as $M^T$, is an $n \times m$ matrix where the element at the $i$-th row and $j$-th column of $M$ becomes the element at the $j$-th row and $i$-th column of $M^T$.</p>
<h4 id="example">Example</h4>
<p>Consider the matrix $M$:</p>
<p><div><pre><code class="language-python">import numpy as np

M = np.array([[-4, 5], [1, 7], [8, 3]])
print("Original Matrix:
", M)
print("Transpose of Matrix:
", M.T)</code></pre></div></p>
<p>Expected output:</p>
<p><div><pre><code class="language-shell">Original Matrix:
[[-4  5]
 [ 1  7]
 [ 8  3]]

Transpose of Matrix:
[[-4  1  8]
 [ 5  7  3]]</code></pre></div></p>
<h4 id="properties-of-transpose">Properties of Transpose</h4>
<ul>
<li>The property of <strong>double transpose</strong> states that the transpose of the transpose of a matrix is the original matrix, expressed as $(M^T)^T = M$.</li>
<li>For the <strong>sum</strong> of matrices, the transpose of the sum is equal to the sum of the transposes, denoted by $(A + B)^T = A^T + B^T$.</li>
<li>In <strong>scalar multiplication</strong>, the transpose of a scalar multiple is the scalar multiple of the transpose, which is written as $(cA)^T = cA^T$.</li>
<li>The <strong>product</strong> rule for transposes indicates that the transpose of a product of two matrices is the product of the transposes in reverse order, given by $(AB)^T = B^T A^T$.</li>
</ul>
<h3 id="determinants">Determinants</h3>
<p>The determinant is a scalar value that is computed from a square matrix. It has significant applications in linear algebra, including solving systems of linear equations, computing inverses of matrices, and determining whether a matrix is invertible.</p>
<h4 id="definition">Definition</h4>
<p>For a square matrix $A$, the determinant is denoted as $\text{det}(A)$ or $|A|$. For a $2 \times 2$ matrix:</p>
<p>$$
A = \begin{pmatrix} 
a &amp; b \ 
c &amp; d 
\end{pmatrix}
$$</p>
<p>The determinant is calculated as:</p>
<p>$$
\text{det}(A) = ad - bc
$$</p>
<h4 id="example">Example</h4>
<p>Consider the matrix $M$:</p>
<p><div><pre><code class="language-python">M = np.array([[-4, 5], [1, 7]])
det_M = np.linalg.det(M)
print("Determinant of M:", det_M)</code></pre></div></p>
<p>Expected output: <code>-33.0</code></p>
<h4 id="properties-of-determinants">Properties of Determinants</h4>
<p>I. The <strong>multiplicative property</strong> of determinants states that for any two square matrices $A$ and $B$ of the same size, the determinant of their product is equal to the product of their determinants: $\text{det}(AB) = \text{det}(A) \cdot \text{det}(B)$.</p>
<p>II. According to the <strong>transpose</strong> property, the determinant of a matrix is the same as the determinant of its transpose, represented as $\text{det}(A) = \text{det}(A^T)$.</p>
<p>III. The <strong>inverse</strong> property indicates that if $A$ is invertible, then the determinant of the inverse is the reciprocal of the determinant of the matrix, expressed as $\text{det}(A^{-1}) = \frac{1}{\text{det}(A)}$.</p>
<p>IV. For <strong>row operations</strong> on a matrix:</p>
<ul>
<li>Swapping two rows results in the determinant being multiplied by $-1$.</li>
<li>Multiplying a row by a scalar $k$ causes the determinant to be multiplied by $k$.</li>
<li>Adding a multiple of one row to another row does not affect the determinant.</li>
</ul>
<h3 id="identity-and-inverse-matrices">Identity and Inverse Matrices</h3>
<h4 id="identity-matrix">Identity Matrix</h4>
<p>The identity matrix, typically denoted as $I$, is a special square matrix with ones on its main diagonal and zeros in all other positions. It serves as the multiplicative identity in matrix operations, meaning any matrix multiplied by the identity matrix remains unchanged.</p>
<h4 id="definition">Definition</h4>
<p>For an $n \times n$ identity matrix $I$:</p>
<p>$$
I = 
\begin{bmatrix} 
1      &amp; 0      &amp; \cdots &amp; 0      \\
0      &amp; 1      &amp; \cdots &amp; 0      \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0      &amp; 0      &amp; \cdots &amp; 1
\end{bmatrix}
$$</p>
<h4 id="example">Example</h4>
<p>Creating an identity matrix using NumPy:</p>
<p><div><pre><code class="language-python">I = np.eye(3)
print("Identity Matrix I:", I)</code></pre></div></p>
<p>Expected output:</p>
<p><div><pre><code class="language-shell">Identity Matrix I:
[[1. 0. 0.]
 [0. 1. 0.]
 [0. 0. 1.]]</code></pre></div></p>
<h4 id="inverse-matrix">Inverse Matrix</h4>
<p>A square matrix $A$ is said to have an inverse, denoted $A^{-1}$, if:</p>
<p>$$
A \times A^{-1} = A^{-1} \times A = I
$$</p>
<p>The inverse matrix is crucial in solving systems of linear equations and various other applications.</p>
<h4 id="example">Example</h4>
<p>Consider the matrix $M$:</p>
<p><div><pre><code class="language-python">M = np.array([[-4, 5], [1, 7]])
inv_M = np.linalg.inv(M)
print("Inverse of M:", inv_M)</code></pre></div></p>
<p>Expected output:</p>
<p><div><pre><code class="language-shell">[[-0.21212121  0.15151515]
 [ 0.03030303  0.12121212]]</code></pre></div></p>
<h4 id="properties-of-inverse-matrices">Properties of Inverse Matrices</h4>
<ul>
<li>The property of <strong>uniqueness</strong> states that if a matrix $A$ has an inverse, then the inverse is unique.</li>
<li>According to the <strong>product</strong> property, the inverse of a product of matrices is the product of their inverses in reverse order, expressed as $(AB)^{-1} = B^{-1}A^{-1}$.</li>
<li>The <strong>transpose</strong> property indicates that the inverse of the transpose of a matrix is the transpose of the inverse, denoted as $(A^T)^{-1} = (A^{-1})^T$.</li>
</ul>
<h3 id="rank-of-a-matrix">Rank of a Matrix</h3>
<p>The rank of a matrix provides insight into its structure and properties. Essentially, it is the number of linearly independent rows or columns present in the matrix. The rank can reveal information about the solutions of linear systems or the invertibility of a matrix.</p>
<h4 id="understanding-matrix-rank">Understanding Matrix Rank</h4>
<ul>
<li><strong>Linear Independence</strong>: Rows (or columns) are linearly independent if no row (or column) can be expressed as a linear combination of others.</li>
<li><strong>Full Rank</strong>: A matrix is considered to have full rank if its rank is equal to the lesser of its number of rows and columns. A matrix with full rank has the maximum possible rank given its dimensions.</li>
</ul>
<h4 id="determining-the-rank">Determining the Rank</h4>
<p>Python's NumPy library offers a convenient function to compute the rank: <code>np.linalg.matrix_rank</code>.</p>
<p>Example:</p>
<p><div><pre><code class="language-python">import numpy as np

# Define a matrix
A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

# Calculate its rank
rank_A = np.linalg.matrix_rank(A)

print("Rank of A:", rank_A)</code></pre></div></p>
<p>Output:</p>
<p><div><pre><code class="language-shell">Rank of A: 2</code></pre></div></p>
<p>In this instance, the rank of matrix A is 2, suggesting that only 2 of its rows (or columns) are linearly independent.</p>
<h4 id="singular-matrices-and-rank">Singular Matrices and Rank</h4>
<p>A matrix's rank can indicate whether it's singular (non-invertible). A square matrix is singular if its rank is less than its size (number of rows or columns). Singular matrices don't possess unique inverses.</p>
<p>To check for singularity using rank:</p>
<p><div><pre><code class="language-python">import numpy as np

# Create a matrix, which is clearly singular due to linearly dependent rows
A = np.array([[1, 2], [2, 4]])

# Calculate the rank
rank_A = np.linalg.matrix_rank(A)

# Check for singularity
is_singular = "Matrix A is singular." if rank_A &lt; A.shape[1] else "Matrix A is not singular."

print(is_singular)</code></pre></div></p>
<p>Output:</p>
<p><div><pre><code class="language-shell">Matrix A is singular.</code></pre></div></p>
<p>By understanding the rank, one can determine the properties of a matrix and its ability to be inverted, which is crucial in numerous linear algebra applications.</p>
<h3 id="summary-of-matrix-operations">Summary of Matrix Operations</h3>
<p>
<table><tr><td>Operation</td><td>Purpose</td><td>Primary NumPy Call</td><td>Python Shorthand</td><td>Shape Rules</td></tr><tr><td><strong>Dot / Inner product</strong></td><td>‚Ä¢ 1-D arrays ‚Üí scalar (inner product)<br/>‚Ä¢ 2-D arrays ‚Üí matrix product</td><td><code>np.dot(a, b)</code></td><td><code>a @ b</code></td><td>Last dim of <em>a</em> = second-to-last dim of <em>b</em></td></tr><tr><td><strong>Matrix product</strong></td><td>General (broadcast-aware) matrix multiplication</td><td><code>np.matmul(a, b)</code></td><td><code>a @ b</code></td><td>Handles <code>(..., m, k) @ (..., k, n)</code> batched</td></tr><tr><td><strong>Element-wise multiply</strong></td><td>Hadamard product (same shape)</td><td><code>a * b</code></td><td><code>*</code></td><td>Broadcasting-compatible shapes</td></tr><tr><td><strong>Transpose</strong></td><td>Swap axes 0 and 1 (or any via <code>axes=</code>)</td><td><code>a.T</code> or <code>np.transpose(a)</code></td><td>‚Äî</td><td>For &gt;2D use <code>np.swapaxes</code>/<code>np.moveaxis</code></td></tr><tr><td><strong>Inverse</strong></td><td>Matrix inverse (square, non-singular)</td><td><code>np.linalg.inv(a)</code></td><td>‚Äî</td><td>Prefer <code>np.linalg.solve(a, b)</code> for systems</td></tr><tr><td><strong>Determinant</strong></td><td>Scalar determinant of square matrix</td><td><code>np.linalg.det(a)</code></td><td>‚Äî</td><td>Ill-conditioned if <code>det(a) ‚âà 0</code></td></tr><tr><td><strong>Rank</strong></td><td>Numerical rank (‚âà # of linearly independent rows)</td><td><code>np.linalg.matrix_rank(a)</code></td><td>‚Äî</td><td>Uses SVD under the hood</td></tr><tr><td><strong>Trace</strong></td><td>Sum of diagonal elements</td><td><code>np.trace(a)</code></td><td>‚Äî</td><td>Works on the last two axes by default</td></tr><tr><td><strong>Eigenvalues / vectors</strong></td><td>Spectral decomposition (square)</td><td><code>vals, vecs = np.linalg.eig(a)</code></td><td>‚Äî</td><td>Use <code>np.linalg.eigvals(a)</code> for values only</td></tr><tr><td><strong>SVD</strong></td><td>Singular-value decomposition</td><td><code>u, s, vh = np.linalg.svd(a)</code></td><td>‚Äî</td><td>Robust for rectangular or rank-deficient mats</td></tr><tr><td><strong>Matrix power</strong></td><td>Integer power <em>k</em> (square)</td><td><code>np.linalg.matrix_power(a, k)</code></td><td>‚Äî</td><td><code>k &lt; 0</code> gives inverse powers</td></tr></table>
</p>
<p>Tips &amp; Best Practices:</p>
<ul>
<li><strong>Prefer <code>@</code></strong> for readability and automatic broadcasting; it resolves to <code>np.matmul</code> for ‚â•2-D inputs and to <code>np.dot</code> for 1-D.</li>
<li><strong>Use <code>solve</code> not <code>inv</code></strong>: to compute $x$ in $Ax=b$, <code>x = np.linalg.solve(A, b)</code> is faster and stabler than <code>np.linalg.inv(A) @ b</code>.</li>
<li><strong>Check conditioning</strong> with <code>np.linalg.cond(a)</code> before inverting or solving.</li>
<li>For <strong>large sparse matrices</strong>, switch to <code>scipy.sparse.linalg</code> counterparts to avoid excessive memory use.</li>
<li>When experimenting, inspect shapes with <code>a.shape</code>‚Äîmost dimension-mismatch bugs arise from overlooked trailing axes.</li>
</ul>
</article-section><div id="table-of-contents"><h2>Table of Contents</h2><ol><li><a href="#matrices">Matrices</a><ol><li><a href="#vector-norms-and-matrix-norms">Vector Norms and Matrix Norms</a><ol><li><a href="#vector-norms">Vector Norms</a></li><li><a href="#matrix-norms">Matrix Norms</a></li><li><a href="#types-of-matrix-norms">Types of Matrix Norms</a></li><li><a href="#norms-in-numpy">Norms in NumPy</a></li></ol></li><li><a href="#practical-applications-of-matrix-norms">Practical Applications of Matrix Norms</a></li><li><a href="#example-using-matrix-norms-in-optimization">Example: Using Matrix Norms in Optimization</a></li><li><a href="#sub-multiplicative-property">Sub-multiplicative Property</a><ol><li><a href="#understanding-the-sub-multiplicative-property">Understanding the Sub-multiplicative Property</a></li><li><a href="#implications-of-the-sub-multiplicative-property">Implications of the Sub-multiplicative Property</a></li><li><a href="#example-verifying-the-sub-multiplicative-property">Example: Verifying the Sub-multiplicative Property</a></li></ol></li></ol></li><li><a href="#matrix-multiplication">Matrix Multiplication</a><ol><li><a href="#definition-and-computation">Definition and Computation</a></li><li><a href="#properties-of-matrix-multiplication">Properties of Matrix Multiplication</a></li><li><a href="#matrix-multiplication">Matrix Multiplication</a><ol><li><a href="#using-np-dot-">Using np.dot()</a></li><li><a href="#using-operator">Using @ Operator</a></li><li><a href="#using-np-matmul-">Using np.matmul()</a></li></ol></li><li><a href="#examples-of-applications">Examples of Applications</a></li><li><a href="#performance-considerations">Performance Considerations</a><ol><li><a href="#example-of-large-matrix-multiplication">Example of Large Matrix Multiplication</a></li></ol></li><li><a href="#strassen-s-algorithm-for-matrix-multiplication">Strassen's Algorithm for Matrix Multiplication</a></li><li><a href="#matrix-transpose">Matrix Transpose</a><ol><li><a href="#definition">Definition</a></li><li><a href="#example">Example</a></li><li><a href="#properties-of-transpose">Properties of Transpose</a></li></ol></li><li><a href="#determinants">Determinants</a><ol><li><a href="#definition">Definition</a></li><li><a href="#example">Example</a></li><li><a href="#properties-of-determinants">Properties of Determinants</a></li></ol></li><li><a href="#identity-and-inverse-matrices">Identity and Inverse Matrices</a><ol><li><a href="#identity-matrix">Identity Matrix</a></li><li><a href="#definition">Definition</a></li><li><a href="#example">Example</a></li><li><a href="#inverse-matrix">Inverse Matrix</a></li><li><a href="#example">Example</a></li><li><a href="#properties-of-inverse-matrices">Properties of Inverse Matrices</a></li></ol></li><li><a href="#rank-of-a-matrix">Rank of a Matrix</a><ol><li><a href="#understanding-matrix-rank">Understanding Matrix Rank</a></li><li><a href="#determining-the-rank">Determining the Rank</a></li><li><a href="#singular-matrices-and-rank">Singular Matrices and Rank</a></li></ol></li><li><a href="#summary-of-matrix-operations">Summary of Matrix Operations</a></li></ol></li></ol><div id="related-articles"><h2>Related Articles</h2><ol><li><a href="https://adamdjellouli.com/articles/numpy_tutorials/1_creating_arrays.html">Creating Arrays</a></li><li><a href="https://adamdjellouli.com/articles/numpy_tutorials/2_accessing_modifying_elements.html">Accessing Modifying Elements</a></li><li><a href="https://adamdjellouli.com/articles/numpy_tutorials/3_vector_operations.html">Vector Operations</a></li><li><a href="https://adamdjellouli.com/articles/numpy_tutorials/4_matrix_operations.html">Matrix Operations</a></li><li><a href="https://adamdjellouli.com/articles/numpy_tutorials/5_reshaping_arrays.html">Reshaping Arrays</a></li><li><a href="https://adamdjellouli.com/articles/numpy_tutorials/6_searching_filtering_and_sorting.html">Searching Filtering and Sorting</a></li><li><a href="https://adamdjellouli.com/articles/numpy_tutorials/7_combining_arrays.html">Combining Arrays</a></li><li><a href="https://adamdjellouli.com/articles/numpy_tutorials/8_linear_equations.html">Linear Equations</a></li><li><a href="https://adamdjellouli.com/articles/numpy_tutorials/9_statistics_and_random_numbers.html">Statistics and Random Numbers</a></li></ol></div></div></div><footer>
<div class="footer-columns">
<div class="footer-column">
<img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png"/>
</div>
<div class="footer-column">
<h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
<p>Thank you for visiting my personal website. All content here is free to use, but please remember to be respectful and avoid any misuse of the site. If you‚Äôd like to get in touch, feel free to reach out via my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or connect with me on <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have technical questions or ideas to share. Wishing you all the best and a fantastic life ahead!</p>
</div>
<div class="footer-column">
<h2>Follow me</h2>
<ul class="social-media">
<li>
<a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
</a>YouTube
                </li>
<li>
<a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
</a>LinkedIn
                </li>
<li>
<a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
</a>Instagram
                </li>
<li>
<a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
</a>Github
                </li>
</ul>
</div>
</div>
<div>
<p id="copyright">
            ¬© Adam Djellouli. All rights reserved.
        </p>
</div>
<script>
        document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
    </script>
<script src="../../app.js"></script>
</footer></body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script></html>