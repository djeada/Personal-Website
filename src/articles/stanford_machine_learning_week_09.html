<!DOCTYPE html>
<html lang="en">

<head>
    <title>Adam Djellouli - Blog</title>
    <meta charset="utf-8">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" />
    <link rel="icon" href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico">
    <link rel="stylesheet" type="text/css" href="../resources/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie-edge" />
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089" crossorigin="anonymous"></script>
</head>

<body>
    <nav>
        <a class="logo" href="../index.html">
            <img id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" alt="Adam Djellouli">
        </a>
        <input id="navbar-toggle" type="checkbox" />
        <ul>
            <li> <a href="../index.html"> Home </a> </li>
            <li> <a href="../core/blog.html" class="active"> Blog </a> </li>
            <li> <a href="../core/tools.html"> Tools </a> </li>
            <li> <a href="../core/projects.html"> Projects </a> </li>
            <li> <a href="../core/resume.html"> Resume </a> </li>
            <li> <a href="../core/about.html"> About </a> </li>
            <button id="dark-mode-button"></button>
        </ul>
    </nav>
    <section id="article-body">

        <h2>Neural Networks - Learning</h2>
        <p><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
        <p>Neural networks are a type of machine learning algorithm that are inspired by the structure and function of the human brain. They consist of layers of interconnected nodes, where each node represents a unit that performs a specific task. There are different types of classification problems that can be solved using neural networks, including binary classification, where the goal is to predict a single output that can take on only two values, and multi-class classification, where the goal is to predict one of multiple possible classes. The cost function is used to measure the difference between the predicted values and the actual values and is used to adjust the parameters of the neural network to improve its performance. For neural networks, the cost function is a generalization of the logistic regression cost function and involves summing over all the output units for each of the possible classes. The gradient computation involves using the chain rule to calculate the partial derivative of the cost function with respect to each parameter in the neural network. The backpropagation algorithm is then used to calculate these partial derivatives and update the parameters of the network in order to minimize the cost function.</p>
        <h2>Types of classification problems with NNs</h2>
        <ul>
            <li>Training set is ${(x^1, y^1), (x^2, y^2), ..., (x^n, y^n)}$.</li>
            <li>$L$ = number of layers in the network.</li>
            <li>$s_l$ = number of units (not counting bias unit) in layer $l$.</li>
        </ul>
        <p><img alt="big_multi_layer" src="https://user-images.githubusercontent.com/37275728/201518449-ec13fac4-0716-4131-8e5e-f0405ce075a5.png" /></p>
        <p>So here we have:</p>
        <ul>
            <li>$L=4$.</li>
            <li>$s_1 = 3$.</li>
            <li>$s_2 = 5$.</li>
            <li>$s_3 = 5$.</li>
            <li>$s_4 = 4$.</li>
        </ul>
        <h3>Binary classification</h3>
        <ul>
            <li>1 output (0 or 1).</li>
            <li>So single output node - value is going to be a real number.</li>
            <li>$k = 1$.</li>
            <li>$s_l = 1$</li>
        </ul>
        <h3>Multi-class classification</h3>
        <ul>
            <li>$k$ distinct classifications.</li>
            <li>$y$ is a $k$-dimensional vector of real numbers.</li>
            <li>$s_l = k$</li>
        </ul>
        <h2>Cost function</h2>
        <p>Logistic regression cost function is as follows:</p>
        <p>$$J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} log h_{\theta}(x^{(i)}) + (1- y^{(i)})log(1 - h_{\theta}(x^{(i)}))] + \frac{\lambda}{2m} \sum_{j=1}^{m} \theta_j^2$$</p>
        <p>For neural networks our cost function is a generalization of this equation above, so instead of one output we generate $k$ outputs:</p>
        <p>$$J(\Theta) = -\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K}[ y_k^{(i)} \log{(h_{\theta}(x^{(i)}))}<em>k + (1- y_k^{(i)}) \log(1 - {(h</em>{\theta} (x^{(i)}))}<em>k)] + \frac{\lambda}{2m} \sum</em>{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_l+1} (\Theta^{(l)}_{ji})^2$$</p>
        <ul>
            <li>Our cost function now outputs a $k$ dimensional vector.</li>
            <li>Costfunction $J(\Theta)$ is $-1/m$ times a sum of a similar term to which we had for logic regression.</li>
            <li>But now this is also a sum from $k = 1$ through to $K$ ($K$ is number of output nodes).</li>
            <li>Summation is a sum over the $k$ output units - i.e. for each of the possible classes.</li>
            <li>We don't sum over the bias terms (hence starting at 1 for the summation).</li>
        </ul>
        <h3>Partial derivative terms</h3>
        <ul>
            <li>$\Theta^{(1)}$ is the matrix of weights which define the function mapping from layer 1 to layer 2.</li>
            <li>$\Theta^{(1)}_{10}$ is the real number parameter which you multiply the bias unit (i.e. 1) with for the bias unit input into the first unit in the second layer.</li>
            <li>$\Theta^{(1)}_{11}$ is the real number parameter which you multiply the first (real) unit with for the first input into the first unit in the second layer.</li>
            <li>$\Theta^{(1)}_{21}$ is the real number parameter which you multiply the first (real) unit with for the first input into the second unit in the second layer.</li>
        </ul>
        <h3>Gradient computation</h3>
        <ul>
            <li>One training example.</li>
            <li>Imagine we just have a single pair (x,y) - entire training set.</li>
            <li>The following is how the forward propagation method works:</li>
            <li>Layer 1: $a^{(1)} = x$ and $z^{(2)} = \Theta^{(1)}a^{(1)}$.</li>
            <li>Layer 2: $a^{(2)} = g(z^{(2)}) + a^{(2)}_0$ and $z^{(3)} = \Theta^{(2)}a^{(2)}$.</li>
            <li>Layer 3: $a^{(3)} = g(z^{(3)}) + a^{(3)}_0$ and $z^{(4)} = \Theta^{(3)}a^{(3)}$.</li>
            <li>Ouptut: $a^{(4)} = h_{\Theta}(x) = g(z^{(4)})$.</li>
        </ul>
        <p><img alt="gradient_computing" src="https://user-images.githubusercontent.com/37275728/201518441-7740e76d-9a6b-426f-98ad-85a5ff207a89.png" /></p>
        <h3>Calculate backpropagation</h3>
        <ul>
            <li>$\delta_j$ is $Lx1$ vector.</li>
            <li>First we compute the LAST element: $\delta^{(L)}_j = a^{(L)}_j - y_j$.</li>
            <li>Value of each element is based on the value of the next element:</li>
        </ul>
        <p>$$\delta^{(l)}_j = (\Theta^{(l)}_j)^T\delta^{(l+1)}_j \cdot g'(z^{(l)}_j)$$</p>
        <ul>
            <li>Finally, use $\Delta$ to accumulate the partial derivative terms:</li>
        </ul>
        <p>$$\Delta^{(l)}<em>{ij} := \Delta^{(l)}</em>{ij} + a^{(l)}_j\delta^{(l+1)}_i$$</p>
        <ul>
            <li>$l$ = layer.</li>
            <li>$j$ = node in that layer.</li>
            <li>$i$ = the error of the affected node in the target layer</li>
        </ul>
        <p>$$
            \frac{\partial}{\partial \Theta^{(l)}<em>{ij}}J(\Theta) = \begin{cases}
                \frac{1}{m} \Delta^{(l)}</em>{ij} + \lambda \Theta ^{(l)}<em>{ij} \quad &amp;\text{if} \, j \neq 0 \
                \frac{1}{m} \Delta^{(l)}</em>{ij} \quad &amp;\text{if} \, j=0 \
            \end{cases}
            $$</p>
        <h3>Gradient checking</h3>
        <p>Backpropagation contains a lot of details, and tiny flaws can break it.
            As a result, employing a numerical approach to verify the gradient can aid in the quick diagnosis of a problem.</p>
        <ul>
            <li>Have a function $J(\Theta)$.</li>
            <li>Compute $\Theta + \epsilon$.</li>
            <li>Compute $\Theta - \epsilon$.</li>
            <li>Join them by a straight line.</li>
            <li>Use the slope of that line as an approximation to the derivative.</li>
        </ul>
        <p><img alt="gradient_checking" src="https://github.com/djeada/Stanford-Machine-Learning/blob/main/slides/resources/gradient_checking.png" /></p>

    </section>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" alt="Adam Djellouli">

            </div>
            <div class="footer-column">

                <p>
                    Thank you for visiting my personal website. All of the </br>
                    content on this site is free to use, but please remember </br>
                    to be a good human being and refrain from any abuse</br>
                    of the site. If you would like to contact me, please use </br>
                    my LinkedIn profile or my GitHub if you have any technical </br>
                    issues or ideas to share. I wish you the best and hope you </br>
                    have a fantastic life. </br>
                </p>

            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" class="fa fa-youtube" target="_blank">

                        </a>YouTube
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" class="fa fa-linkedin" target="_blank">

                        </a>LinkedIn
                    </li>
                    <li>
                        <a href="https://www.instagram.com/addjellouli/" class="fa fa-instagram" target="_blank">
                        </a>Instagram

                    </li>
                    <li>
                        <a href="https://github.com/djeada" class="fa fa-github">
                        </a>Github

                    </li>

                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                &copy; Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../app.js"></script>
    </footer>
</body>

</html>

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>