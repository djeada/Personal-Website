<!DOCTYPE html>
<html lang="en">

<head>
    <title>Adam Djellouli - Blog</title>
    <meta charset="utf-8">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" />
    <link rel="icon" href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico">
    <link rel="stylesheet" type="text/css" href="../resources/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie-edge" />
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089" crossorigin="anonymous"></script>
</head>

<body>
    <nav>
        <a class="logo" href="../index.html">
            <img id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" alt="Adam Djellouli">
        </a>
        <input id="navbar-toggle" type="checkbox" />
        <ul>
            <li> <a href="../index.html"> Home </a> </li>
            <li> <a href="../core/blog.html" class="active"> Blog </a> </li>
            <li> <a href="../core/tools.html"> Tools </a> </li>
            <li> <a href="../core/projects.html"> Projects </a> </li>
            <li> <a href="../core/resume.html"> Resume </a> </li>
            <li> <a href="../core/about.html"> About </a> </li>
            <button id="dark-mode-button"></button>
        </ul>
    </nav>
    <section id="article-body">

        <h2>Linear Regression</h2>
        <p><i>This article is written in: üá∫üá∏</i></p>
        <p>The previously described home price data example is an example of a supervised learning regression problem.</p>
        <h3>Notation (used throughout the course)</h3>
        <ul>
            <li>m = number of training examples.</li>
            <li>$x$ = input variables / features.</li>
            <li>$y$ = output variable ‚Äùtarget‚Äù variables.</li>
            <li>$(x, y)$ - single training example.</li>
            <li>$(x^i, y^i)$ - specific example (ith training example).</li>
        </ul>
        <p><img alt="house price table" src="https://github.com/djeada/Stanford-Machine-Learning/blob/main/slides/resources/house_price_table.png" /></p>
        <h3>With our training set defined - how do we use it?</h3>
        <ul>
            <li>Take training set.</li>
            <li>Pass into a learning algorithm.</li>
            <li>Algorithm outputs a function (h = hypothesis).</li>
            <li>This function takes an input (e.g. size of new house).</li>
            <li>Tries to output the estimated value of Y.</li>
        </ul>
        <p>$$h_{\theta}(x) = \theta_0 + \theta_1x$$</p>
        <ul>
            <li>Y is a linear function of x!</li>
            <li>$\theta_0$ is zero condition.</li>
            <li>$\theta_1$ is gradient.</li>
        </ul>
        <h3>Cost function</h3>
        <ul>
            <li>A cost function expresses how dissatisfied we are with the model's present coefficients in the prediction of output y from input x. </li>
            <li>We may use the cost function to determine how to fit the best straight line to our data.</li>
            <li>We want to want to solve a minimization problem. Minimize $(h_{\theta}(x) - y)^2$.</li>
            <li>Sum this over the training set.</li>
        </ul>
        <p>$$J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{m}^{i=1}(h_{\theta}(x^{(i)}) - y^{(i)})^2$$</p>
        <p>For $\theta_0 = 0$ we have:</p>
        <p>$$h_{\theta}(x) = \theta_1x\quad and \quad J(\theta_1) = \frac{1}{2m} \sum_{m}^{i=1}(h_{\theta}(x^{(i)}) - y^{(i)})^2$$</p>
        <p>Data:
            * $\theta_1 = 1$ and $J(\theta_1)= 0$.
            * $\theta_1 = 0.5$ and $J(\theta_1)= 0.58$.
            * $\theta_1 = 0$ and $J(\theta_1)= 2.3$.</p>
        <p><img alt="cost_function" src="https://github.com/djeada/Stanford-Machine-Learning/blob/main/slides/resources/cost_function.png" /></p>
        <p>The optimization objective for the learning algorithm is find the value of Œ∏1
            which minimizes J(Œ∏1 ). So, here Œ∏1 = 1 is the best value for Œ∏1 .</p>
        <h3>A deeper insight into the cost function - simplified cost function</h3>
        <p>The real cost function takes two variables as parameters! $J(\theta_0, \theta_1)$.
            We can now generates a 3D surface plot where axis are:</p>
        <ul>
            <li>$X = \theta_1$.</li>
            <li>$Z = \theta_0$.</li>
            <li>$Y = J(\theta_0,\theta_1)$.</li>
        </ul>
        <p><img alt="surface_cost_function" src="https://github.com/djeada/Stanford-Machine-Learning/blob/main/slides/resources/surface_cost_function.png" /></p>
        <p>The best hypothesis is at the bottom of the bowl.
            Instead of a surface plot we can use a contour figures/plots.
            * Set of ellipses in different colors.
            * Each colour is the same value of $J(\theta_0,\theta_1)$, but obviously plot to different
            locations because Œ∏1 and Œ∏0 will vary.
            * Imagine a bowl shape function coming out of the screen so the middle is
            the concentric circles.</p>
        <p><img alt="contour_cost_function" src="https://github.com/djeada/Stanford-Machine-Learning/blob/main/slides/resources/contour_cost_function.png" /></p>
        <p>The best hypothesis is located in the center of the contour plot.</p>
        <h2>Gradient descent algorithm</h2>
        <pre><code>\theta = [0, 0]
while not converged:
  for j in [0, 1]:
      \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)
</code></pre>
        <ul>
            <li>Begin with initial guesses, could be (0,0) or any other value.</li>
            <li>Repeatedly change values of $\theta_0$ and $\theta_1$ to try to reduce $J(\theta_0, \theta_1)$.</li>
            <li>Continue until you reach a local minimum.</li>
            <li>Reached minimum is determined by the starting point.</li>
        </ul>
        <p><img alt="gradient_descent" src="https://github.com/djeada/Stanford-Machine-Learning/blob/main/slides/resources/gradient_descent.png" /></p>
        <p><img alt="gradient_descent" src="https://user-images.githubusercontent.com/37275728/201476896-555ad8c4-8422-428b-937f-12cdf70d75bd.png" /></p>
        <p>Two key terms in the algorithm:
            * $\alpha$ term.
            * Derivative term.</p>
        <h3>Partial derivative vs. derivative</h3>
        <ul>
            <li>Use partial derivative when we have multiple variables but only derive with respect to one.</li>
            <li>Use derivative when we are deriving with respect to all the variables.</li>
        </ul>
        <p>Derivative says:
            * Let‚Äôs look at the slope of the line by taking the tangent at the point.
            * As a result, going towards the minimum (down) will result in a negative derivative; nevertheless, alpha is always positive, thus $J(\theta_1)$ will be updated to a lower value.
            * Similarly, as we progress up a slope, we increase the value of $J(\theta_1)$.</p>
        <p>$\alpha$ term
            * If it‚Äôs too small, it takes too long to converge.
            * If it is too large, it may exceed the minimum and fail to converge.</p>
        <p>When you get to a local minimum
            * Gradient of tangent/derivative is 0.
            * So derivative term = 0.
            * $\alpha \cdot 0 = 0$.
            * So $\theta_1 = \theta_1 - 0$.
            * So $\theta_1$ remains the same.</p>
        <h2>Linear regression with gradient descent</h2>
        <p>Apply gradient descent to minimize the squared error cost function $J(\theta_0, \theta_1)$.</p>
        <p>$$\frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1) = \frac{\partial}{\partial \theta_j} \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^2$$</p>
        <p>$$= \frac{\partial}{\partial \theta_j} \frac{1}{2m} \sum_{i=1}^{m} (\theta_0 + \theta_1x^{(i)} - y^{(i)})^2$$</p>
        <p>For each case, we must determine the partial derivative:</p>
        <p>$$j=0:\frac{\partial}{\partial \theta_0} J(\theta_0, \theta_1)=\frac{\partial}{\partial \theta_0} \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})$$</p>
        <p>$$j=1:\frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1)=\frac{\partial}{\partial \theta_1} \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}$$</p>
        <h2>Normal equations method</h2>
        <ul>
            <li>To solve the minimization problem we can solve it $[ min J(\theta_0, \theta_1) ]$ exactly using a numerical method which avoids the iterative approach used by gradient descent.</li>
            <li>Can be much faster for some problems, but it is much more complicated (will be covered in detail later).</li>
        </ul>
        <h2>Extension of the current model</h2>
        <p>We could learn with a larger number of features.
            * e.g. with houses: Size, Age, Number bedrooms, Number floors...
            * The disadvantage is that we can't plot in more than three dimensions.
            * Best way to get around with this is the notation of linear algebra (matrices and vectors).</p>

    </section>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" alt="Adam Djellouli">

            </div>
            <div class="footer-column">

                <p>
                    Thank you for visiting my personal website. All of the </br>
                    content on this site is free to use, but please remember </br>
                    to be a good human being and refrain from any abuse</br>
                    of the site. If you would like to contact me, please use </br>
                    my LinkedIn profile or my GitHub if you have any technical </br>
                    issues or ideas to share. I wish you the best and hope you </br>
                    have a fantastic life. </br>
                </p>

            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" class="fa fa-youtube" target="_blank">

                        </a>YouTube
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" class="fa fa-linkedin" target="_blank">

                        </a>LinkedIn
                    </li>
                    <li>
                        <a href="https://www.instagram.com/addjellouli/" class="fa fa-instagram" target="_blank">
                        </a>Instagram

                    </li>
                    <li>
                        <a href="https://github.com/djeada" class="fa fa-github">
                        </a>Github

                    </li>

                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                &copy; Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../app.js"></script>
    </footer>
</body>

</html>

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>