<!DOCTYPE html>
<html lang="en">

<head>
    <title>Adam Djellouli - Blog</title>
    <meta charset="utf-8">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" />
    <link rel="icon" href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico">
    <link rel="stylesheet" type="text/css" href="../resources/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie-edge" />
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089" crossorigin="anonymous"></script>
</head>

<body>
    <nav>
        <a class="logo" href="../index.html">
            <img id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" alt="Adam Djellouli">
        </a>
        <input id="navbar-toggle" type="checkbox" />
        <ul>
            <li> <a href="../index.html"> Home </a> </li>
            <li> <a href="../core/blog.html" class="active"> Blog </a> </li>
            <li> <a href="../core/tools.html"> Tools </a> </li>
            <li> <a href="../core/projects.html"> Projects </a> </li>
            <li> <a href="../core/resume.html"> Resume </a> </li>
            <li> <a href="../core/about.html"> About </a> </li>
            <button id="dark-mode-button"></button>
        </ul>
    </nav>
    <section id="article-body">
        <p style='text-align: right;'><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>

        <h2>Regularization</h2>
        <p>Overfitting is a common problem in machine learning, where a model performs well on the training data but poorly on new, unseen data. One way to prevent overfitting is through regularization, which involves adding a penalty term to the cost function to reduce the complexity of the model. In linear regression, this can be done by adding a term that penalizes large coefficients. In logistic regression, the cost function can be regularized by adding a term that penalizes large coefficients. This helps to keep the model simple and improve its generalization to new data. The strength of the regularization term is controlled by the parameter lambda. Automated methods can be used to select an appropriate value for lambda. In both linear and logistic regression, the gradient descent algorithm can be modified to include the regularization term. The normal equation can also be used to solve for the optimal coefficients in regularized linear regression.</p>
        <h2>Overfitting with linear regression</h2>
        <ul>
            <li>The same thing may happen with logistic regression.</li>
            <li>Sigmoidal function is an underfit.</li>
            <li>A high order polynomial, on the other hand, results in overfitting (high variance hypothesis).</li>
            <li>Regularization prevents the model from overfitting to the training data.</li>
        </ul>
        <p><img alt="overfitting_logistic_regression" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/overfitting_logistic_regression.png" /></p>
        <h2>Cost function optimization for regularization</h2>
        <p>Penalize and make some of the $\theta$ parameters really small.</p>
        <p>$$min \frac{1}{2m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)} - y^{(i)})^2 + 1000 \theta_3^2 + 1000 \theta_4^2$$</p>
        <p>So we simply end up with $\theta_3$ and $\theta_4$ being near to zero (because of the huge constants) and we're basically left with a quadratic function.</p>
        <p><img alt="optimization_regularization" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/optimization_regularization.png" /></p>
        <h3>Regularization</h3>
        <p>Small parameter values correlate to a simpler hypothesis (you effectively get rid of some of the terms). Overfitting is less likely with a simpler hypothesis.</p>
        <p>$$J(\theta) = \frac{1}{2m} [ \sum_{i=1}^{m}(h_{\theta}(x^{(i)} - y^{(i)})^2 + \lambda \sum_{j=1}^{m} \theta_j^2] $$</p>
        <p>$\lambda$ is the regularization parameter that controls a trade off between our two goals:</p>
        <ul>
            <li>Want to fit the training set well.</li>
            <li>Want to keep parameters small.</li>
        </ul>
        <p>Later in the course, we'll look at various automated methods for selecting $\lambda$.</p>
        <p>The gradient of the cost function is a vector where the j th element is defined as follows:</p>
        <p>$$\frac{\partial}{\partial \theta_0} J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} \quad for\;j=0$$</p>
        <p>$$\frac{\partial}{\partial \theta_j} J(\theta) = (\frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}) + \frac{\lambda}{m}\theta_j \quad for\;j\ge1$$</p>
        <h2>Regularized linear regression</h2>
        <pre><div><pre><code class="language-while">not converged:
  for j in [0, ..., n]:
      \theta_j := \theta_j - \alpha [\frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)} + y^{(i)})x_j^{(i)} + \frac{\lambda}{m} \theta_j]</code></pre>
        </div>
        </pre>
        <h2>Regularization with the normal equation</h2>
        <p><img alt="regularized_normal_equation" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/regularized_normal_equation.png" /></p>
        <h2>Regularized logistic regression</h2>
        <p>Logistic regression cost function is as follows:</p>
        <p>$$J(\theta) = \frac{1}{m} \sum_{i=1}^{m}[-y^{(i)}log(h_{\theta}(x^{(i)})) - (1-y^{(i)})log(1 - h_{\theta}(x^{(i)}))] + \frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2$$</p>
        <p>The gradient of the cost function is a vector where the j th element is defined as follows:</p>
        <p>$$\frac{\partial}{\partial \theta_0} J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} \quad for\;j=0$$</p>
        <p>$$\frac{\partial}{\partial \theta_j} J(\theta) = (\frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}) + \frac{\lambda}{m}\theta_j \quad for\;j\ge1$$</p>
        <pre><div><pre><code class="language-while">not converged:
  for j in [0, ..., n]:
      \theta_j := \theta_j - \alpha [\frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)} + y^{(i)})x_j^{(i)} + \frac{\lambda}{m} \theta_j]</code></pre>
        </div>
        </pre>
        <p>It appears to be the same as linear regression, except that the hypothesis is different.</p>

    </section>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" alt="Adam Djellouli">

            </div>
            <div class="footer-column">

                <p>
                    Thank you for visiting my personal website. All of the </br>
                    content on this site is free to use, but please remember </br>
                    to be a good human being and refrain from any abuse</br>
                    of the site. If you would like to contact me, please use </br>
                    my LinkedIn profile or my GitHub if you have any technical </br>
                    issues or ideas to share. I wish you the best and hope you </br>
                    have a fantastic life. </br>
                </p>

            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" class="fa fa-youtube" target="_blank">

                        </a>YouTube
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" class="fa fa-linkedin" target="_blank">

                        </a>LinkedIn
                    </li>
                    <li>
                        <a href="https://www.instagram.com/addjellouli/" class="fa fa-instagram" target="_blank">
                        </a>Instagram

                    </li>
                    <li>
                        <a href="https://github.com/djeada" class="fa fa-github">
                        </a>Github

                    </li>

                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                &copy; Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../app.js"></script>
    </footer>
</body>

</html>

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"], ["\(","\)"] ], displayMath: [ ["$$","$$"], ["\[", "\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>

<script type="text/javascript" id="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>