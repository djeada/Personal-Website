<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <title>Regularization</title>
    <meta charset="utf-8" />
    <meta content="Overfitting is a common problem in machine learning, where a model performs well on the training data but poorly on new, unseen data." name="description" />
    <meta content="Adam Djellouli" name="keywords" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../resources/style.css" rel="stylesheet" type="text/css" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <link href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="ie-edge" http-equiv="X-UA-Compatible" />
</head>

<body>
    <nav>
        <a class="logo" href="../index.html" title="Adam Djellouli - Home">
            <img alt="Adam Djellouli Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul>
            <li> <a href="../index.html" title="Home"> Home </a> </li>
            <li> <a class="active" href="../core/blog.html" title="Adam Djellouli's Blog - Programming, technology and more"> Blog </a> </li>
            <li> <a href="../core/tools.html" title="Useful Tools by Adam Djellouli"> Tools </a> </li>
            <li> <a href="../core/projects.html" title="Projects by Adam Djellouli"> Projects </a> </li>
            <li> <a href="../core/resume.html" title="Adam Djellouli's Resume"> Resume </a> </li>
            <li> <a href="../core/about.html" title="About Adam Djellouli"> About </a> </li>
            <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
        </ul>
    </nav>
    <section id="article-body"></section>
    <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
    <div id="article-wrapper">
        <section id="article-body">

            <h2 id="regularization">Regularization</h2>
            <p>Overfitting is a common problem in machine learning, where a model performs well on the training data but poorly on new, unseen data. One way to prevent overfitting is through regularization, which involves adding a penalty term to the cost function to reduce the complexity of the model. In linear regression, this can be done by adding a term that penalizes large coefficients. In logistic regression, the cost function can be regularized by adding a term that penalizes large coefficients. This helps to keep the model simple and improve its generalization to new data. The strength of the regularization term is controlled by the parameter lambda. Automated methods can be used to select an appropriate value for lambda. In both linear and logistic regression, the gradient descent algorithm can be modified to include the regularization term. The normal equation can also be used to solve for the optimal coefficients in regularized linear regression.</p>
            <h2 id="overfitting-with-linear-regression">Overfitting with linear regression</h2>
            <ul>
                <li>The same thing may happen with logistic regression.</li>
                <li>Sigmoidal function is an underfit.</li>
                <li>A high order polynomial, on the other hand, results in overfitting (high variance hypothesis).</li>
                <li>Regularization prevents the model from overfitting to the training data.</li>
            </ul>
            <p><img alt="overfitting_logistic_regression" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/overfitting_logistic_regression.png" /></p>
            <h2 id="cost-function-optimization-for-regularization">Cost function optimization for regularization</h2>
            <p>Penalize and make some of the $\theta$ parameters really small.</p>
            <p>$$min \frac{1}{2m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)} - y^{(i)})^2 + 1000 \theta_3^2 + 1000 \theta_4^2$$</p>
            <p>So we simply end up with $\theta_3$ and $\theta_4$ being near to zero (because of the huge constants) and we're basically left with a quadratic function.</p>
            <p><img alt="optimization_regularization" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/optimization_regularization.png" /></p>
            <h3 id="regularization">Regularization</h3>
            <p>Small parameter values correlate to a simpler hypothesis (you effectively get rid of some of the terms). Overfitting is less likely with a simpler hypothesis.</p>
            <p>$$J(\theta) = \frac{1}{2m} [ \sum_{i=1}^{m}(h_{\theta}(x^{(i)} - y^{(i)})^2 + \lambda \sum_{j=1}^{m} \theta_j^2] $$</p>
            <p>$\lambda$ is the regularization parameter that controls a trade off between our two goals:</p>
            <ul>
                <li>Want to fit the training set well.</li>
                <li>Want to keep parameters small.</li>
            </ul>
            <p>Later in the course, we'll look at various automated methods for selecting $\lambda$.</p>
            <p>The gradient of the cost function is a vector where the j th element is defined as follows:</p>
            <p>$$\frac{\partial}{\partial \theta_0} J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} \quad for\;j=0$$</p>
            <p>$$\frac{\partial}{\partial \theta_j} J(\theta) = (\frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}) + \frac{\lambda}{m}\theta_j \quad for\;j\ge1$$</p>
            <h2 id="regularized-linear-regression">Regularized linear regression</h2>
            <pre><div><pre><code class="language-while">not converged:
  for j in [0, ..., n]:
      \theta_j := \theta_j - \alpha [\frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)} + y^{(i)})x_j^{(i)} + \frac{\lambda}{m} \theta_j]</code></pre>
    </div>
    </pre>
    <h2 id="regularization-with-the-normal-equation">Regularization with the normal equation</h2>
    <p><img alt="regularized_normal_equation" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/regularized_normal_equation.png" /></p>
    <h2 id="regularized-logistic-regression">Regularized logistic regression</h2>
    <p>Logistic regression cost function is as follows:</p>
    <p>$$J(\theta) = \frac{1}{m} \sum_{i=1}^{m}[-y^{(i)}log(h_{\theta}(x^{(i)})) - (1-y^{(i)})log(1 - h_{\theta}(x^{(i)}))] + \frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2$$</p>
    <p>The gradient of the cost function is a vector where the j th element is defined as follows:</p>
    <p>$$\frac{\partial}{\partial \theta_0} J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} \quad for\;j=0$$</p>
    <p>$$\frac{\partial}{\partial \theta_j} J(\theta) = (\frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}) + \frac{\lambda}{m}\theta_j \quad for\;j\ge1$$</p>
    <pre><div><pre><code class="language-while">not converged:
  for j in [0, ..., n]:
      \theta_j := \theta_j - \alpha [\frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)} + y^{(i)})x_j^{(i)} + \frac{\lambda}{m} \theta_j]</code></pre>
    </div>
    </pre>
    <p>It appears to be the same as linear regression, except that the hypothesis is different.</p>
    </section>
    <div id="table-of-contents">
        <h2>Table of Contents</h2>
        <ol>
            <li><a href="#regularization">Regularization</a></li>
            <li><a href="#overfitting-with-linear-regression">Overfitting with linear regression</a></li>
            <li><a href="#cost-function-optimization-for-regularization">Cost function optimization for regularization</a></li>
            <li><a href="#regularization">Regularization</a></li>
            <li><a href="#regularized-linear-regression">Regularized linear regression</a></li>
            <li><a href="#regularization-with-the-normal-equation">Regularization with the normal equation</a></li>
            <li><a href="#regularized-logistic-regression">Regularized logistic regression</a></li>
        </ol>
    </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/addjellouli/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../app.js"></script>
    </footer>
</body>

</html>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>