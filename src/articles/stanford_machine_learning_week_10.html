<!DOCTYPE html>
<html lang="en">

<head>
    <title>Adam Djellouli - Blog</title>
    <meta charset="utf-8">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" />
    <link rel="icon" href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico">
    <link rel="stylesheet" type="text/css" href="../resources/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie-edge" />
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089" crossorigin="anonymous"></script>
</head>

<body>
    <nav>
        <a class="logo" href="../index.html">
            <img id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" alt="Adam Djellouli">
        </a>
        <input id="navbar-toggle" type="checkbox" />
        <ul>
            <li> <a href="../index.html"> Home </a> </li>
            <li> <a href="../core/blog.html" class="active"> Blog </a> </li>
            <li> <a href="../core/tools.html"> Tools </a> </li>
            <li> <a href="../core/projects.html"> Projects </a> </li>
            <li> <a href="../core/resume.html"> Resume </a> </li>
            <li> <a href="../core/about.html"> About </a> </li>
            <button id="dark-mode-button"></button>
        </ul>
    </nav>
    <section id="article-body">

        <h2>Advice for applying machine learning techniques</h2>
        <p><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
        <p>If you're having trouble with your machine learning model producing high errors when tested on new data, there are several steps you can take to troubleshoot the problem. You can try adding more training data or features, or you can try changing the value of the regularization parameter. You can also split your data into a training set and a test set to evaluate the model's performance. Another option is to use a technique called "model selection" and create a training, validation, and test set to identify the best performing model. If your model is underperforming, it may be due to either high bias (underfitting) or high variance (overfitting). You can diagnose the issue by plotting the error for both the training and validation set as a function of the polynomial degree. If all else fails, you can try using an advanced optimization algorithm to minimize the cost function and improve the performance of your model.</p>
        <h2>Debugging a learning algorithm</h2>
        <p>Imagine you've used regularized linear regression to forecast home prices:</p>
        <p>$$J(\theta) = \frac{1}{2m} [ \sum_{i=1}^{m}(h_{\theta}(x^{(i)} + y^{(i)})^2 + \lambda \sum_{j=1}^{m} \theta_j^2] $$</p>
        <ul>
            <li>Trained it.</li>
            <li>However, when tested on new data, it produces unacceptably high errors in its predictions.</li>
            <li>What should your next step be? <ul>
                    <li>Obtain additional training data.</li>
                    <li>Try a smaller set of features.</li>
                    <li>Consider getting more features.</li>
                    <li>Add polynomial features.</li>
                    <li>Change the value of $\lambda$.</li>
                </ul>
            </li>
        </ul>
        <h2>Evaluating the hypothesis</h2>
        <ul>
            <li>Split data into two portions: training set and test set.</li>
            <li>Learn parameters $\theta$ from training data, minimizing $J(\theta)$ using 70\% of the training data.</li>
            <li>Compute the test error.</li>
        </ul>
        <p>$$J_{test}(\theta) = \frac{1}{2m_{test}} \sum_{i=1}^{m_{test}}(h_{\theta}(x^{(i)}<em>{test} + y^{(i)}</em>{test})^2$$</p>
        <h2>Model selection and training validation test sets</h2>
        <ul>
            <li>How should a regularization parameter or polynomial degree be chosen?</li>
            <li>We've previously discussed the issue of overfitting.</li>
            <li>This is why, in general, training set error is a poor predictor of hypothesis accuracy for new data (generalization).</li>
            <li>
                <p>Try to determine the degree of polynomial that will fit data.</p>
                <ol>
                    <li>
                        <p>$h_{\theta}(x) = \theta_0 + \theta_1x$</p>
                    </li>
                    <li>
                        <p>$h_{\theta}(x) = \theta_0 + \theta_1x + \theta_2x^2$</p>
                    </li>
                    <li>
                        <p>$h_{\theta}(x) = \theta_0 + ... + \theta_3x^3$</p>
                    </li>
                </ol>
                <p>$$\vdots$$</p>
                <ol>
                    <li>$h_{\theta}(x) = \theta_0 + ... + \theta_{10}x^{10}$</li>
                </ol>
            </li>
            <li>
                <p>Introduce a new parameter d, which represents the degree of polynomial you want to use.</p>
            </li>
            <li>Model 1 is minimized using training data, resulting in a parameter vector $\theta^1$ (where d =1).</li>
            <li>Same goes for other models up to $n$.</li>
            <li>Using the previous formula, examine the test set error for each computed parameter $J_{test}(\theta^k)$.</li>
            <li>Minimize cost function for each of the models as before.</li>
            <li>Test these hypothesis on the cross validation set to generate the cross validation error.</li>
            <li>Pick the hypothesis with the lowest cross validation error.</li>
        </ul>
        <p>Training error:</p>
        <p>$$J_{train}(\theta) = \frac{1}{2m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)} + y^{(i)})^2$$</p>
        <p>Cross Validation error:</p>
        <p>$$J_{cv}(\theta) = \frac{1}{2m_{cv}} \sum_{i=1}^{m_{cv}}(h_{\theta}(x^{(i)}<em>{cv} + y^{(i)}</em>{cv})^2$$</p>
        <p>Test error:</p>
        <p>$$J_{test}(\theta) = \frac{1}{2m_{test}} \sum_{i=1}^{m_{test}}(h_{\theta}(x^{(i)}<em>{test} + y^{(i)}</em>{test})^2$$</p>
        <h2>Model selection and training validation test sets</h2>
        <p>Bad results are generally the consequence of one of the following:</p>
        <ul>
            <li>High bias - under fitting problem.</li>
            <li>High variance - over fitting problem.</li>
        </ul>
        <p><img alt="diagnosis" src="https://github.com/djeada/Stanford-Machine-Learning/blob/main/slides/resources/diagnosis.png" /></p>
        <p>Now plot</p>
        <ul>
            <li>$x$ = degree of polynomial d</li>
            <li>$y$ = error for both training and cross validation (two lines)</li>
        </ul>
        <p><img alt="error_vs_d" src="https://github.com/djeada/Stanford-Machine-Learning/blob/main/slides/resources/error_vs_d.png" /></p>
        <ul>
            <li>For the high bias case, we find both cross validation and training error are high</li>
            <li>For high variance, we find the cross validation error is high but training error is low</li>
        </ul>
        <h2>Regularization and bias/variance</h2>
        <p>Linear regression with regularization:</p>
        <p>$$h_{\theta}(x) = \theta_0 + \theta_1x + \theta_2x^2 + \theta_3x^3 + \theta_4x^4$$</p>
        <p>$$J(\theta) = \frac{1}{2m} [ \sum_{i=1}^{m}(h_{\theta}(x^{(i)} + y^{(i)})^2 + \lambda \sum_{j=1}^{m} \theta_j^2]$$</p>
        <p>The above equation describes the fitting of a high order polynomial with regularization (used to keep parameter values small).</p>
        <ul>
            <li>$\lambda$ is large (high bias $-&gt;$ under fitting data)</li>
            <li>$\lambda$ is intermediate (good)</li>
            <li>$\lambda$ is small (high variance $-&gt;$ overfitting)</li>
        </ul>
        <p><img alt="lambda" src="https://github.com/djeada/Stanford-Machine-Learning/blob/main/slides/resources/lambda.png" /></p>
        <ul>
            <li>Have a set or range of values to use (for example from 0 to 15).</li>
            <li>For each $\lambda_i$ minimize the cost function. Result is $\theta^{(i)}$.</li>
            <li>For each $\theta^{(i)}$ measure average squared error on cross validation set.</li>
            <li>Pick the model which gives the lowest error.</li>
        </ul>
        <h2>Learning curves</h2>
        <p>Plot $J_{train}$ (average squared error on training set) and $J_{cv}$ (average squared error on cross validation set) against m (number of training examples).</p>
        <ul>
            <li>$J_{train}$ on smaller sample sizes is smaller (as less variance to accommodate).</li>
            <li>As training set grows your hypothesis generalize better and $J_{cv}$ gets smaller.</li>
        </ul>
        <p><img alt="learning_curve" src="https://github.com/djeada/Stanford-Machine-Learning/blob/main/slides/resources/learning_curve.png" /></p>
        <ul>
            <li>A small gap between training error and cross validation error might indicate high bias. Here, more data will not help.</li>
            <li>A large gap between training error and cross validation error might indicate high variance. Here, more data will probably help.</li>
        </ul>

    </section>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" alt="Adam Djellouli">

            </div>
            <div class="footer-column">

                <p>
                    Thank you for visiting my personal website. All of the </br>
                    content on this site is free to use, but please remember </br>
                    to be a good human being and refrain from any abuse</br>
                    of the site. If you would like to contact me, please use </br>
                    my LinkedIn profile or my GitHub if you have any technical </br>
                    issues or ideas to share. I wish you the best and hope you </br>
                    have a fantastic life. </br>
                </p>

            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" class="fa fa-youtube" target="_blank">

                        </a>YouTube
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" class="fa fa-linkedin" target="_blank">

                        </a>LinkedIn
                    </li>
                    <li>
                        <a href="https://www.instagram.com/addjellouli/" class="fa fa-instagram" target="_blank">
                        </a>Instagram

                    </li>
                    <li>
                        <a href="https://github.com/djeada" class="fa fa-github">
                        </a>Github

                    </li>

                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                &copy; Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../app.js"></script>
    </footer>
</body>

</html>

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>