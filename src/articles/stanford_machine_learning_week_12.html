<!DOCTYPE html>
<html lang="en">

<head>
    <title>Adam Djellouli - Blog</title>
    <meta charset="utf-8">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" />
    <link rel="icon" href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico">
    <link rel="stylesheet" type="text/css" href="../resources/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie-edge" />
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089" crossorigin="anonymous"></script>
</head>

<body>
    <nav>
        <a class="logo" href="../index.html">
            <img id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" alt="Adam Djellouli">
        </a>
        <input id="navbar-toggle" type="checkbox" />
        <ul>
            <li> <a href="../index.html"> Home </a> </li>
            <li> <a href="../core/blog.html" class="active"> Blog </a> </li>
            <li> <a href="../core/tools.html"> Tools </a> </li>
            <li> <a href="../core/projects.html"> Projects </a> </li>
            <li> <a href="../core/resume.html"> Resume </a> </li>
            <li> <a href="../core/about.html"> About </a> </li>
            <button id="dark-mode-button"></button>
        </ul>
    </nav>
    <section id="article-body">

        <h2>Support Vector Machines</h2>
        <p><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
        <p>In this article, we explore how to use logistic regression cost functions to create support vector machine (SVM) cost functions. We start by introducing logistic regression cost functions and how they can be used to make predictions about data. Then, we show how the SVM cost functions are derived from the logistic regression cost functions by replacing the logistic regression terms with $cost_1(\theta^Tx)$ and $cost_0(\theta^Tx)$. We also explain how adjusting the value of C in the SVM cost function can impact the bias and variance of the resulting hypothesis, with larger values of C leading to higher variance and lower bias, and smaller values leading to lower variance and higher bias. Finally, we discuss the concept of large margin classification, which involves trying to find a hypothesis that separates the classes as widely as possible.</p>
        <h2>An alternative view of logistic regression</h2>
        <p>As previously stated, the logistic regression hypothesis is as follows:</p>
        <p>$$h_{\theta}(x) = \frac{1}{1+e^{-\theta^Tx}}$$</p>
        <p>We have an example in which $y = 1$. We expect that $h_{\theta}(x)$ is close to 1.</p>
        <p><img alt="sigmoid" src="https://github.com/djeada/Stanford-Machine-Learning/blob/main/slides/resources/sigmoid2.png" /></p>
        <p>When you look at the cost function, you'll see that each example contributes a term like the one below to the total cost function.</p>
        <p>$$-(ylogh_{\theta}(x)+(1-y)log(1-h_{\theta}(x)))$$</p>
        <p>After plugging in the hypothesis function $h_{\theta}(x)$, you obtain an enlarged cost function equation:</p>
        <p>$$-ylog\frac{1}{1+e^{-\theta^Tx}}-(1-y)log(1-\frac{1}{1+e^{-\theta^Tx}})$$</p>
        <p><img alt="log_function" src="https://user-images.githubusercontent.com/37275728/201519577-c93854b4-1270-4082-9d9b-da0d543b0375.png" /></p>
        <ul>
            <li>As a result, if z is large, the cost is small.</li>
            <li>If z is 0 or negative, however, the cost contribution is large..</li>
            <li>This is why, when logistic regression encounters a positive case, it attempts to make $\theta^Tx$ a very big term.</li>
        </ul>
        <h2>SVM cost functions from logistic regression cost functions</h2>
        <ul>
            <li>Instead of a curved line, draw two straight lines (magenta) to approximate the logistic regression y = 1 function.</li>
            <li>Flat when cost is 0.</li>
            <li>Straight growing line after 1.</li>
            <li>So this is the new y=1 cost function, which provides the SVM with a computational advantage and makes optimization easier.</li>
        </ul>
        <p><img alt="svm_cost" src="https://github.com/djeada/Stanford-Machine-Learning/blob/main/slides/resources/svm_cost.png" /></p>
        <p>Logistic regression cost function:</p>
        <p>$$J(\theta) = -\frac{1}{m} \sum_{i=1}^{m}[ y^{(i)} log h_{\theta}(x^{(i)}) + (1- y^{(i)})log(1 - h_{\theta}(x^{(i)}))] + \frac{\lambda}{2m} \sum_{j=1}^{m} \theta_j^2$$</p>
        <p>For the SVM we take our two logistic regression $y=1$ and $y=0$ terms described previously and replace with $cost_1(\theta^Tx)$ and $cost_0(\theta^Tx)$.</p>
        <p>$$J(\theta) = -\frac{1}{m} \sum_{i=1}^{m}[ y^{(i)} cost_1(\theta^Tx^{(i)}) + (1- y^{(i)}) cost_0(\theta^Tx^{(i)})] + \frac{\lambda}{2m} \sum_{j=1}^{m} \theta_j^2$$</p>
        <p>Which can be rewritten as:</p>
        <p>$$J(\theta) = C \sum_{i=1}^{m}[ y^{(i)} cost_1(\theta^Tx^{(i)}) + (1- y^{(i)}) cost_0(\theta^Tx^{(i)})] + \frac{1}{2} \sum_{j=1}^{m} \theta_j^2$$</p>
        <ul>
            <li>Large C gives a hypothesis of low bias high variance $-&gt;$ overfitting</li>
            <li>Small C gives a hypothesis of high bias low variance $-&gt;$ underfitting</li>
        </ul>
        <h2>Large margin intuition</h2>
        <ul>
            <li>So, given that we're aiming to minimize CA + B.</li>
            <li>Consider the following scenario: we set C to be really large.</li>
            <li>If C is large, we will choose an A value such that A equals zero.</li>
            <li>If y = 1, then we must find a value of $\theta$ so that $\theta^Tx$ is larger than or equal to 1 in order to make our "A" term 0.</li>
            <li>If y = 0, then we must find a value of $\theta$ so that $\theta^Tx$ is equal to or less than -1 in order to make our "A" term 0.</li>
            <li>So we're minimizing B, under the constraints shown below:</li>
        </ul>
        <p>$$min\ \frac{1}{2} \sum_{j=1}^{m} \theta_j^2$$</p>
        <p>$$\theta^Tx^{(i)} \geq 1 \quad if\ y^{(i)}=1$$
            $$\theta^Tx^{(i)} \leq 1 \quad if\ y^{(i)}=0$$</p>
        <p><img alt="large_dist" src="https://github.com/djeada/Stanford-Machine-Learning/blob/main/slides/resources/large_dist.png" /></p>
        <ul>
            <li>The green and magenta lines represent functional decision limits that might be selected using logistic regression. However, they are unlikely to generalize effectively.</li>
            <li>The black line, on the other hand, is the one picked by the SVM as a result of the optimization graph's safety net. Stronger separator.</li>
            <li>That black line has a greater minimum distance (margin) than any of the training samples.</li>
        </ul>
        <h2>SVM decision boundary</h2>
        <p>Assume we only have two features and $\theta_0=0$. Then we can rewrite th expression for minimizing B as follows:</p>
        <p>$$\frac{1}{2}(\theta_1^2 + \theta_2^2) =\frac{1}{2}(\sqrt{\theta_1^2 + \theta_2^2})^2 = \frac{1}{2}||\theta||^2$$</p>
        <ul>
            <li>Given this, what are $\theta^Tx$ parameters doing?</li>
            <li>Assume we have just one positive training example (red cross below).</li>
            <li>Assume we have our parameter vector and plot it on the same axis.</li>
            <li>The following question asks what the inner product of these two vectors is.</li>
        </ul>
        <p><img alt="svm_vectors" src="https://github.com/djeada/Stanford-Machine-Learning/blob/main/slides/resources/svm_vectors.png" /></p>
        <p>$p$, is in fact $p^i$, because it's the length of $p$ for example $i$.</p>
        <p>$$\theta^Tx^{(i)} = p^i \cdot ||\theta||$$</p>
        <p>$$min\ \frac{1}{2} \sum_{j=1}^{m} \theta_j^2 = \frac{1}{2} ||\theta||^2$$</p>
        <p>$$p^{(i)} \cdot ||\theta|| \geq 1 \quad if\ y^{(i)}=1$$
            $$p^{(i)} \cdot ||\theta|| \leq 1 \quad if\ y^{(i)}=0$$</p>
        <h2>Adapting SVM to non-linear classifiers</h2>
        <ul>
            <li>We have a training set.</li>
            <li>We want to find a non-linear boundary.</li>
        </ul>
        <p><img alt="non_linear_boundary" src="https://github.com/djeada/Stanford-Machine-Learning/blob/main/slides/resources/non_linear_boundary.png" /></p>
        <ul>
            <li>Define three features in this example (ignore $x_0$).</li>
            <li>Have a graph of $x_1$ vs. $x_2$ (don't plot the values, just define the space).</li>
            <li>Pick three points.</li>
        </ul>
        <p><img alt="landmarks" src="https://github.com/djeada/Stanford-Machine-Learning/blob/main/slides/resources/landmarks.png" /></p>
        <ul>
            <li>These points $l^1$, $l^2$, and $l^3$, were chosen manually and are called landmarks.</li>
            <li>Kernel is the name given to the similarity function between $(x, l^i)$.</li>
        </ul>
        <p>$$f_1 = k(X, l^1) = exp(- \frac{||x-l^{(1)}||^2}{2\sigma^2})$$</p>
        <ul>
            <li>Large $\sigma^2$ - $f$ features vary more smoothly - higher bias, lower variance.</li>
            <li>Small $\sigma^2$ - $f$ features vary abruptly - low bias, high variance.</li>
            <li>With training examples x we predict "1" when: $\theta_0+\theta_1f_1+\theta_2f_2+\theta_3f_3 \geq 0$</li>
            <li>Let's say that: $\theta_0 = -0.5,\ \theta_1=1,\ \theta_2=1,\ \theta_3=0$</li>
            <li>Given our placement of three examples, what happens if we evaluate an example at the magenta dot below?</li>
        </ul>
        <p><img alt="landmarks_magneta" src="https://github.com/djeada/Stanford-Machine-Learning/blob/main/slides/resources/landmarks_magneta.png" /></p>
        <ul>
            <li>We can see from our formula that f1 will be close to 1, whereas f2 and f3 will be close to 0.</li>
            <li>We have: $-0.5+1\cdot1+0\cdot1+0\cdot0 \geq 0$.</li>
            <li>The inequality holds. We predict 1.</li>
            <li>If we had another point far away from all three. The inequality wouldn't hold. As a result, we would predict 0.</li>
        </ul>
        <h2>Choosing the landmarks</h2>
        <ul>
            <li>Take the training data. Vectors X and Y, both with m elements.</li>
            <li>As a result, you'll wind up having m landmarks. Each training example has one landmark per location.</li>
            <li>So we just cycle over each landmark, determining how close $x^i$ is to that landmark. Here we are using the kernel function.</li>
            <li>Take these m features $(f_1, f_2 ... f_m)$ group them into an $[m +1 \times 1]$ dimensional vector called $f$.</li>
        </ul>
        <h2>Kernels</h2>
        <ul>
            <li>Linear kernel: no kernel, no $f$ vector. Predict $y=1$ if $(\theta^Tx) \geq 0$.</li>
            <li>Not all similarity functions you develop are valid kernels. Must satisfy Merecer's Theorem.</li>
            <li>Polynomial kernel.</li>
            <li>String kernel.</li>
            <li>Chi-squared kernel.</li>
            <li>Histogram intersection kernel.</li>
        </ul>
        <h2>Logistic regression vs. SVM</h2>
        <ul>
            <li>Use logistic regression or SVM with a linear kernel if n (features) is much greater than m (training set).</li>
            <li>If n is small and m is intermediate, the Gaussian kernel is suitable.</li>
            <li>With a Gaussian kernel, SVM will be sluggish if n is small and m is large. Use logistic regression or SVM with a linear kernel.</li>
            <li>A lot of SVM's power is using diferent kernels to learn complex non-linear functions.</li>
            <li>Because SVM is a convex optimization problem, it gives a global minimum.</li>
        </ul>

    </section>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" alt="Adam Djellouli">

            </div>
            <div class="footer-column">

                <p>
                    Thank you for visiting my personal website. All of the </br>
                    content on this site is free to use, but please remember </br>
                    to be a good human being and refrain from any abuse</br>
                    of the site. If you would like to contact me, please use </br>
                    my LinkedIn profile or my GitHub if you have any technical </br>
                    issues or ideas to share. I wish you the best and hope you </br>
                    have a fantastic life. </br>
                </p>

            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" class="fa fa-youtube" target="_blank">

                        </a>YouTube
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" class="fa fa-linkedin" target="_blank">

                        </a>LinkedIn
                    </li>
                    <li>
                        <a href="https://www.instagram.com/addjellouli/" class="fa fa-instagram" target="_blank">
                        </a>Instagram

                    </li>
                    <li>
                        <a href="https://github.com/djeada" class="fa fa-github">
                        </a>Github

                    </li>

                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                &copy; Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../app.js"></script>
    </footer>
</body>

</html>

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>