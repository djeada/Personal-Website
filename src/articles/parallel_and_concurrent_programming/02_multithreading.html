<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Multithreading</title>
    <meta content="Multithreading refers to the capability of a CPU, or a single core within a multi-core processor, to execute multiple threads concurrently." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>Last modified: February 05, 2025</i></p>
            <p style="text-align: right;"><i>This article is written in: 🇺🇸</i></p>
            <h2 id="multithreading">Multithreading</h2>
            <p><strong>Multithreading</strong> refers to the capability of a CPU, or a single core within a multi-core processor, to execute multiple threads concurrently. A thread is the smallest unit of processing that can be scheduled by an operating system. In a multithreaded environment, a program, or process, can perform multiple tasks at the same time, as each thread runs in the same shared memory space. This can be useful for tasks that are IO-bound, as threads can be used to keep the CPU busy while waiting for IO operations to complete. However, because threads share the same memory, they must be carefully synchronized to avoid issues like race conditions, where two threads attempt to modify the same data concurrently, leading to unpredictable outcomes.</p>
            <h3 id="thread-pool-vs-on-demand-thread">Thread Pool vs On-Demand Thread</h3>
            <p>
            <div>
                <pre><code class="language-shell">+----------------+        +----------------+        +------------------+
| Incoming Tasks |        |  Pool Manager  |        |   Thread Pool    |
|                |        |                |        |                  |
| +-----------+  |        |                |        |  +-----------+   |
| | Task 1    |-------------&gt; Assigns Task ---------&gt; | Thread 1   |   |
| +-----------+  |        |                |        |  +-----------+   |
| +-----------+  |        |                |        |  +-----------+   |
| | Task 2    |-------------&gt; Assigns Task ---------&gt; | Thread 2   |   |
| +-----------+  |        |                |        |  +-----------+   |
| +-----------+  |        |                |        |  +-----------+   |
| | Task 3    |-------------&gt; Assigns Task ---------&gt; | Thread 3   |   |
| +-----------+  |        |                |        |  +-----------+   |
| +-----------+  |        |                |        |  +-----------+   |
| | Task 4    |-------------&gt; Assigns Task ---------&gt; | Thread 4   |   |
| +-----------+  |        |                |        |  +-----------+   |
| +-----------+  |        |                |        +------------------+
| | Task 5    |-------------&gt; Waiting      | 
| +-----------+  |        |                | 
+----------------+        +----------------+</code></pre>
            </div>
            </p>
            <ul>
                <li>Two ways to create threads in multithreading are using a thread pool or on-demand thread spawning.</li>
                <li>Thread pool pre-spawns threads to reduce the creation costs associated with starting new threads.</li>
                <li>On-demand thread spawning creates threads as needed, which can help in reducing resource wastage.</li>
                <li>However, on-demand thread spawning may slow down the program when threads are needed due to the overhead of creating threads at runtime.</li>
            </ul>
            <h3 id="worker-threads">Worker Threads</h3>
            <ul>
                <li>In multithreading, the main thread typically initiates all other threads, which are known as worker threads.</li>
                <li>Worker threads only perform tasks when they are allocated by the main thread or another controlling thread.</li>
                <li>To regulate and limit the number of worker threads, a thread pool can be employed.</li>
            </ul>
            <p>A web server process, for example, receives a request and assigns it to a thread from its pool for processing. That thread then follows the main thread's instructions, completes the task, and returns to the pool, allowing the main thread to remain free for other tasks.</p>
            <h3 id="advantages-of-threads-over-processes">Advantages of Threads over Processes</h3>
            <ul>
                <li>Multithreading has several advantages over using multiple processes.</li>
                <li>One key advantage is better responsiveness, allowing a program to remain responsive even when part of it is performing a lengthy operation.</li>
                <li>Another benefit is faster context transitions between threads compared to processes, as threads share the same memory space.</li>
                <li>Threads also improve resource sharing since code, data, and files can be shared across all threads within a process.</li>
            </ul>
            <h3 id="challenges-with-multithreading">Challenges with Multithreading</h3>
            <ul>
                <li>In multithreaded programs, threads share a <strong>common state</strong>, which makes inter-thread communication easier but introduces risks when accessing shared resources.</li>
                <li>A primary concern is maintaining <strong>data consistency</strong>. Without proper synchronization, multiple threads can attempt to read or modify shared data at the same time, causing <strong>race conditions</strong> and unpredictable outcomes.</li>
                <li><strong>Efficient resource management</strong> is important. Thread creation, context switching, and lock handling introduce <strong>overhead</strong>. If not managed properly, these factors can negate the performance benefits of multithreading.</li>
                <li>Managing <strong>shared memory</strong> is challenging. When multiple threads access the same memory location, inconsistencies can occur unless synchronization mechanisms like locks, mutexes, or semaphores are in place.</li>
                <li>The <strong>nondeterministic</strong> nature of thread scheduling by the operating system complicates debugging and testing. Errors that depend on timing and ordering may only appear sporadically, making them difficult to reproduce and fix.</li>
                <li>Balancing <strong>performance</strong> with thread safety is vital. Techniques such as locking prevent data corruption but may reduce concurrency, increasing wait times and hindering potential speedups.</li>
            </ul>
            <h4 id="data-race">Data Race</h4>
            <ul>
                <li>A data race (or <strong>race condition</strong>) happens when the correctness of a multithreaded program depends on the timing or sequence of thread execution, potentially causing errors and unpredictable results.</li>
                <li>Because threads are <strong>preemptively switched</strong> by the OS, programmers have limited control over when a context switch happens, increasing the likelihood of conflicts.</li>
                <li>While preemptive switching removes the burden of manually controlling <strong>task-switching</strong>, it also means a thread can be paused at any point, possibly causing inconsistent or incomplete operations on shared data.</li>
            </ul>
            <p>Consider an example: two functions, <code>funA()</code> and <code>funB()</code>, where <code>funB()</code> relies on the output of <code>funA()</code>. In a single-threaded program:</p>
            <p>
            <div>
                <pre><code class="language-python">funA()
funB()</code></pre>
            </div>
            </p>
            <p>The order is guaranteed. However, in a multithreaded scenario:</p>
            <p>
            <div>
                <pre><code class="language-python"># Thread 1
funA()

# Thread 2
funB()</code></pre>
            </div>
            </p>
            <p>The execution order becomes unpredictable. If <code>funB()</code> runs before <code>funA()</code> has completed, the result could be incorrect.</p>
            <ul>
                <li>A data race specifically occurs when two threads <strong>concurrently</strong> access the same memory location, with at least one thread modifying it. This can lead to <strong>memory corruption</strong> if no proper safeguards are in place.</li>
                <li><strong>Locks</strong> (or other synchronization primitives) are typically used to protect important sections so that only one thread can access specific memory at a time, ensuring data integrity.</li>
            </ul>
            <p><strong>Analogy</strong>: </p>
            <p><em>Imagine a busy kitchen with multiple chefs working on the same dish. They share the same utensils and ingredients. Without coordination, two chefs might grab the same tool or ingredient at the same time, causing confusion or mistakes. Likewise, a data race occurs when multiple threads share data without proper synchronization, leading to unpredictable outcomes.</em></p>
            <p><strong>Example</strong>:</p>
            <p>
            <div>
                <pre><code class="language-clike">#include &lt;iostream&gt;
#include &lt;thread&gt;
#include &lt;vector&gt;

// Shared counter variable
int counter = 0;

// Function to increment the counter
void incrementCounter(int numIncrements) {
    for (int i = 0; i &lt; numIncrements; ++i) {
        // Read, increment, and write back the counter
        // This is not an atomic operation and can cause race conditions
        counter++;
    }
}

int main() {
    const int numThreads = 10;                  // Number of threads
    const int incrementsPerThread = 100000;     // Increments per thread

    std::vector&lt;std::thread&gt; threads;

    // Start timer
    auto start = std::chrono::high_resolution_clock::now();

    // Create and start threads
    for (int i = 0; i &lt; numThreads; ++i) {
        threads.emplace_back(incrementCounter, incrementsPerThread);
    }

    // Wait for all threads to finish
    for (auto&amp; th : threads) {
        th.join();
    }

    // Stop timer
    auto end = std::chrono::high_resolution_clock::now();
    std::chrono::duration&lt;double&gt; elapsed = end - start;

    // Expected result
    int expected = numThreads * incrementsPerThread;

    // Output results
    std::cout &lt;&lt; "Final counter value: " &lt;&lt; counter &lt;&lt; std::endl;
    std::cout &lt;&lt; "Expected counter value: " &lt;&lt; expected &lt;&lt; std::endl;
    std::cout &lt;&lt; "Time taken: " &lt;&lt; elapsed.count() &lt;&lt; " seconds" &lt;&lt; std::endl;

    return 0;
}</code></pre>
            </div>
            </p>
            <p><strong>Possible Output</strong>:</p>
            <p>
            <div>
                <pre><code class="language-shell">Final counter value: 282345
Expected counter value: 1000000
Time taken: 0.023456 seconds</code></pre>
            </div>
            </p>
            <p><strong>What is happening</strong>:</p>
            <p>
            <div>
                <pre><code class="language-shell">+----------------------------+

| Shared Counter: 100        |

+----------------------------+
        ^            ^

        |            |

  +-----+-----+  +---+------+

  | Thread 1  |  | Thread 2 |

  +-----------+  +----------+

        |               |
        |               |
        |               |

[Thread 1]           [Thread 2]
Read Counter = 100   Read Counter = 100

        |               |
        |               |
        |               |

[Thread 1]           [Thread 2]
Increment: 100 + 1 = 101

        |               |
        |               |
        |               |

[Thread 1]           [Thread 2]
Write Counter = 101  Write Counter = 101

        |               |

+----------------------------+

| Shared Counter: 101        |

+----------------------------+</code></pre>
            </div>
            </p>
            <p>In this scenario, both threads read the same value (100) before either has a chance to write back the incremented value. This leads to lost updates and an incorrect final result.</p>
            <p><strong>What do we mean by a resource?</strong></p>
            <p>In the context of computing and multithreading, a resource refers to any hardware or software component that applications and processes need to operate effectively. This includes elements such as CPU time, memory, storage, network bandwidth, files, and shared data structures. Resources are limited and must be managed efficiently to make sure that multiple threads or processes can access them without conflicts. Proper resource management is necessary for maintaining optimal system performance, preventing bottlenecks, and avoiding issues like deadlocks or excessive contention when multiple threads compete for the same assets.</p>
            <h4 id="mutex">Mutex</h4>
            <ul>
                <li>A <strong>mutex</strong> (short for <em>mutual exclusion</em>) ensures that only one thread can access a important section of code (and thus shared data) at any given time.</li>
                <li>If one thread holds the mutex, other threads attempting to acquire it will block (or go to sleep) until the mutex is released.</li>
            </ul>
            <p><strong>Analogy</strong>: </p>
            <p><em>Imagine a single-stall public restroom. If multiple people try to enter simultaneously, chaos ensues. Instead, a lock on the door ensures only one person can use it at a time. Similarly, a mutex ensures exclusive access to a shared resource.</em></p>
            <p><strong>Example</strong>:</p>
            <p>
            <div>
                <pre><code class="language-clike">#include &lt;iostream&gt;
#include &lt;thread&gt;
#include &lt;vector&gt;
#include &lt;mutex&gt;

// Shared counter variable
int counter = 0;

// Mutex to protect the counter
std::mutex counterMutex;

// Function to increment the counter with synchronization
void incrementCounterSafe(int numIncrements) {
    for (int i = 0; i &lt; numIncrements; ++i) {
        std::lock_guard&lt;std::mutex&gt; lock(counterMutex);
        counter++;
    }
}

int main() {
    const int numThreads = 10;
    const int incrementsPerThread = 100000;

    std::vector&lt;std::thread&gt; threads;

    // Start timer
    auto start = std::chrono::high_resolution_clock::now();

    // Create and start threads
    for (int i = 0; i &lt; numThreads; ++i) {
        threads.emplace_back(incrementCounterSafe, incrementsPerThread);
    }

    // Wait for all threads to finish
    for (auto&amp; th : threads) {
        th.join();
    }

    // Stop timer
    auto end = std::chrono::high_resolution_clock::now();
    std::chrono::duration&lt;double&gt; elapsed = end - start;

    // Expected result
    int expected = numThreads * incrementsPerThread;

    // Output results
    std::cout &lt;&lt; "Final counter value: " &lt;&lt; counter &lt;&lt; std::endl;
    std::cout &lt;&lt; "Expected counter value: " &lt;&lt; expected &lt;&lt; std::endl;
    std::cout &lt;&lt; "Time taken: " &lt;&lt; elapsed.count() &lt;&lt; " seconds" &lt;&lt; std::endl;

    return 0;
}</code></pre>
            </div>
            </p>
            <p><strong>Possible Output</strong>:</p>
            <p>
            <div>
                <pre><code class="language-shell">Final counter value: 1000000
Expected counter value: 1000000
Time taken: 0.234567 seconds</code></pre>
            </div>
            </p>
            <p><strong>What is happening</strong>:</p>
            <p>
            <div>
                <pre><code class="language-shell">┌────────────────────────────┐
│      Shared Counter: 100   │
└────────────────────────────┘
           ▲                  ▲
           │                  │
     ┌─────┴─────┐      ┌─────┴─────┐
     │  Thread 1 │      │  Thread 2 │
     └─────┬─────┘      └─────┬─────┘
           │                  │         WAITING 
           │                  -----------------
           ▼                                  |
┌─────────────────────────────────┐           |
│ [Thread 1 acquires mutex]       │           |
│ [Thread 1] Read Counter = 100   │           |
│ [Thread 1] Increment to 101     │           |
│ [Thread 1] Write Counter = 101  │           |
│ [Thread 1 releases mutex]       │           |
└─────────────────────────────────┘           |
                                              ▼ 
                        ┌────────────────────────────────┐
                        │ [Thread 2 acquires mutex]      │
                        │ [Thread 2] Read Counter = 101  │
                        │ [Thread 2] Increment to 102    │
                        │ [Thread 2] Write Counter = 102 │
                        │ [Thread 2 releases mutex]      │
                        └────────────────────────────────┘</code></pre>
            </div>
            </p>
            <p>The mutex ensures that only one thread can modify the shared counter at a time, resulting in a correct final value but with additional locking overhead.</p>
            <h4 id="atomic">Atomic</h4>
            <p>An <strong>atomic</strong> operation ensures that a read-modify-write sequence completes as one indivisible action. This means no other thread can interrupt or observe a partial update, preventing data races for simple shared variables without needing a heavier synchronization mechanism like a mutex. Atomic operations can apply to various fundamental data types (e.g., <code>int</code>, <code>bool</code>, <code>pointer</code> types) and, in many implementations, to user-defined types that are trivially copyable and do not exceed a certain size (often the size of a machine word). </p>
            <p>In C++, these atomic types are provided by <code>std::atomic&lt;T&gt;</code>, and some specialized versions like <code>std::atomic_flag</code> offer specific functionalities. The standard guarantees that reads and writes to these types occur as single, uninterruptible steps. Operations like <code>load</code>, <code>store</code>, <code>fetch_add</code>, <code>fetch_sub</code>, <code>compare_exchange</code>, and similar can all be made atomic.</p>
            <p><strong>What do we gain by using atomics?</strong></p>
            <ul>
                <li>Atomics utilize <strong>hardware</strong>-level instructions that are lighter than mutexes, enhancing efficiency for simple operations.</li>
                <li>Operations using atomics avoid <strong>contention</strong> since threads don't wait for locks to be released, allowing independent progression.</li>
                <li>Atomics provide <strong>simplicity</strong> for managing basic shared data like counters and flags, reducing the risk of race conditions.</li>
            </ul>
            <p><strong>What do we lose by using atomics?</strong></p>
            <ul>
                <li>Using atomics requires careful management of <strong>memory</strong> ordering, as incorrect orderings can lead to subtle bugs.</li>
                <li>Atomics are <strong>limited</strong> to simple operations and are not suitable for complex data structures or large objects.</li>
                <li>Ensuring the correctness of higher-level algorithms with atomics can lead to <strong>concurrency</strong> pitfalls such as livelocks or ABA problems.</li>
                <li>In scenarios with heavy contention, atomic operations may not be <strong>faster</strong> than other synchronization methods, depending on hardware and use case.</li>
            </ul>
            <p><strong>Analogy</strong>:</p>
            <p><em>Imagine a vending machine that instantly dispenses an item the moment you press a button and inserts your bill into a slot—no one can see a partial transaction or grab the bill out mid-transaction. The entire action (paying and getting the item) is handled as a single, uninterruptible event.</em></p>
            <p><strong>Example</strong>:</p>
            <p>
            <div>
                <pre><code class="language-clike">#include &lt;iostream&gt;
#include &lt;thread&gt;
#include &lt;vector&gt;
#include &lt;atomic&gt;

std::atomic&lt;int&gt; counter(0);

void incrementCounterAtomic(int numIncrements) {
    for (int i = 0; i &lt; numIncrements; ++i) {
        counter.fetch_add(1, std::memory_order_relaxed);
    }
}

int main() {
    const int numThreads = 10;
    const int incrementsPerThread = 100000;

    std::vector&lt;std::thread&gt; threads;
    
    // Create and start threads
    for (int i = 0; i &lt; numThreads; ++i) {
        threads.emplace_back(incrementCounterAtomic, incrementsPerThread);
    }
    
    // Wait for all threads to finish
    for (auto&amp; th : threads) {
        th.join();
    }
    
    std::cout &lt;&lt; "Final counter value: " &lt;&lt; counter &lt;&lt; std::endl;
    std::cout &lt;&lt; "Expected counter value: " &lt;&lt; (numThreads * incrementsPerThread) &lt;&lt; std::endl;
    return 0;
}</code></pre>
            </div>
            </p>
            <p><strong>What is happening</strong>:</p>
            <p>
            <div>
                <pre><code class="language-shell">Atomic Counter
    Thread 1       |         Thread 2
-------------------+-------------------
  Read &amp; Inc        | 
      |            Read &amp; Inc
      |                |
  Write: 101 ----&gt; No Interruption &lt;---- Write: 102
      |                |
      v                v
  next iteration  next iteration

  (All increments happen as atomic steps,
   so partial updates are never seen)</code></pre>
            </div>
            </p>
            <p>To clear up the common confusion surrounding this term, let’s clarify how it differs from related concepts:</p>
            <ul>
                <li>An operation executed as a single, indivisible step, known as <strong>atomic</strong>, ensures it is free from race conditions.</li>
                <li>The <strong>lock-free</strong> property guarantees that at least one thread will always make progress, even under contention, preventing the system from blocking entirely.</li>
                <li>A <strong>wait-free</strong> guarantee ensures that every thread makes progress within a bounded number of steps, offering maximum fairness and predictability.</li>
            </ul>
            <h4 id="deadlock">Deadlock</h4>
            <p>A <strong>deadlock</strong> occurs when two or more threads are blocked, each waiting for a lock that another thread already holds. Because all threads are waiting on one another, no progress can be made, and the system is effectively stuck.</p>
            <p><strong>Analogy</strong>: </p>
            <p><em>Imagine two cars on a narrow one-lane bridge coming from opposite ends. Each driver refuses to back up, and neither can move forward. Both are blocked indefinitely, waiting for the other to yield.</em></p>
            <p><strong>Example</strong>:</p>
            <p>
            <div>
                <pre><code class="language-clike">#include &lt;iostream&gt;
#include &lt;thread&gt;
#include &lt;mutex&gt;

std::mutex mutexA;
std::mutex mutexB;

void threadFunc1() {
    std::lock_guard&lt;std::mutex&gt; lock1(mutexA);
    std::this_thread::sleep_for(std::chrono::milliseconds(50)); // simulate work
    std::lock_guard&lt;std::mutex&gt; lock2(mutexB);
}

void threadFunc2() {
    std::lock_guard&lt;std::mutex&gt; lock1(mutexB);
    std::this_thread::sleep_for(std::chrono::milliseconds(50)); // simulate work
    std::lock_guard&lt;std::mutex&gt; lock2(mutexA);
}

int main() {
    std::thread t1(threadFunc1);
    std::thread t2(threadFunc2);

    t1.join();
    t2.join();

    return 0;
}</code></pre>
            </div>
            </p>
            <p><strong>What is happening</strong>:</p>
            <p>
            <div>
                <pre><code class="language-shell">Thread 1                    Thread 2
    |                           |
    v                           v
 Lock(mutexA)              Lock(mutexB)
      |                         |
      |-------Wait(mutexB) &lt;----|
      |                         |
      |                         |-------Wait(mutexA)
      v                         v
   BLOCKED                   BLOCKED

(Each thread holds one lock and waits
for the other lock to be released.
Neither lock is ever freed -&gt; deadlock)</code></pre>
            </div>
            </p>
            <h4 id="livelock">Livelock</h4>
            <p>A <strong>livelock</strong> occurs when two or more threads actively respond to each other in a way that prevents them from making progress. Unlike a deadlock, the threads are not blocked; they keep "moving," but they continually change their states in a manner that still prevents the system from completing its task.</p>
            <p><strong>Analogy</strong>: </p>
            <p><em>Picture two people in a narrow hallway who both step aside to let the other pass—only to keep stepping in the same direction repeatedly. They’re not standing still, but neither can get by the other.</em></p>
            <p><strong>Example</strong>:</p>
            <p>
            <div>
                <pre><code class="language-clike">#include &lt;iostream&gt;
#include &lt;thread&gt;
#include &lt;mutex&gt;
#include &lt;atomic&gt;

std::mutex mutex1;
std::mutex mutex2;
std::atomic&lt;bool&gt; is_done(false);

void thread1() {
    while (!is_done.load()) {
        if (mutex1.try_lock()) {
            if (mutex2.try_lock()) {
                std::cout &lt;&lt; "Thread 1 completes work.\n";
                is_done.store(true);
                mutex2.unlock();
            }
            mutex1.unlock();
        }
        // Thread tries, fails or succeeds,
        // then repeats without blocking indefinitely.
    }
}

void thread2() {
    while (!is_done.load()) {
        if (mutex2.try_lock()) {
            if (mutex1.try_lock()) {
                std::cout &lt;&lt; "Thread 2 completes work.\n";
                is_done.store(true);
                mutex1.unlock();
            }
            mutex2.unlock();
        }
    }
}

int main() {
    std::thread t1(thread1);
    std::thread t2(thread2);

    t1.join();
    t2.join();

    return 0;
}</code></pre>
            </div>
            </p>
            <p><strong>What is happening</strong>:</p>
            <p>
            <div>
                <pre><code class="language-shell">Thread 1                Thread 2
  try_lock(mutex1)       try_lock(mutex2)
       |                      |
   success?               success?
       |                      |
   try_lock(mutex2)       try_lock(mutex1)
       |                      |
   success?               success?
       |                      |
 release/retry         release/retry
       |                      |
       v                      v
  loop again             loop again

(Threads keep attempting to acquire both locks,
but they often release them and try again at the
same time, never settling and never fully blocking,
thus making no actual forward progress -&gt; livelock)</code></pre>
            </div>
            </p>
            <h4 id="semaphore">Semaphore</h4>
            <p>A <strong>semaphore</strong> is a synchronization mechanism that uses a counter to control how many threads can access a shared resource at once. Each thread performs an atomic <strong>wait</strong> (or <em>acquire</em>) operation before entering the critical section, which decrements the semaphore’s counter. When a thread finishes its work, it performs a <strong>signal</strong> (or <em>release</em>) operation, incrementing the counter and allowing other waiting threads to proceed.</p>
            <p><strong>Analogy</strong>: </p>
            <p><em>Think of a parking garage with a limited number of spaces. Each car (thread) must check if a space is available before entering (acquire). If no space is free, the car must wait. When a car leaves (release), a space opens up for the next waiting car.</em></p>
            <p><strong>Example</strong> (using C++20 counting semaphore):</p>
            <p>
            <div>
                <pre><code class="language-clike">#include &lt;iostream&gt;
#include &lt;thread&gt;
#include &lt;vector&gt;
#include &lt;semaphore&gt;
#include &lt;chrono&gt;

// A counting semaphore initialized to allow 2 concurrent threads
std::counting_semaphore&lt;2&gt; sem(2);

void worker(int id) {
    // Acquire a slot
    sem.acquire();
    std::cout &lt;&lt; "Thread " &lt;&lt; id &lt;&lt; " enters critical section.\n";
    
    // Simulate some work
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
    
    std::cout &lt;&lt; "Thread " &lt;&lt; id &lt;&lt; " leaves critical section.\n";
    // Release the slot
    sem.release();
}

int main() {
    std::vector&lt;std::thread&gt; threads;
    
    // Launch multiple threads
    for (int i = 0; i &lt; 5; ++i) {
        threads.emplace_back(worker, i);
    }
    
    // Wait for all to finish
    for (auto &amp;t : threads) {
        t.join();
    }
    
    return 0;
}</code></pre>
            </div>
            </p>
            <p><strong>What is happening</strong>:</p>
            <p>
            <div>
                <pre><code class="language-shell">[Semaphore with count = 2]
 -----------------+-----------------+-----------------
  Thread 0        |   Thread 1     |    Thread 2 ...
  tries sem.acquire()              | 
        |                          |
[Slot1 free, Slot2 free]          |
  acquires Slot1 -&gt; count=1       | 
        |                          |
        v                          |
   "In critical section"           |
        |                          |
  Thread 1 tries sem.acquire()     |
  acquires Slot2 -&gt; count=0        |
        |                          |
        v                          |
   "In critical section"           |
                   ... Meanwhile ...
               Thread 2 tries sem.acquire()
                     |       
                     v
               Must wait because count=0
               
    Once Thread 0 or 1 calls sem.release():
    - count increments by 1
    - Thread 2 (or next in line) acquires and enters</code></pre>
            </div>
            </p>
            <h4 id="common-misconceptions">Common Misconceptions</h4>
            <p><strong>Binary Semaphore vs. Mutex</strong> </p>
            <p>There is a common misconception that a binary semaphore and a mutex are equivalent. While both can restrict access to a resource, their primary use cases differ:</p>
            <ul>
                <li>A <strong>mutex</strong> is typically used to gain exclusive ownership over a resource. Only the thread that acquires the mutex can unlock it.</li>
                <li>A <strong>binary semaphore</strong>, although it can only hold one of two possible states (0 or 1), is commonly employed as a <strong>signaling mechanism</strong>. A “producer” thread signals that an event or condition has occurred (e.g., data is ready), and one or more “consumer” threads can then proceed to act on that information.</li>
            </ul>
            <p><strong>Multithreading Automatically Improves Performance</strong></p>
            <p>Many developers believe that incorporating multiple threads always leads to faster execution. However, multithreading can also slow down an application if not designed and tuned properly. The overhead of context switching, synchronization, and resource contention can negate performance gains, especially if the tasks are not well-suited for parallelism.</p>
            <p><strong>More Threads Equals Better Performance</strong></p>
            <p>It is often assumed that creating more threads will consistently boost performance. In reality, once the number of threads exceeds the available CPU cores or the nature of the task’s concurrency limits, performance may degrade. Excessive thread creation can lead to increased scheduling overhead, cache thrashing, and resource contention—ultimately harming efficiency.</p>
            <p><strong>Multithreaded Code Is Always Harder to Write and Maintain</strong> </p>
            <p>While concurrency introduces challenges—such as synchronization, potential race conditions, and timing-related bugs—multithreaded code is not necessarily more difficult to manage than single-threaded code. Modern languages and frameworks provide abstractions (e.g., thread pools, futures, async/await mechanisms) that simplify parallelism. With proper design, testing strategies, and usage of these tools, writing reliable and maintainable multithreaded applications becomes more approachable.</p>
            <h4 id="problems-for-which-multithreading-is-the-answer">Problems for which multithreading is the answer</h4>
            <ul>
                <li>Intensive computations, such as large-scale data analysis, scientific simulations, or complex mathematical calculations that require significant processing power.</li>
                <li>External clients sending requests to a process in a random and unpredictable fashion, like a PostgreSQL database handling multiple simultaneous queries from various users.</li>
                <li>Tasks that can be intuitively split into independent processing steps, allowing different threads to handle separate parts of a workflow concurrently.</li>
                <li>Continuous access to a large read-only data set, where multiple threads can efficiently read and process the data without needing to modify it.</li>
                <li>Tasks whose performance would be unacceptable as a single thread, necessitating parallel execution to meet performance requirements.</li>
                <li>Managing concurrent access to multiple resources, such as an operating system coordinating access to hardware components, memory, and peripherals.</li>
                <li>Processing a stream of large data files, enabling different threads to handle different segments of the data simultaneously for faster processing.</li>
                <li>Problems where each step has a clear input and output, facilitating parallel processing of sequential steps in a pipeline.</li>
                <li>Processes where the workload cannot be anticipated, allowing the system to dynamically allocate threads to handle varying loads effectively.</li>
                <li>Real-time data processing tasks, such as financial trading systems that require immediate handling of incoming market data to execute trades without delay.</li>
                <li>Asynchronous I/O operations, where applications perform multiple file reads and writes simultaneously without blocking the main execution thread.</li>
                <li>Maintaining user interface responsiveness in applications by offloading long-running tasks to background threads, ensuring the UI remains interactive.</li>
                <li>Parallel data processing pipelines, like ETL (Extract, Transform, Load) processes in data warehousing, where different stages run concurrently to enhance throughput.</li>
                <li>Simulation and modeling applications, such as climate models or physics simulations, that divide the environment into regions processed in parallel to speed up computations.</li>
                <li>Network servers and web services that handle multiple client connections simultaneously, with each thread managing a separate client session to ensure efficient request handling.</li>
                <li>Machine learning model training, where large neural networks are trained by parallelizing computations across multiple threads or cores to accelerate the learning process.</li>
                <li>Multimedia processing tasks, including audio and video encoding or decoding, where different streams or segments are processed in parallel to reduce latency and improve performance.</li>
                <li>Automated testing and continuous integration systems that run multiple test suites or build processes in parallel to speed up the development and deployment pipeline.</li>
            </ul>
            <h4 id="problems-for-which-multithreading-is-not-the-answer">Problems for which multithreading is not the answer</h4>
            <ul>
                <li>Tasks with sequential dependencies require operations to be performed in a strict order without the possibility of parallel execution, making multithreading ineffective.</li>
                <li>Minimal processing tasks involve simple or quick operations where the overhead of creating and managing threads outweighs any potential performance gains.</li>
                <li>High contention for shared resources occurs when multiple threads frequently compete for the same resources, leading to excessive locking and reduced performance.</li>
                <li>Applications relying on single-threaded libraries or APIs are not designed to be thread-safe, making multithreading difficult or error-prone.</li>
                <li>Limited hardware resources mean environments have insufficient CPU cores or memory, where adding more threads could degrade overall system performance.</li>
                <li>Real-time systems with strict timing requirements need predictable and deterministic execution times, where the unpredictability of thread scheduling can cause issues.</li>
                <li>Applications requiring high synchronization involve tasks that need extensive coordination between threads, resulting in bottlenecks and diminishing returns from parallelism.</li>
                <li>Debugging and maintenance complexity arises in projects where the added complexity of multithreading introduces significant challenges in debugging, testing, and maintaining the codebase.</li>
                <li>Deterministic execution needs are present in applications that require consistent and repeatable behavior for debugging, security, or compliance reasons, which can be disrupted by the non-deterministic nature of multithreading.</li>
                <li>Environments with poor multithreading support have programming languages or runtime environments that lack robust multithreading capabilities, making implementation difficult or inefficient.</li>
                <li>Tasks better suited for asynchronous programming benefit from asynchronous, non-blocking approaches that provide better performance and scalability without the complexities of multithreading.</li>
                <li>Problems better addressed by multiprocessing or distributed computing involve situations where using multiple processes or distributing tasks across different machines is more effective than using multiple threads within a single process.</li>
                <li>Memory-constrained applications need to minimize memory usage, as each thread consumes additional memory for its stack and management overhead.</li>
                <li>Simple, single-user applications operate with a single user or do not require concurrent processing, where multithreading offers no tangible benefits.</li>
                <li>Tasks with high initialization costs involve operations where the cost of starting and stopping threads is prohibitively high compared to the task's execution time.</li>
                <li>Security-sensitive applications could introduce vulnerabilities through race conditions or improper handling of shared data when using multithreading.</li>
                <li>Legacy codebases are existing applications that were not designed with multithreading in mind, where retrofitting multithreading could be risky or impractical.</li>
                <li>Energy-constrained devices are battery-powered or low-energy devices where the additional power consumption from managing multiple threads is a concern.</li>
            </ul>
            <h3 id="examples">Examples</h3>
            <h4 id="examples-in-c-">Examples in C++</h4>
            <p>In C++, every application starts with a single default main thread, represented by the <code>main()</code> function. This main thread can create additional threads, which are useful for performing multiple tasks simultaneously. Since C++11, the Standard Library provides the <code>std::thread</code> class to create and manage threads. The creation of a new thread involves defining a function that will execute in parallel and passing it to the <code>std::thread</code> constructor, along with any arguments required by that function.</p>
            <h5>Creating Threads</h5>
            <p>A new thread in C++ can be created by instantiating the <code>std::thread</code> object. The constructor accepts a callable object (like a function, lambda, or function object) and optional arguments to be passed to the callable object.</p>
            <p>
            <div>
                <pre><code class="language-clike">#include &lt;iostream&gt;
#include &lt;thread&gt;

void printMessage(const std::string&amp; message) {
    std::cout &lt;&lt; message &lt;&lt; std::endl;
}

int main() {
    std::thread t1(printMessage, "Hello from thread!");
    t1.join(); // Wait for the thread to finish
    return 0;
}</code></pre>
            </div>
            </p>
            <p>In this example, <code>printMessage</code> is called in a separate thread, and the main thread waits for <code>t1</code> to complete using <code>join()</code>.</p>
            <h5>Thread Joining</h5>
            <p>The <code>join()</code> function is called on a <code>std::thread</code> object to wait for the associated thread to complete execution. This blocks the calling thread until the thread represented by <code>std::thread</code> finishes.</p>
            <p><strong>Advantages</strong>:</p>
            <ul>
                <li>The main program can wait for the thread to <strong>complete</strong>, ensuring synchronization between threads.</li>
                <li>It facilitates proper resource <strong>management</strong> by ensuring threads finish before the program terminates.</li>
            </ul>
            <p><strong>Disadvantages</strong>:</p>
            <ul>
                <li>A <strong>drawback</strong> is that the main program may block while waiting, potentially reducing responsiveness.</li>
                <li>It requires careful handling to prevent deadlocks or race <strong>conditions</strong> during synchronization.</li>
            </ul>
            <p>
            <div>
                <pre><code class="language-clike">t1.join(); // Main thread waits for t1 to finish</code></pre>
            </div>
            </p>
            <h5>Thread Detaching</h5>
            <p>Using <code>detach()</code>, a thread is separated from the <code>std::thread</code> object and continues to execute independently. This allows the main thread to proceed without waiting for the detached thread to finish. However, once detached, the thread becomes non-joinable, meaning it cannot be waited on or joined, and it will run independently until completion.</p>
            <p><strong>Advantages</strong>:</p>
            <ul>
                <li>The main program continues without waiting for the detached thread, facilitating fire-and-forget tasks.</li>
                <li>It is useful for <strong>fire-and-forget</strong> tasks.</li>
            </ul>
            <p><strong>Disadvantages</strong>:</p>
            <ul>
                <li>There is no <strong>control</strong> over when the thread finishes.</li>
                <li>There is a risk of resources not being properly managed, as the program might end before the thread completes.</li>
            </ul>
            <p>
            <div>
                <pre><code class="language-clike">std::thread t2(printMessage, "This is a detached thread");
t2.detach(); // Main thread does not wait for t2</code></pre>
            </div>
            </p>
            <h5>Thread Lifecycle and Resource Management</h5>
            <p>Each thread has a lifecycle, beginning with creation, execution, and finally termination. Upon termination, the resources held by the thread need to be cleaned up. If a thread object goes out of scope and is still joinable (not yet joined or detached), the program will terminate with <code>std::terminate</code> because it is considered an error to destroy a <code>std::thread</code> object without properly handling the thread.</p>
            <h5>Passing Arguments to Threads</h5>
            <p>Arguments can be passed to the thread function through the <code>std::thread</code> constructor. The arguments are copied or moved as necessary. Special care must be taken when passing pointers or references, as these must refer to objects that remain valid throughout the thread's execution.</p>
            <p>
            <div>
                <pre><code class="language-clike">#include &lt;iostream&gt;
#include &lt;thread&gt;

void printSum(int a, int b) {
    std::cout &lt;&lt; "Sum: " &lt;&lt; (a + b) &lt;&lt; std::endl;
}

int main() {
    int x = 5, y = 10;
    std::thread t(printSum, x, y); // Passing arguments by value
    t.join();
    return 0;
}</code></pre>
            </div>
            </p>
            <p>In this example, <code>x</code> and <code>y</code> are passed by value to the <code>printSum</code> function.</p>
            <h5>Using Lambdas with Threads</h5>
            <p>Lambda expressions provide a convenient way to define thread tasks inline. They can capture local variables by value or reference, allowing for flexible and concise thread management.</p>
            <p>
            <div>
                <pre><code class="language-clike">#include &lt;iostream&gt;
#include &lt;thread&gt;

int main() {
    int a = 5, b = 10;
    std::thread t([a, b]() {
        std::cout &lt;&lt; "Lambda Sum: " &lt;&lt; (a + b) &lt;&lt; std::endl;
    });
    t.join();
    return 0;
}</code></pre>
            </div>
            </p>
            <p>In this case, the lambda captures <code>a</code> and <code>b</code> by value and uses them inside the thread.</p>
            <h5>Mutex for Synchronization</h5>
            <p><code>std::mutex</code> is used to protect shared data from being accessed simultaneously by multiple threads. It ensures that only one thread can access the critical section at a time, preventing data races.</p>
            <p>
            <div>
                <pre><code class="language-clike">#include &lt;iostream&gt;
#include &lt;thread&gt;
#include &lt;mutex&gt;

std::mutex mtx;
int sharedCounter = 0;

void increment() {
    std::lock_guard&lt;std::mutex&gt; lock(mtx);
    ++sharedCounter;
}

int main() {
    std::thread t1(increment);
    std::thread t2(increment);
    t1.join();
    t2.join();
    std::cout &lt;&lt; "Shared Counter: " &lt;&lt; sharedCounter &lt;&lt; std::endl;
    return 0;
}</code></pre>
            </div>
            </p>
            <p>In this example, <code>std::lock_guard</code> automatically locks the mutex on creation and unlocks it on destruction, ensuring the increment operation is thread-safe.</p>
            <h5>Deadlocks and Avoidance</h5>
            <p>Deadlocks occur when two or more threads are waiting for each other to release resources, resulting in a standstill. To avoid deadlocks, it's crucial to lock multiple resources in a consistent order, use try-lock mechanisms, or employ higher-level concurrency primitives like <code>std::lock</code> or condition variables.</p>
            <p>
            <div>
                <pre><code class="language-clike">#include &lt;iostream&gt;
#include &lt;thread&gt;
#include &lt;mutex&gt;

std::mutex mutex1;
std::mutex mutex2;

void taskA() {
    std::lock(mutex1, mutex2);
    std::lock_guard&lt;std::mutex&gt; lock1(mutex1, std::adopt_lock);
    std::lock_guard&lt;std::mutex&gt; lock2(mutex2, std::adopt_lock);
    std::cout &lt;&lt; "Task A acquired both mutexes\n";
}

void taskB() {
    std::lock(mutex1, mutex2);
    std::lock_guard&lt;std::mutex&gt; lock1(mutex1, std::adopt_lock);
    std::lock_guard&lt;std::mutex&gt; lock2(mutex2, std::adopt_lock);
    std::cout &lt;&lt; "Task B acquired both mutexes\n";
}

int main() {
    std::thread t1(taskA);
    std::thread t2(taskB);
    t1.join();
    t2.join();
    return 0;
}</code></pre>
            </div>
            </p>
            <p>Here, <code>std::lock</code> locks both mutexes without risking a deadlock by ensuring that both mutexes are acquired in a consistent order.</p>
            <h5>Condition Variables</h5>
            <p><code>std::condition_variable</code> is used for thread synchronization by allowing threads to wait until they are notified to proceed. This is useful for scenarios where a thread must wait for some condition to become true.</p>
            <p>
            <div>
                <pre><code class="language-clike">#include &lt;iostream&gt;
#include &lt;thread&gt;
#include &lt;mutex&gt;
#include &lt;condition_variable&gt;

std::mutex mtx;
std::condition_variable cv;
bool ready = false;

void print_id(int id) {
    std::unique_lock&lt;std::mutex&gt; lock(mtx);
    cv.wait(lock, [] { return ready; });
    std::cout &lt;&lt; "Thread " &lt;&lt; id &lt;&lt; "\n";
}

void set_ready() {
    std::unique_lock&lt;std::mutex&gt; lock(mtx);
    ready = true;
    cv.notify_all();
}

int main() {
    std::thread t1(print_id, 1);
    std::thread t2(print_id, 2);
    std::thread t3(set_ready);

    t1.join();
    t2.join();
    t3.join();
    return 0;
}</code></pre>
            </div>
            </p>
            <p>In this example, <code>cv.wait</code> makes the threads wait until <code>ready</code> becomes true. <code>set_ready</code> changes the condition and notifies all waiting threads.</p>
            <h5>Semaphores</h5>
            <p>C++20 introduces <code>std::counting_semaphore</code> and <code>std::binary_semaphore</code>. Semaphores are synchronization primitives that control access to a common resource by multiple threads. They use a counter to allow a fixed number of threads to access a resource concurrently.</p>
            <p>
            <div>
                <pre><code class="language-clike">#include &lt;iostream&gt;
#include &lt;thread&gt;
#include &lt;semaphore&gt;

std::binary_semaphore semaphore(1);

void task(int id) {
    semaphore.acquire();
    std::cout &lt;&lt; "Task " &lt;&lt; id &lt;&lt; " is running\n";
    std::this_thread::sleep_for(std::chrono::milliseconds(100)); // simulate some work
    semaphore.release();
}

int main() {
    std::thread t1(task, 1);
    std::thread t2(task, 2);
    t1.join();
    t2.join();
    return 0;
}</code></pre>
            </div>
            </p>
            <p>Here, <code>semaphore.acquire()</code> ensures that only one thread can access the critical section at a time, and <code>semaphore.release()</code> signals that the resource is available again.</p>
            <h5>Thread Local Storage</h5>
            <p>C++ provides thread-local storage via the <code>thread_local</code> keyword, allowing data to be local to each thread. This is useful when each thread requires its own instance of a variable, such as when storing non-shared data.</p>
            <p>
            <div>
                <pre><code class="language-clike">#include &lt;iostream&gt;
#include &lt;thread&gt;

thread_local int localVar = 0;

void increment(int id) {
    ++localVar;
    std::cout &lt;&lt; "Thread " &lt;&lt; id &lt;&lt; ": localVar = " &lt;&lt; localVar &lt;&lt; std::endl;
}

int main() {
    std::thread t1(increment, 1);
    std::thread t2(increment, 2);
    t1.join();


    t2.join();
    return 0;
}</code></pre>
            </div>
            </p>
            <p>In this example, each thread has its own instance of <code>localVar</code>, independent of the other threads.</p>
            <h5>Atomic Operations</h5>
            <p>For cases where synchronization is needed, but mutexes are too heavy-weight, C++ provides atomic operations via the <code>std::atomic</code> template. This allows for lock-free programming and can be used to implement simple data structures or counters safely in a multithreaded environment.</p>
            <p>
            <div>
                <pre><code class="language-clike">#include &lt;iostream&gt;
#include &lt;thread&gt;
#include &lt;atomic&gt;

std::atomic&lt;int&gt; atomicCounter(0);

void increment() {
    for (int i = 0; i &lt; 100000; ++i) {
        ++atomicCounter;
    }
}

int main() {
    std::thread t1(increment);
    std::thread t2(increment);
    t1.join();
    t2.join();
    std::cout &lt;&lt; "Atomic Counter: " &lt;&lt; atomicCounter &lt;&lt; std::endl;
    return 0;
}</code></pre>
            </div>
            </p>
            <p>In this example, <code>std::atomic&lt;int&gt;</code> ensures that the increment operation is atomic, preventing data races.</p>
            <h5>Memory Orderings</h5>
            <p>When using atomic operations in C++, we not only specify <em>which</em> operations should be atomic, but also <em>how</em> they synchronize with other memory operations in the program. This “how” is controlled by <strong>memory orderings</strong>—a set of rules that govern visibility and ordering of reads and writes.</p>
            <p>C++ provides six memory order enumerations in <code>std::memory_order</code>:</p>
            <ol>
                <li><strong><code>std::memory_order_relaxed</code></strong> </li>
                <li><strong><code>std::memory_order_consume</code></strong> (mostly unimplemented in mainstream compilers) </li>
                <li><strong><code>std::memory_order_acquire</code></strong> </li>
                <li><strong><code>std::memory_order_release</code></strong> </li>
                <li><strong><code>std::memory_order_acq_rel</code></strong> </li>
                <li><strong><code>std::memory_order_seq_cst</code></strong> </li>
            </ol>
            <p>Each ordering offers different guarantees about how operations on one thread become visible to other threads and in what sequence they appear to happen. Understanding these guarantees can greatly affect both the correctness and performance of concurrent code.</p>
            <p>Below is a <strong>comparison table</strong> that summarizes the main C++ memory orderings, their guarantees, common use cases, and potential pitfalls. Use this as a quick reference to decide which ordering is best suited for a particular concurrency scenario.</p>
            <p>
            <table>
                <tr>
                    <td><strong>Memory Order</strong></td>
                    <td><strong>Brief Description</strong></td>
                    <td><strong>Key Guarantees</strong></td>
                    <td><strong>Common Use Cases</strong></td>
                    <td><strong>Pitfalls &amp; Advice</strong></td>
                </tr>
                <tr>
                    <td><strong><code>std::memory_order_relaxed</code></strong></td>
                    <td>Provides only atomicity, no ordering constraints</td>
                    <td>- The operation itself is atomic (indivisible) <br /> - No guarantees about visibility or ordering relative to other operations</td>
                    <td>- Simple counters or statistics <br /> - Non-critical flags where ordering doesn’t matter</td>
                    <td>- Easy to introduce data races if other parts of the program rely on the update’s order <br /> - Great performance but requires careful design</td>
                </tr>
                <tr>
                    <td><strong><code>std::memory_order_consume</code></strong></td>
                    <td>Intended to enforce data dependency ordering (rarely implemented properly)</td>
                    <td>- Theoretically only dependent reads are ordered <br /> - In practice, compilers often treat it like <code>acquire</code></td>
                    <td>- Very specialized, mostly replaced by <code>acquire</code> in real-world code</td>
                    <td>- Not well supported by most compilers <br /> - Avoid in portable or production code</td>
                </tr>
                <tr>
                    <td><strong><code>std::memory_order_acquire</code></strong></td>
                    <td>Prevents following reads/writes from moving before the acquire operation</td>
                    <td>- Ensures that subsequent operations see all side effects that happened before a matching <code>release</code> <br /> - Acts as a one-way barrier after the load</td>
                    <td>- Loading a “ready” flag to know that data is now valid <br /> - Synchronizing consumer who must see the producer’s writes</td>
                    <td>- Only ensures that instructions <em>after</em> the acquire load can’t be reordered before it <br /> - Must pair with <code>release</code> for full producer-consumer semantics</td>
                </tr>
                <tr>
                    <td><strong><code>std::memory_order_release</code></strong></td>
                    <td>Prevents preceding reads/writes from moving after the release operation</td>
                    <td>- Ensures all prior writes are visible to a thread that does an <code>acquire</code> on the same atomic <br /> - One-way barrier before the store</td>
                    <td>- Setting a “ready” flag after populating shared data <br /> - Synchronizing producer who writes data before signaling availability</td>
                    <td>- Doesn’t prevent instructions <em>after</em> the release from moving before it <br /> - Must pair with <code>acquire</code> to guarantee another thread will observe the updates</td>
                </tr>
                <tr>
                    <td><strong><code>std::memory_order_acq_rel</code></strong></td>
                    <td>Acquire + Release in one read-modify-write operation</td>
                    <td>- Combines the effects of <code>acquire</code> and <code>release</code> for RMW ops (e.g., <code>fetch_add</code>, <code>compare_exchange</code>) <br /> - Ensures no reorder before or after the operation</td>
                    <td>- Updating shared state in a single atomic step where you must see previous writes and publish new writes (e.g., lock-free structures)</td>
                    <td>- Can be stronger (thus slower) than needed if you only require a one-way barrier <br /> - Must be used carefully in highly concurrent scenarios</td>
                </tr>
                <tr>
                    <td><strong><code>std::memory_order_seq_cst</code></strong></td>
                    <td>Enforces total sequential consistency across all threads</td>
                    <td>- Provides a single, global order of all sequentially consistent operations <br /> - Easiest model to reason about, strongest ordering guarantee</td>
                    <td>- When correctness is paramount and performance concerns are secondary <br /> - Prototyping concurrency code before optimizing</td>
                    <td>- Highest potential performance cost <br /> - May introduce unnecessary fences on weaker architectures</td>
                </tr>
            </table>
            </p>
            <p><strong>What Do We Gain By Careful Use of Memory Orderings?</strong></p>
            <ul>
                <li><strong>Weaker orderings</strong> such as <code>relaxed</code>, <code>acquire</code>, and <code>release</code> can compile to more efficient instructions on some hardware, resulting in better performance compared to using a blanket <code>seq_cst</code>.</li>
                <li>Careful use of memory orderings provides <strong>control</strong> by ensuring only the minimal necessary barriers are in place, which helps prevent the use of expensive hardware fences when they are not needed.</li>
            </ul>
            <p><strong>What Do We Lose / Need to Beware Of?</strong></p>
            <ul>
                <li>Managing memory orderings introduces <strong>complexity</strong>, making it easy to introduce subtle bugs if the chosen ordering is too weak to guarantee the necessary data visibility.</li>
                <li>Code that utilizes specialized memory orderings can suffer from reduced <strong>portability</strong> and become harder to maintain, especially when new developers join the project.</li>
                <li>Using <strong>overly strong orderings</strong> like <code>seq_cst</code> everywhere can lead to over-synchronization, causing potential performance losses by missing out on possible optimizations.</li>
            </ul>
            <p><strong>Analogy</strong></p>
            <p>Imagine you’re coordinating a relay race: </p>
            <ul>
                <li>A <code>release</code> operation is like handing the baton off—ensuring everything you’ve done (run your segment) is finished before the next runner picks it up. </li>
                <li>An <code>acquire</code> operation is the next runner receiving the baton—ensuring they see everything you did (how far you ran, the state of the race) the moment they take it. </li>
                <li><code>relaxed</code> would be like running without caring about handing the baton off or receiving it properly—fast, but not synchronized. </li>
                <li><code>seq_cst</code> would be like having a strict official track judge making sure everyone runs in a strictly observed, universal order—less chance of cheating but more overhead.</li>
            </ul>
            <p><strong>Example</strong></p>
            <p>Below is a small snippet that demonstrates <code>release</code> and <code>acquire</code>:</p>
            <p>
            <div>
                <pre><code class="language-clike">#include &lt;atomic&gt;
#include &lt;vector&gt;
#include &lt;thread&gt;
#include &lt;iostream&gt;

struct SharedData {
    int value;
};

std::atomic&lt;bool&gt; ready(false);
SharedData data;

void producer() {
    // 1. Write to shared data
    data.value = 42;

    // 2. Publish that data is ready
    ready.store(true, std::memory_order_release);
}

void consumer() {
    // Wait until the data is ready
    while (!ready.load(std::memory_order_acquire)) {
        // spin or sleep
    }

    // Now it is guaranteed that we see data.value = 42
    std::cout &lt;&lt; "Shared data value = " &lt;&lt; data.value &lt;&lt; std::endl;
}

int main() {
    std::thread t1(producer);
    std::thread t2(consumer);
    t1.join();
    t2.join();
    return 0;
}</code></pre>
            </div>
            </p>
            <ul>
                <li>The <strong>producer</strong> writes <code>data.value = 42</code> and then calls <code>ready.store(true, std::memory_order_release)</code>, ensuring that any subsequent acquire operation on <code>ready</code> will see the updated <code>data.value</code>.</li>
                <li>The <strong>consumer</strong> spins until <code>ready.load(std::memory_order_acquire)</code> becomes true, and because it’s an acquire load, once it returns true, the consumer also sees <code>data.value = 42</code>.</li>
            </ul>
            <p><strong>What is happening</strong>:</p>
            <p>
            <div>
                <pre><code class="language-shell">Producer Thread                Consumer Thread
         |                              |
   data.value = 42                     ...
         |                              |
 ready.store(true, release)     ready.load(acquire) --&gt; sees true
         |                              |
         v                              |
    [ memory fence ]                    v
                                 sees data.value = 42</code></pre>
            </div>
            </p>
            <ul>
                <li>A <strong>release</strong> operation ensures that all writes before it, including <code>data.value = 42</code>, are visible to another thread that performs an acquire operation.</li>
                <li>An <strong>acquire</strong> operation ensures that once <code>ready</code> is seen as <code>true</code>, the consumer consistently sees the "before-release" state, such as <code>data.value = 42</code>.</li>
            </ul>
            <h5>Performance Considerations and Best Practices</h5>
            <ul>
                <li>The frequent creation and destruction of threads can be costly, leading to significant overhead. To minimize this, it is advisable to use thread pools or reuse threads, which can reduce the performance impact associated with thread lifecycle management.</li>
                <li>Synchronization mechanisms, such as mutexes, should be used sparingly because excessive synchronization can lead to contention and reduced performance. It is important to apply these mechanisms only when necessary to avoid unnecessary delays and overhead.</li>
                <li>To avoid data races, it is crucial to always protect shared data with appropriate synchronization primitives. This ensures that only one thread can access the data at a time, preventing concurrent modifications that could lead to inconsistent or incorrect data states.</li>
                <li>Utilizing modern features introduced in C++11 and later, such as <code>std::thread</code>, <code>std::mutex</code>, <code>std::lock_guard</code>, and <code>std::future</code>, can greatly simplify thread management and help avoid common pitfalls. These features provide robust and standardized ways to handle concurrency, making the code more maintainable and less error-prone.</li>
            </ul>
            <p>Here are some example code snippets demonstrating various aspects of multithreading in C++:</p>
            <p>
            <table>
                <tr>
                    <td>#</td>
                    <td>Example</td>
                    <td>Description</td>
                </tr>
                <tr>
                    <td>1</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/cpp/multithreading/01_single_worker_thread.cpp">single_worker_thread</a></td>
                    <td>Introduce the concept of threads by creating a single worker thread using <code>std::thread</code>.</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/cpp/multithreading/02_thread_subclass.cpp">thread_subclass</a></td>
                    <td>Demonstrate how to create a custom thread class by inheriting <code>std::thread</code>.</td>
                </tr>
                <tr>
                    <td>3</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/cpp/multithreading/03_multiple_worker_threads.cpp">multiple_worker_threads</a></td>
                    <td>Show how to create and manage multiple worker threads using <code>std::thread</code>.</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/cpp/multithreading/04_race_condition.cpp">race_condition</a></td>
                    <td>Explain race conditions and their impact on multi-threaded applications using C++ examples.</td>
                </tr>
                <tr>
                    <td>5</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/cpp/multithreading/05_mutex.cpp">mutex</a></td>
                    <td>Illustrate the use of <code>std::mutex</code> to protect shared resources and avoid race conditions in C++ applications.</td>
                </tr>
                <tr>
                    <td>6</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/cpp/multithreading/06_semaphore.cpp">semaphore</a></td>
                    <td>Demonstrate the use of <code>std::counting_semaphore</code> to limit the number of concurrent threads accessing a shared resource in C++ applications.</td>
                </tr>
                <tr>
                    <td>7</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/cpp/multithreading/07_producer_consumer.cpp">producer_consumer</a></td>
                    <td>Present a classic multi-threading problem (Producer-Consumer) and its solution using C++ synchronization mechanisms like <code>std::mutex</code> and <code>std::condition_variable</code>.</td>
                </tr>
                <tr>
                    <td>8</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/cpp/multithreading/08_fetch_parallel.cpp">fetch_parallel</a></td>
                    <td>Showcase a practical application of multi-threading for parallel fetching of data from multiple sources using C++ threads.</td>
                </tr>
                <tr>
                    <td>9</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/cpp/multithreading/09_merge_sort.cpp">merge_sort</a></td>
                    <td>Use multi-threading in C++ to parallelize a merge sort algorithm, demonstrating the potential for performance improvements.</td>
                </tr>
                <tr>
                    <td>10</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/cpp/multithreading/10_schedule_every_n_sec.cpp">schedule_every_n_sec</a></td>
                    <td>Show how to schedule tasks to run periodically at fixed intervals using C++ threads.</td>
                </tr>
                <tr>
                    <td>11</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/cpp/multithreading/11_barrier.cpp">barrier</a></td>
                    <td>Demonstrate the use of <code>std::barrier</code> to synchronize multiple threads at a specific point in the execution.</td>
                </tr>
                <tr>
                    <td>12</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/cpp/multithreading/12_thread_local_storage.cpp">thread_local_storage</a></td>
                    <td>Illustrate the concept of Thread Local Storage (TLS) and how it can be used to store thread-specific data.</td>
                </tr>
                <tr>
                    <td>13</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/cpp/multithreading/13_thread_pool.cpp">thread_pool</a></td>
                    <td>Show how to create and use a thread pool to efficiently manage a fixed number of worker threads for executing multiple tasks.</td>
                </tr>
                <tr>
                    <td>14</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/cpp/multithreading/14_reader_writer_lock.cpp">reader_writer_lock</a></td>
                    <td>Explain the concept of Reader-Writer Locks and their use for efficient access to shared resources with multiple readers and a single writer.</td>
                </tr>
            </table>
            </p>
            <h4 id="examples-in-python">Examples in Python</h4>
            <p>Python provides built-in support for concurrent execution through the <code>threading</code> module. While the Global Interpreter Lock (GIL) in CPython limits the execution of multiple native threads to one at a time per process, threading is still useful for I/O-bound tasks, where the program spends a lot of time waiting for external events.</p>
            <h5>Creating Threads</h5>
            <p>To create a new thread, you can instantiate the <code>Thread</code> class from the <code>threading</code> module. The target function to be executed by the thread is passed to the <code>target</code> parameter, along with any arguments required by the function.</p>
            <p>
            <div>
                <pre><code class="language-python">import threading

def print_message(message):
    print(message)

# Create a thread
t1 = threading.Thread(target=print_message, args=("Hello from thread!",))
t1.start()
t1.join()  # Wait for the thread to finish</code></pre>
            </div>
            </p>
            <p>In this example, the <code>print_message</code> function is executed in a new thread.</p>
            <h5>Thread Joining</h5>
            <p>Using the <code>join()</code> method ensures that the main thread waits for the completion of the thread. This is important for coordinating threads, especially when the main program depends on the thread's results.</p>
            <p>
            <div>
                <pre><code class="language-python">t1.join()  # Main thread waits for t1 to finish</code></pre>
            </div>
            </p>
            <h5>Thread Detaching</h5>
            <p>Python threads do not have a direct <code>detach()</code> method like C++. However, once started, a thread runs independently. The main program can continue executing without waiting for the threads, similar to detached threads in C++. However, you should ensure that all threads complete before the program exits to avoid abrupt termination.</p>
            <h5>Thread Lifecycle and Resource Management</h5>
            <p>Python threads are automatically managed by the interpreter. However, you should still ensure that threads are properly joined or allowed to finish their tasks to prevent any issues related to resource management or incomplete executions.</p>
            <h5>Passing Arguments to Threads</h5>
            <p>Arguments can be passed to the thread function via the <code>args</code> parameter when creating the <code>Thread</code> object. This allows for flexible and dynamic argument passing.</p>
            <p>
            <div>
                <pre><code class="language-python">import threading

def add(a, b):
    print(f"Sum: {a + b}")

# Create a thread
t2 = threading.Thread(target=add, args=(5, 10))
t2.start()
t2.join()</code></pre>
            </div>
            </p>
            <h5>Using Lambdas with Threads</h5>
            <p>Lambda expressions can also be used with threads, providing a concise way to define thread tasks. This is particularly useful for simple operations.</p>
            <p>
            <div>
                <pre><code class="language-python">import threading

# Create a thread with a lambda function
t3 = threading.Thread(target=lambda: print("Hello from a lambda thread"))
t3.start()
t3.join()</code></pre>
            </div>
            </p>
            <h5>Mutex for Synchronization</h5>
            <p>The <code>Lock</code> class from the <code>threading</code> module is used to ensure that only one thread accesses a critical section of code at a time. This prevents race conditions by locking the shared resource.</p>
            <p>
            <div>
                <pre><code class="language-python">import threading

counter = 0
counter_lock = threading.Lock()

def increment():
    global counter
    with counter_lock:
        counter += 1

# Create multiple threads
threads = [threading.Thread(target=increment) for _ in range(10)]

for t in threads:
    t.start()

for t in threads:
    t.join()

print(f"Counter: {counter}")</code></pre>
            </div>
            </p>
            <p>In this example, <code>counter_lock</code> ensures that only one thread modifies the <code>counter</code> variable at a time.</p>
            <h5>Deadlocks and Avoidance</h5>
            <p>Deadlocks can occur when multiple threads are waiting for each other to release resources. In Python, you can avoid deadlocks by carefully planning the order of acquiring locks or by using <code>try-lock</code> mechanisms.</p>
            <p>
            <div>
                <pre><code class="language-python">import threading

lock1 = threading.Lock()
lock2 = threading.Lock()

def task1():
    with lock1:
        print("Task 1 acquired lock1")
        with lock2:
            print("Task 1 acquired lock2")

def task2():
    with lock2:
        print("Task 2 acquired lock2")
        with lock1:
            print("Task 2 acquired lock1")

# Create threads
t4 = threading.Thread(target=task1)
t5 = threading.Thread(target=task2)

t4.start()
t5.start()
t4.join()
t5.join()</code></pre>
            </div>
            </p>
            <p>In this example, care must be taken to avoid deadlocks by ensuring that locks are acquired in a consistent order.</p>
            <h5>Condition Variables</h5>
            <p><code>Condition</code> variables allow threads to wait for some condition to be true before proceeding. This is useful in producer-consumer scenarios.</p>
            <p>
            <div>
                <pre><code class="language-python">import threading

condition = threading.Condition()
item_available = False

def producer():
    global item_available
    with condition:
        item_available = True
        print("Producer produced an item")
        condition.notify()

def consumer():
    global item_available
    with condition:
        condition.wait_for(lambda: item_available)
        print("Consumer consumed an item")
        item_available = False

# Create threads
t6 = threading.Thread(target=producer)
t7 = threading.Thread(target=consumer)

t6.start()
t7.start()
t6.join()
t7.join()</code></pre>
            </div>
            </p>
            <p>Here, the consumer waits for the producer to produce an item before proceeding.</p>
            <h5>Semaphores</h5>
            <p>Python's <code>threading</code> module includes <code>Semaphore</code> and <code>BoundedSemaphore</code> for managing access to a limited number of resources.</p>
            <p>
            <div>
                <pre><code class="language-python">import threading

sem = threading.Semaphore(2)  # Allows up to 2 threads to access the resource

def access_resource(thread_id):
    with sem:
        print(f"Thread {thread_id} is accessing the resource")
        # Simulate some work
        threading.Thread.sleep(1)

# Create multiple threads
threads = [threading.Thread(target=access_resource, args=(i,)) for i in range(5)]

for t in threads:
    t.start()

for t in threads:
    t.join()</code></pre>
            </div>
            </p>
            <p>In this example, the semaphore limits access to a resource, allowing only two threads to enter the critical section at a time.</p>
            <h5>Thread Local Storage</h5>
            <p>Python provides <code>threading.local()</code> to store data that should not be shared between threads.</p>
            <p>
            <div>
                <pre><code class="language-python">import threading

local_data = threading.local()

def process():
    local_data.value = 5
    print(f"Thread {threading.current_thread().name} has value {local_data.value}")

# Create threads
t8 = threading.Thread(target=process, name="Thread-A")
t9 = threading.Thread(target=process, name="Thread-B")

t8.start()
t9.start()
t8.join()
t9.join()</code></pre>
            </div>
            </p>
            <p>In this example, each thread has its own <code>local_data</code> value, independent of the others.</p>
            <h5>Atomic Operations</h5>
            <p>In multi-threaded Python programs, there is often confusion regarding whether certain operations are truly atomic. This confusion largely stems from the presence of the Global Interpreter Lock (GIL), which ensures that only one thread is executing Python bytecode at any given time. Some developers interpret this to mean that operations like <code>counter += 1</code> are automatically safe and cannot cause race conditions. However, this is <strong>not</strong> guaranteed by Python's documentation or design.</p>
            <p>While the GIL does prevent multiple threads from running Python <em>bytecode</em> simultaneously, many Python operations, including integer increments, actually consist of several steps under the hood (e.g., loading the current value, creating a new integer, and storing it). These intermediate steps can be interleaved with operations from other threads, making race conditions possible if no additional synchronization mechanism is employed. Therefore, if you need to ensure correct and consistent results when multiple threads modify a shared variable, you must use locks (like <code>threading.Lock</code>) or other thread-safe data structures.</p>
            <p>Below is an example illustrating the use of a lock to ensure a thread-safe increment of a shared <code>counter</code>:</p>
            <p>
            <div>
                <pre><code class="language-python">import threading

counter = 0
counter_lock = threading.Lock()

def safe_increment():
    global counter
    with counter_lock:
        temp = counter
        temp += 1
        counter = temp

# Create and start threads
threads = [threading.Thread(target=safe_increment) for _ in range(1000)]

for t in threads:
    t.start()

for t in threads:
    t.join()

print(f"Counter: {counter}")</code></pre>
            </div>
            </p>
            <p>In this example, <code>counter_lock</code> ensures that the increment operation is effectively atomic by preventing multiple threads from modifying <code>counter</code> at the same time. Without this lock, two or more threads could potentially load the same value of <code>counter</code>, increment it independently, and overwrite each other's updates—resulting in an incorrect final value. Keep in mind that <strong>the GIL itself does not guarantee atomicity</strong> for these kinds of operations, which is why locks (or other concurrency primitives) are essential when sharing mutable state across threads.</p>
            <h5>Performance Considerations and Best Practices</h5>
            <ul>
                <li>It is important to avoid creating an excessive number of threads, as this can lead to significant context switching overhead and increased memory usage. To manage a fixed number of threads efficiently, thread pools like <code>concurrent.futures.ThreadPoolExecutor</code> should be used.</li>
                <li>Minimizing lock contention is crucial for performance. To achieve this, fine-grained locks can be implemented, and the time spent in critical sections should be minimized to reduce the likelihood of threads waiting for access to shared resources.</li>
                <li>Appropriate synchronization mechanisms, such as locks, semaphores, and condition variables, should be used to coordinate thread activities and prevent data races. This ensures that threads operate safely without corrupting shared data.</li>
                <li>Understanding the Global Interpreter Lock (GIL) is essential, especially in Python. The GIL can limit the effectiveness of threading in CPU-bound applications by allowing only one thread to execute Python bytecode at a time. In such cases, using multiprocessing or other parallelism strategies may be more effective than threading.</li>
                <li>For background tasks, daemon threads should be used, as they automatically exit when the program terminates. This can be done by setting <code>thread.setDaemon(True)</code>, ensuring that these threads do not prevent the application from closing if they are still running.</li>
            </ul>
            <p>Here are some example code snippets demonstrating various aspects of multithreading in Python:</p>
            <p>
            <table>
                <tr>
                    <td>#</td>
                    <td>Example</td>
                    <td>Description</td>
                </tr>
                <tr>
                    <td>1</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/python/multithreading/01_single_worker_thread.py">single_worker_thread</a></td>
                    <td>Introduce the concept of threads by creating a single worker thread.</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/python/multithreading/02_thread_subclass.py">thread_subclass</a></td>
                    <td>Demonstrate how to create a custom thread class by subclassing <code>Thread</code>.</td>
                </tr>
                <tr>
                    <td>3</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/python/multithreading/03_multiple_worker_threads.py">multiple_worker_threads</a></td>
                    <td>Show how to create and manage multiple worker threads.</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/python/multithreading/04_race_condition.py">race_condition</a></td>
                    <td>Explain race conditions and their impact on multi-threaded applications.</td>
                </tr>
                <tr>
                    <td>5</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/python/multithreading/05_mutex.py">mutex</a></td>
                    <td>Illustrate the use of mutexes to protect shared resources and avoid race conditions.</td>
                </tr>
                <tr>
                    <td>6</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/python/multithreading/06_semaphore.py">semaphore</a></td>
                    <td>Demonstrate the use of semaphores to limit the number of concurrent threads accessing a shared resource.</td>
                </tr>
                <tr>
                    <td>7</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/python/multithreading/07_producer_consumer.py">producer_consumer</a></td>
                    <td>Present a classic multi-threading problem (Producer-Consumer) and its solution using synchronization mechanisms like mutexes and condition variables.</td>
                </tr>
                <tr>
                    <td>8</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/python/multithreading/08_fetch_parallel.py">fetch_parallel</a></td>
                    <td>Showcase a practical application of multi-threading for parallel fetching of data from multiple sources.</td>
                </tr>
                <tr>
                    <td>9</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/python/multithreading/09_merge_sort.py">merge_sort</a></td>
                    <td>Use multi-threading to parallelize a merge sort algorithm, demonstrating the potential for performance improvements.</td>
                </tr>
                <tr>
                    <td>10</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/python/multithreading/10_schedule_every_n_sec.py">schedule_every_n_sec</a></td>
                    <td>Show how to schedule tasks to run periodically at fixed intervals using threads.</td>
                </tr>
                <tr>
                    <td>11</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/python/multithreading/11_barrier.py">barrier</a></td>
                    <td>Demonstrate the use of barriers to synchronize multiple threads at a specific point in the execution.</td>
                </tr>
                <tr>
                    <td>12</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/python/multithreading/12_thread_local_storage.py">thread_local_storage</a></td>
                    <td>Illustrate the concept of Thread Local Storage (TLS) and how it can be used to store thread-specific data.</td>
                </tr>
                <tr>
                    <td>13</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/python/multithreading/13_thread_pool.py">thread_pool</a></td>
                    <td>Show how to create and use a thread pool to efficiently manage a fixed number of worker threads for executing multiple tasks.</td>
                </tr>
                <tr>
                    <td>14</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/python/multithreading/14_reader_writer_lock.py">reader_writer_lock</a></td>
                    <td>Explain the concept of Reader-Writer Locks and their use for efficient access to shared resources with multiple readers and a single writer.</td>
                    <td></td>
                </tr>
            </table>
            </p>
            <h4 id="examples-in-javascript-node-js-">Examples in JavaScript (Node.js)</h4>
            <p>Node.js traditionally uses a single-threaded event loop to handle asynchronous operations. However, since version <strong>10.5.0</strong>, Node.js has included support for worker threads, which allow multi-threaded execution. This is particularly useful for <strong>CPU-intensive</strong> tasks (e.g., image processing, cryptography), which can block the event loop and degrade performance in a purely single-threaded environment.</p>
            <p>Worker threads in Node.js are provided by the <code>worker_threads</code> module, enabling the creation of additional JavaScript execution contexts. Each worker thread runs in its own isolated V8 instance and does <strong>not</strong> share state with other worker threads or with the main thread. Instead, communication is accomplished by <strong>message passing</strong> and, optionally, by sharing specific memory buffers (e.g., <code>SharedArrayBuffer</code>).</p>
            <h5>Creating Worker Threads</h5>
            <p>To create a new worker thread, you instantiate the <code>Worker</code> class from the <code>worker_threads</code> module. The worker is initialized with a script (or a code string) to execute:</p>
            <p>
            <div>
                <pre><code class="language-javascript">// main.js
const { Worker } = require('worker_threads');

const worker = new Worker('./worker.js'); // Separate file containing worker code

worker.on('message', (message) =&gt; {
  console.log(<code>Received message from worker: ${message}</code>);
});

worker.on('error', (error) =&gt; {
  console.error(<code>Worker error: ${error}</code>);
});

worker.on('exit', (code) =&gt; {
  console.log(<code>Worker exited with code ${code}</code>);
});</code></pre>
            </div>
            </p>
            <p>
            <div>
                <pre><code class="language-javascript">// worker.js
const { parentPort } = require('worker_threads');

parentPort.postMessage('Hello from worker');</code></pre>
            </div>
            </p>
            <p>In this example:</p>
            <ol>
                <li><strong><code>main.js</code></strong> creates a <code>Worker</code> instance pointing to the <code>worker.js</code> file. </li>
                <li>The main thread listens for events: </li>
                <li><strong><code>message</code></strong>: Triggered when the worker sends data back. </li>
                <li><strong><code>error</code></strong>: Triggered if an uncaught exception occurs in the worker. </li>
                <li><strong><code>exit</code></strong>: Triggered when the worker stops execution. </li>
                <li><strong><code>worker.js</code></strong> obtains a reference to <code>parentPort</code> (the communication channel back to the main thread) and sends a message.</li>
            </ol>
            <h5>Handling Communication</h5>
            <p>Communication between the main thread and worker threads is done via <strong>message passing</strong> using <code>postMessage</code> and <code>on('message', callback)</code>. This serialization-based messaging ensures that no implicit shared state is introduced.</p>
            <p>
            <div>
                <pre><code class="language-javascript">// main.js (continued)
worker.postMessage({ command: 'start', data: 'example data' });

// worker.js (continued)
const { parentPort } = require('worker_threads');

parentPort.on('message', (message) =&gt; {
  console.log(<code>Worker received: ${JSON.stringify(message)}</code>);
  // Perform CPU-intensive task or other operations
  parentPort.postMessage('Processing complete');
});</code></pre>
            </div>
            </p>
            <p>Here, the main thread sends a structured message to the worker with a <code>command</code> property and some <code>data</code>. The worker, upon receiving it, can process the data and then respond back to the main thread.</p>
            <h5>Worker Termination</h5>
            <p>Workers can be terminated from <strong>either</strong> the main thread or within the worker itself.</p>
            <p>I. From the main thread, you can call <code>worker.terminate()</code>, which returns a Promise resolving to the exit code:</p>
            <p>
            <div>
                <pre><code class="language-javascript">// main.js
worker.terminate().then((exitCode) =&gt; {
  console.log(<code>Worker terminated with code ${exitCode}</code>);
});</code></pre>
            </div>
            </p>
            <p>II. Inside the worker, you can terminate execution using <code>process.exit()</code>:</p>
            <p>
            <div>
                <pre><code class="language-javascript">// worker.js
process.exit(0); // Graceful exit</code></pre>
            </div>
            </p>
            <p>Terminating the worker ends its event loop and frees its resources. Any pending operations in the worker are discarded once termination begins.</p>
            <h5>Passing Data to Workers</h5>
            <p>You can also pass initial data to the worker at creation time through the <code>Worker</code> constructor using the <code>workerData</code> option:</p>
            <p>
            <div>
                <pre><code class="language-javascript">// main.js
const { Worker } = require('worker_threads');

const worker = new Worker('./worker.js', {
  workerData: { initialData: 'Hello' }
});</code></pre>
            </div>
            </p>
            <p>Within <strong><code>worker.js</code></strong>:</p>
            <p>
            <div>
                <pre><code class="language-javascript">// worker.js
const { workerData, parentPort } = require('worker_threads');
console.log(workerData); // { initialData: 'Hello' }

// Do work, then optionally respond
parentPort.postMessage('Worker started with initial data!');</code></pre>
            </div>
            </p>
            <p>This pattern is useful for small or essential bits of configuration data that the worker needs right from startup.</p>
            <h5>Transferring Ownership of Objects</h5>
            <p>Some objects (like <code>ArrayBuffer</code> and <code>MessagePort</code>) can be <strong>transferred</strong> to a worker, meaning the main thread loses ownership and can no longer use the object once it’s transferred. This can be more efficient than copying large data structures.</p>
            <p>
            <div>
                <pre><code class="language-javascript">// main.js
const { Worker } = require('worker_threads');
const buffer = new SharedArrayBuffer(1024);

const worker = new Worker('./worker.js', { workerData: buffer });</code></pre>
            </div>
            </p>
            <p>In this snippet, a <code>SharedArrayBuffer</code> is provided to the worker. Both the main thread and the worker thread can access and modify this shared memory concurrently, which is useful for scenarios requiring <strong>high-performance concurrent access</strong> (e.g., streaming or real-time data processing). Synchronization in such cases typically uses <code>Atomics</code> (part of JavaScript’s standard library).</p>
            <h5>Using <code>Atomics</code> and <code>SharedArrayBuffer</code></h5>
            <p>When sharing memory (via <code>SharedArrayBuffer</code>), JavaScript provides the <code>Atomics</code> object for performing atomic operations (e.g., <code>Atomics.add</code>, <code>Atomics.load</code>, <code>Atomics.store</code>). Unlike higher-level synchronization primitives in other languages (like mutexes or semaphores), JavaScript concurrency with <code>SharedArrayBuffer</code> and <code>Atomics</code> relies on these low-level primitives for correctness.</p>
            <p><strong>Example</strong>:</p>
            <p>
            <div>
                <pre><code class="language-javascript">// main.js
const { Worker } = require('worker_threads');
const sharedBuffer = new SharedArrayBuffer(4);  // Enough for one 32-bit integer

const worker = new Worker('./worker.js', { workerData: sharedBuffer });

// Optionally communicate via messages as well
worker.on('message', (msg) =&gt; {
  console.log('Message from worker:', msg);
});</code></pre>
            </div>
            </p>
            <p>
            <div>
                <pre><code class="language-javascript">// worker.js
const { parentPort, workerData } = require('worker_threads');
const { Atomics, Int32Array } = globalThis;

// Interpret the shared buffer as a 32-bit integer array of length 1
const sharedArray = new Int32Array(workerData);

for (let i = 0; i &lt; 100000; i++) {
  // Atomically increment the integer
  Atomics.add(sharedArray, 0, 1);
}

// Once done, send a message back
parentPort.postMessage('Incrementing done!');</code></pre>
            </div>
            </p>
            <p>In this example:</p>
            <ol>
                <li>The main thread creates a <code>SharedArrayBuffer</code> of 4 bytes (enough space for an <code>Int32Array</code> element). </li>
                <li>That buffer is passed to the worker. </li>
                <li>The worker increments the shared integer atomically 100,000 times using <code>Atomics.add</code>. </li>
                <li>Both threads can read the final value in <code>sharedArray[0]</code> safely, without data races.</li>
            </ol>
            <h5>Error Handling</h5>
            <p>Proper error handling in multi-threaded environments is crucial:</p>
            <p>
            <div>
                <pre><code class="language-javascript">// main.js
worker.on('error', (error) =&gt; {
  console.error('Worker error:', error);
});

// worker.js
try {
  // perform some operation that might throw
  throw new Error('Something went wrong');
} catch (err) {
  // Handle locally or propagate
  parentPort.postMessage({ error: err.message });
  // Optionally re-throw, or process.exit(1) for immediate termination
}</code></pre>
            </div>
            </p>
            <p>If an uncaught exception occurs in the worker, the main thread’s <code>error</code> event will fire, allowing you to clean up resources or attempt a restart. Consider carefully whether to handle errors in the worker itself or bubble them up to the main thread.</p>
            <h5>Performance Considerations and Best Practices</h5>
            <ul>
                <li>When handling CPU-intensive tasks, worker threads are particularly advantageous as they can execute computationally heavy operations without blocking the event loop. For tasks that are I/O-bound, however, the Node.js event loop is generally more efficient and sufficient.</li>
                <li>It is important to avoid heavy data transfers between the main thread and worker threads because the process of serialization and deserialization can be inefficient. To enhance efficiency, shared memory structures like <code>SharedArrayBuffer</code> should be used when possible, as they allow for direct memory access without the overhead of copying data.</li>
                <li>Proper management of the worker lifecycle is important. Workers should be terminated once they have completed their tasks to prevent resource leaks, which can occur if worker threads remain active unnecessarily and continue consuming system resources.</li>
                <li>Strong error handling is necessary for maintaining stability and reliability in applications that use worker threads. This involves catching and managing exceptions and errors that may occur within worker threads, making sure that these failures do not lead to crashes or unpredictable behavior in the main application.</li>
                <li>Security considerations must be taken into account, as worker threads have access to the complete Node.js API and run in separate V8 instances. To mitigate security risks, it is important to avoid executing untrusted code within worker threads, as this could potentially lead to vulnerabilities and exploits in the system.</li>
            </ul>
            <h5>Example: Prime Number Calculation</h5>
            <p>Below is a complete example of using worker threads to calculate prime numbers, demonstrating data passing, message handling, and worker management.</p>
            <p>
            <div>
                <pre><code class="language-javascript">// main.js
const { Worker } = require('worker_threads');

function runService(workerData) {
  return new Promise((resolve, reject) =&gt; {
    const worker = new Worker('./primeWorker.js', { workerData });
    worker.on('message', resolve);
    worker.on('error', reject);
    worker.on('exit', (code) =&gt; {
      if (code !== 0) {
        reject(new Error(<code>Worker stopped with exit code ${code}</code>));
      }
    });
  });
}

runService(10).then((result) =&gt; console.log(result)).catch((err) =&gt; console.error(err));</code></pre>
            </div>
            </p>
            <p>
            <div>
                <pre><code class="language-javascript">// primeWorker.js
const { parentPort, workerData } = require('worker_threads');

function isPrime(num) {
  for (let i = 2, sqrt = Math.sqrt(num); i &lt;= sqrt; i++) {
    if (num % i === 0) return false;
  }
  return num &gt; 1;
}

const primes = [];
for (let i = 2; i &lt;= workerData; i++) {
  if (isPrime(i)) primes.push(i);
}

parentPort.postMessage(primes);</code></pre>
            </div>
            </p>
            <p>In this example, the main thread delegates the task of finding prime numbers up to a certain limit to a worker thread. The worker calculates the primes and sends the results back to the main thread using <code>parentPort.postMessage()</code>.</p>
            <p>Here are some example code snippets demonstrating various aspects of multithreading in JavaScript (Node.js):</p>
            <p>
            <table>
                <tr>
                    <td>#</td>
                    <td>Example</td>
                    <td>Description</td>
                </tr>
                <tr>
                    <td>1</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/js/multithreading/01_single_worker_thread.js">single_worker_thread</a></td>
                    <td>Introduce the concept of threads by creating a single worker thread using Web Workers.</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/js/multithreading/02_thread_subclass.js">thread_subclass</a></td>
                    <td>Demonstrate how to create a custom thread class by extending the <code>Worker</code> class.</td>
                </tr>
                <tr>
                    <td>3</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/js/multithreading/03_multiple_worker_threads.js">multiple_worker_threads</a></td>
                    <td>Show how to create and manage multiple worker threads using Web Workers.</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/js/multithreading/04_race_condition.js">race_condition</a></td>
                    <td>Explain race conditions and their impact on multi-threaded applications using JavaScript examples.</td>
                </tr>
                <tr>
                    <td>5</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/js/multithreading/05_mutex.js">mutex</a></td>
                    <td>Illustrate the use of <code>Atomics</code> and <code>SharedArrayBuffer</code> to protect shared resources and avoid race conditions in JavaScript applications.</td>
                </tr>
                <tr>
                    <td>6</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/js/multithreading/06_semaphore.js">semaphore</a></td>
                    <td>Demonstrate the use of semaphores to limit the number of concurrent threads accessing a shared resource in JavaScript applications using <code>Atomics</code> and <code>SharedArrayBuffer</code>.</td>
                </tr>
                <tr>
                    <td>7</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/js/multithreading/07_producer_consumer.js">producer_consumer</a></td>
                    <td>Present a classic multi-threading problem (Producer-Consumer) and its solution using JavaScript synchronization mechanisms like <code>Atomics</code> and <code>SharedArrayBuffer</code>.</td>
                </tr>
                <tr>
                    <td>8</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/js/multithreading/08_fetch_parallel.js">fetch_parallel</a></td>
                    <td>Showcase a practical application of multi-threading for parallel fetching of data from multiple sources using Web Workers.</td>
                </tr>
                <tr>
                    <td>9</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/js/multithreading/09_merge_sort.js">merge_sort</a></td>
                    <td>Use multi-threading in JavaScript to parallelize a merge sort algorithm, demonstrating the potential for performance improvements.</td>
                </tr>
                <tr>
                    <td>10</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/js/multithreading/10_schedule_every_n_sec.js">schedule_every_n_sec</a></td>
                    <td>Show how to schedule tasks to run periodically at fixed intervals using JavaScript and Web Workers.</td>
                </tr>
                <tr>
                    <td>11</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/js/multithreading/11_barrier.js">barrier</a></td>
                    <td>Demonstrate the use of barriers to synchronize multiple threads at a specific point in the execution.</td>
                </tr>
                <tr>
                    <td>12</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/js/multithreading/12_thread_local_storage.js">thread_local_storage</a></td>
                    <td>Illustrate the concept of Thread Local Storage (TLS) and how it can be used to store thread-specific data.</td>
                </tr>
                <tr>
                    <td>13</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/js/multithreading/13_thread_pool.js">thread_pool</a></td>
                    <td>Show how to create and use a thread pool to efficiently manage a fixed number of worker threads for executing multiple tasks.</td>
                </tr>
                <tr>
                    <td>14</td>
                    <td><a href="https://github.com/djeada/Parallel-And-Concurrent-Programming/blob/master/src/js/multithreading/14_reader_writer_lock.js">reader_writer_lock</a></td>
                    <td>Explain the concept of Reader-Writer Locks and their use for efficient access to shared resources with multiple readers and a single writer.</td>
                </tr>
            </table>
            </p>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol><a href="#multithreading">Multithreading</a>
                <ol>
                    <li><a href="#thread-pool-vs-on-demand-thread">Thread Pool vs On-Demand Thread</a></li>
                    <li><a href="#worker-threads">Worker Threads</a></li>
                    <li><a href="#advantages-of-threads-over-processes">Advantages of Threads over Processes</a></li>
                    <li><a href="#challenges-with-multithreading">Challenges with Multithreading</a>
                        <ol>
                            <li><a href="#data-race">Data Race</a></li>
                            <li><a href="#mutex">Mutex</a></li>
                            <li><a href="#atomic">Atomic</a></li>
                            <li><a href="#deadlock">Deadlock</a></li>
                            <li><a href="#livelock">Livelock</a></li>
                            <li><a href="#semaphore">Semaphore</a></li>
                            <li><a href="#common-misconceptions">Common Misconceptions</a></li>
                            <li><a href="#problems-for-which-multithreading-is-the-answer">Problems for which multithreading is the answer</a></li>
                            <li><a href="#problems-for-which-multithreading-is-not-the-answer">Problems for which multithreading is not the answer</a></li>
                        </ol>
                    </li>
                    <li><a href="#examples">Examples</a>
                        <ol>
                            <li><a href="#examples-in-c-">Examples in C++</a></li>
                            <li><a href="#examples-in-python">Examples in Python</a></li>
                            <li><a href="#examples-in-javascript-node-js-">Examples in JavaScript (Node.js)</a></li>
                        </ol>
                    </li>
                </ol>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/parallel_and_concurrent_programming/01_basic_terminology.html">Basic Terminology</a></li>
                    <li><a href="https://adamdjellouli.com/articles/parallel_and_concurrent_programming/02_multithreading.html">Multithreading</a></li>
                    <li><a href="https://adamdjellouli.com/articles/parallel_and_concurrent_programming/03_multiprocessing.html">Multiprocessing</a></li>
                    <li><a href="https://adamdjellouli.com/articles/parallel_and_concurrent_programming/04_asynchronous_programming.html">Asynchronous Programming</a></li>
                    <li><a href="https://adamdjellouli.com/articles/parallel_and_concurrent_programming/05_mpi.html">Mpi</a></li>
                    <li><a href="https://adamdjellouli.com/articles/parallel_and_concurrent_programming/06_hardware.html">Hardware</a></li>
                    <li><a href="https://adamdjellouli.com/articles/parallel_and_concurrent_programming/07_evaluating_performance.html">Evaluating Performance</a></li>
                    <li><a href="https://adamdjellouli.com/articles/parallel_and_concurrent_programming/08_designing_parallel_programs.html">Designing Parallel Programs</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                © Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>