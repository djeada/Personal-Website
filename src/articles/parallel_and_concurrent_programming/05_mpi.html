<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Message Passing Interface (MPI)</title>
    <meta content="The Message Passing Interface (MPI) is a standardized and portable message-passing system designed to function on parallel computing architectures." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>Last modified: July 29, 2024</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="message-passing-interface-mpi-">Message Passing Interface (MPI)</h2>
            <p>The Message Passing Interface (MPI) is a standardized and portable message-passing system designed to function on parallel computing architectures. MPI allows a collection of processes to execute programs in a standard sequential language, augmented with functions for sending and receiving messages. This system is central to parallel programming and can adapt designs developed using traditional techniques.</p>
            <h3 id="key-concepts-of-mpi">Key Concepts of MPI</h3>
            <ul>
                <li><strong>Message-Passing Programming</strong>: Involves processes communicating through library calls to send and receive messages.</li>
                <li><strong>Standardization</strong>: MPI is the de facto standard for message-passing libraries.</li>
                <li><strong>Broad Applicability</strong>: Techniques in MPI are applicable to other systems like p4, PVM, Express, and PARMACS.</li>
                <li><strong>Focus on Essentials</strong>: MPI consists of 129 functions, but a subset of 24 functions covers most applications.</li>
            </ul>
            <h3 id="mpi-programming-model">MPI Programming Model</h3>
            <ul>
                <li><strong>Process Communication</strong>: Processes communicate by sending and receiving messages via library routines.</li>
                <li><strong>Fixed Processes</strong>: Typically, a fixed set of processes is created at initialization, with one process per processor.</li>
                <li><strong>Multiple Programs</strong>: Different programs can execute on different processes, referred to as the Multiple Program Multiple Data (MPMD) model.</li>
                <li><strong>Communication Types</strong>: </li>
                <li><strong>Point-to-Point</strong>: Sending messages between specific processes.</li>
                <li><strong>Collective Operations</strong>: Global operations like summation and broadcast.</li>
                <li><strong>Asynchronous Communication</strong>: MPI can probe for messages asynchronously.</li>
                <li><strong>Modular Programming</strong>: Uses communicators to define modules that encapsulate internal communication structures.</li>
            </ul>
            <h3 id="implementing-parallel-algorithms-with-mpi">Implementing Parallel Algorithms with MPI</h3>
            <ul>
                <li><strong>Direct Implementation</strong>: Algorithms with one task per processor use point-to-point or collective communication.</li>
                <li><strong>Refinement Needed</strong>: Dynamic task creation or concurrent task execution needs refinement for MPI.</li>
                <li><strong>Branch-and-Bound Example</strong>: Dynamic search task creation must be refined to create a fixed set of worker processes.</li>
            </ul>
            <h3 id="mpi-basics">MPI Basics</h3>
            <p>While MPI is complex, many problems can be solved using just six core functions. These functions initiate and terminate computations, identify processes, and send and receive messages.</p>
            <p>
            <table>
                <tr>
                    <td><strong>Function</strong></td>
                    <td><strong>Purpose</strong></td>
                    <td><strong>Parameters</strong></td>
                    <td><strong>Returns</strong></td>
                </tr>
                <tr>
                    <td><strong>MPI_INIT</strong></td>
                    <td>Initiate an MPI computation.</td>
                    <td><code>int *argc</code>, <code>char ***argv</code> (required only in C language binding).</td>
                    <td>None</td>
                </tr>
                <tr>
                    <td><strong>MPI_FINALIZE</strong></td>
                    <td>Terminate an MPI computation.</td>
                    <td>None</td>
                    <td>None</td>
                </tr>
                <tr>
                    <td><strong>MPI_COMM_SIZE</strong></td>
                    <td>Determine the number of processes in a computation.</td>
                    <td><strong>IN</strong>: <code>comm</code> (communicator handle)</td>
                    <td><strong>OUT</strong>: <code>size</code> (number of processes in the group).</td>
                </tr>
                <tr>
                    <td><strong>MPI_COMM_RANK</strong></td>
                    <td>Determine the identifier of the current process.</td>
                    <td><strong>IN</strong>: <code>comm</code> (communicator handle)</td>
                    <td><strong>OUT</strong>: <code>pid</code> (process id in the group).</td>
                </tr>
                <tr>
                    <td><strong>MPI_SEND</strong></td>
                    <td>Send a message.</td>
                    <td><strong>IN</strong>: <code>buf</code> (address of send buffer) <br /> <strong>IN</strong>: <code>count</code> (number of elements to send) <br /> <strong>IN</strong>: <code>datatype</code> (datatype of send buffer elements) <br /> <strong>IN</strong>: <code>dest</code> (destination process id) <br /> <strong>IN</strong>: <code>tag</code> (message tag) <br /> <strong>IN</strong>: <code>comm</code> (communicator handle).</td>
                    <td>None</td>
                </tr>
                <tr>
                    <td><strong>MPI_RECV</strong></td>
                    <td>Receive a message.</td>
                    <td><strong>IN</strong>: <code>count</code> (size of receive buffer in elements) <br /> <strong>IN</strong>: <code>datatype</code> (datatype of receive buffer elements) <br /> <strong>IN</strong>: <code>source</code> (source process id or <code>MPI_ANY_SOURCE</code>) <br /> <strong>IN</strong>: <code>tag</code> (message tag or <code>MPI_ANY_TAG</code>) <br /> <strong>IN</strong>: <code>comm</code> (communicator handle)</td>
                    <td><strong>OUT</strong>: <code>buf</code> (address of receive buffer) <br /> <strong>OUT</strong>: <code>status</code> (status object).</td>
                </tr>
            </table>
            </p>
            <h4 id="introduction-to-mpi-communicators">Introduction to MPI Communicators</h4>
            <ul>
                <li><strong>Communicator Handle</strong>: Most MPI functions (except MPI_INIT and MPI_FINALIZE) require a communicator handle. A communicator defines the process group and context for operations.</li>
                <li><strong>MPI_COMM_WORLD</strong>: The default communicator, representing all processes involved in a computation.</li>
                <li><strong>Modular Programs</strong>: Communicators help in identifying process subsets and ensuring that messages for different purposes are not confused.</li>
            </ul>
            <h4 id="mpi-initialization-and-finalization">MPI Initialization and Finalization</h4>
            <ul>
                <li><strong>MPI_INIT()</strong>: Must be called once per process to initiate an MPI computation.</li>
                <li><strong>MPI_FINALIZE()</strong>: Shuts down an MPI computation; no MPI functions can be called afterward.</li>
            </ul>
            <h4 id="determining-process-information">Determining Process Information</h4>
            <ul>
                <li><strong>MPI_COMM_SIZE(comm, size)</strong>: Determines the number of processes in the computation.</li>
                <li><strong>comm</strong>: Communicator handle.</li>
                <li><strong>size</strong>: Number of processes in the communicator's group.</li>
                <li><strong>MPI_COMM_RANK(comm, pid)</strong>: Determines the unique identifier of the current process.</li>
                <li><strong>comm</strong>: Communicator handle.</li>
                <li><strong>pid</strong>: Process ID in the communicator's group.</li>
            </ul>
            <h4 id="example-program">Example Program</h4>
            <p>The following example program illustrates basic MPI functions. It initializes MPI, determines the number of processes and their identifiers, prints a message, and finalizes the computation.</p>
            <p>
            <div>
                <pre><code class="language-text">program main
begin
    MPI_INIT()  // Initialize computation
    MPI_COMM_SIZE(MPI_COMM_WORLD, count)  // Find number of processes
    MPI_COMM_RANK(MPI_COMM_WORLD, myid)  // Find my ID
    print("I am", myid, "of", count)  // Print message
    MPI_FINALIZE()  // Shut down
end</code></pre>
            </div>
            </p>
            <h4 id="execution-and-output">Execution and Output</h4>
            <ul>
                <li><strong>Program Execution</strong>: Typically started with a command like <code>myprog -n 4</code>, where <code>myprog</code> is the executable, and <code>-n 4</code> specifies four processes.</li>
                <li><strong>Sample Output</strong>: If executed by four processes, the output may look like:</li>
            </ul>
            <p>
            <div>
                <pre><code class="language-text">I am 1 of 4
  I am 3 of 4
  I am 0 of 4
  I am 2 of 4</code></pre>
            </div>
            </p>
            <h4 id="sending-and-receiving-messages">Sending and Receiving Messages</h4>
            <ul>
                <li><strong>MPI_SEND(buf, count, datatype, dest, tag, comm)</strong>: Sends a message.</li>
                <li><strong>buf</strong>: Address of the send buffer.</li>
                <li><strong>count</strong>: Number of elements to send.</li>
                <li><strong>datatype</strong>: Datatype of the elements.</li>
                <li><strong>dest</strong>: Destination process ID.</li>
                <li><strong>tag</strong>: Message tag.</li>
                <li>
                    <p><strong>comm</strong>: Communicator handle.</p>
                </li>
                <li>
                    <p><strong>MPI_RECV(buf, count, datatype, source, tag, comm, status)</strong>: Receives a message.</p>
                </li>
                <li><strong>buf</strong>: Address of the receive buffer.</li>
                <li><strong>count</strong>: Size of the receive buffer.</li>
                <li><strong>datatype</strong>: Datatype of the elements.</li>
                <li><strong>source</strong>: Source process ID or <code>MPI_ANY_SOURCE</code>.</li>
                <li><strong>tag</strong>: Message tag or <code>MPI_ANY_TAG</code>.</li>
                <li><strong>comm</strong>: Communicator handle.</li>
                <li><strong>status</strong>: Status object.</li>
            </ul>
            <h4 id="example-program-with-mpi_send-and-mpi_recv">Example Program with MPI_SEND and MPI_RECV</h4>
            <p>This example program demonstrates the use of MPI_SEND and MPI_RECV to implement a bridge construction algorithm. Two processes execute different tasks: the first process (foundry) sends messages, and the second process (bridge) receives them.</p>
            <p>
            <div>
                <pre><code class="language-text">program main
begin
    MPI_INIT()  // Initialize computation
    MPI_COMM_SIZE(MPI_COMM_WORLD, count)  // Find number of processes
    if count != 2 then exit  // Must have exactly 2 processes
    MPI_COMM_RANK(MPI_COMM_WORLD, myid)  // Find my ID
    
    if myid = 0 then  // Process 0
        foundry(100)  // Execute foundry procedure
    else  // Process 1
        bridge()  // Execute bridge procedure
    endif
    
    MPI_FINALIZE()  // Shut down
end

procedure foundry(numgirders)  // Code for process 0
begin
    for i = 1 to numgirders  // Send messages
        MPI_SEND(i, 1, MPI_INT, 1, 0, MPI_COMM_WORLD)
    endfor
    i = -1  // Send shutdown message
    MPI_SEND(i, 1, MPI_INT, 1, 0, MPI_COMM_WORLD)
end

procedure bridge  // Code for process 1
begin
    MPI_RECV(msg, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, status)
    while msg != -1 do  // Receive messages
        use_girder(msg)  // Use received message
        MPI_RECV(msg, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, status)
    enddo
end</code></pre>
            </div>
            </p>
            <p>Explanation</p>
            <ul>
                <li><strong>Process 0 (foundry)</strong>: Sends 100 integer messages to Process 1 and a shutdown message (âˆ’1).</li>
                <li><strong>Process 1 (bridge)</strong>: Receives messages from Process 0 until the shutdown message is received. </li>
            </ul>
            <p>This program demonstrates how different tasks can be coordinated using MPI_SEND and MPI_RECV within an MPI computation.</p>
            <h3 id="language-bindings">Language Bindings</h3>
            <p>MPI supports multiple programming languages, including C and Fortran. The functions described can be used across different languages with specific bindings tailored to each languageâ€™s syntax.</p>
            <h4 id="c-language-binding">C Language Binding</h4>
            <ul>
                <li><strong>Function Names</strong>: Use the MPI prefix and capitalize the first letter of the function name.</li>
                <li><strong>Return Codes</strong>: Functions return integer codes; <code>MPI_SUCCESS</code> indicates successful completion.</li>
                <li><strong>Constants and Handles</strong>: Defined in <code>mpi.h</code>, which must be included in any MPI program.</li>
                <li><strong>Parameter Passing</strong>:</li>
                <li><strong>IN Parameters</strong>: Passed by value.</li>
                <li><strong>OUT and INOUT Parameters</strong>: Passed by reference (pointers).</li>
                <li><strong>Status Variables</strong>: Defined as <code>MPI_Status</code>, a structure with fields for source (<code>status.MPI_SOURCE</code>) and tag (<code>status.MPI_TAG</code>).</li>
                <li><strong>MPI Datatypes</strong>: Specific to C, such as <code>MPI_CHAR</code>, <code>MPI_INT</code>, <code>MPI_FLOAT</code>, etc.</li>
            </ul>
            <h4 id="fortran-language-binding">Fortran Language Binding</h4>
            <ul>
                <li><strong>Function Names</strong>: Written in upper case.</li>
                <li><strong>Return Codes</strong>: Represented by an additional integer argument; <code>MPI_SUCCESS</code> indicates successful completion.</li>
                <li><strong>Constants and Handles</strong>: Defined in <code>mpif.h</code>, which must be included in any MPI program.</li>
                <li><strong>Handles</strong>: Represented by <code>INTEGER</code>.</li>
                <li><strong>Status Variables</strong>: Defined as an array of integers of size <code>MPI_STATUS_SIZE</code>, with constants <code>MPI_SOURCE</code> and <code>MPI_TAG</code> indexing the source and tag fields.</li>
                <li><strong>MPI Datatypes</strong>: Specific to Fortran, such as <code>MPI_INTEGER</code>, <code>MPI_REAL</code>, <code>MPI_DOUBLE_PRECISION</code>, etc.</li>
            </ul>
            <h3 id="example-pairwise-interactions-algorithm">Example: Pairwise Interactions Algorithm</h3>
            <p>The pairwise interactions algorithm involves T tasks connected in a ring, computing interactions involving N data over T-1 phases. Below are C and Fortran versions of an MPI implementation.</p>
            <h4 id="c-version">C Version</h4>
            <p>
            <div>
                <pre><code class="language-clike">#include "mpi.h"  /* Include file */

main(int argc, char *argv[]) {  /* Main program */
    int myid, np, ierr, lnbr, rnbr;
    real x[300], buff[300], forces[300];
    MPI_Status status;

    ierr = MPI_Init(&amp;argc, &amp;argv);  /* Initialize */
    if(ierr != MPI_SUCCESS) {  /* Check return code */
        // Error handling
    }
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;myid);  /* Get my ID */
    MPI_Comm_size(MPI_COMM_WORLD, &amp;np);  /* Get total processes */

    // Compute left and right neighbors in the ring
    lnbr = (myid - 1 + np) % np;
    rnbr = (myid + 1) % np;

    // Main computation loop
    for (int phase = 0; phase &lt; np - 1; ++phase) {
        MPI_Sendrecv_replace(buff, 300, MPI_REAL, rnbr, 0, lnbr, 0, MPI_COMM_WORLD, &amp;status);
        // Compute interactions here
    }

    MPI_Finalize();  /* Finalize MPI */
}</code></pre>
            </div>
            </p>
            <h4 id="fortran-version">Fortran Version</h4>
            <p>
            <div>
                <pre><code class="language-fortran">PROGRAM main
    INCLUDE 'mpif.h'  ! Include file
    INTEGER :: myid, np, ierr, lnbr, rnbr
    REAL :: x(300), buff(300), forces(300)
    INTEGER :: status(MPI_STATUS_SIZE)

    CALL MPI_INIT(ierr)  ! Initialize
    IF (ierr .NE. MPI_SUCCESS) THEN
        ! Error handling
    END IF
    CALL MPI_COMM_RANK(MPI_COMM_WORLD, myid, ierr)  ! Get my ID
    CALL MPI_COMM_SIZE(MPI_COMM_WORLD, np, ierr)  ! Get total processes

    ! Compute left and right neighbors in the ring
    lnbr = MOD(myid - 1 + np, np)
    rnbr = MOD(myid + 1, np)

    ! Main computation loop
    DO phase = 1, np - 1
        CALL MPI_SENDRECV_REPLACE(buff, 300, MPI_REAL, rnbr, 0, lnbr, 0, MPI_COMM_WORLD, status, ierr)
        ! Compute interactions here
    END DO

    CALL MPI_FINALIZE(ierr)  ! Finalize MPI
END PROGRAM main</code></pre>
            </div>
            </p>
            <h3 id="determinism">Determinism</h3>
            <h4 id="introduction-to-determinism">Introduction to Determinism</h4>
            <p>Message-passing programming models are inherently nondeterministic because the arrival order of messages sent from two processes (A and B) to a third process (C) is not defined. MPI guarantees the order of messages sent from one process to another, but ensuring a computation is deterministic is the programmer's responsibility.</p>
            <h4 id="ensuring-determinism">Ensuring Determinism</h4>
            <ul>
                <li><strong>Source Specifier</strong>: The <code>MPI_RECV</code> function allows specifying a source process or accepting messages from any source (<code>MPI_ANY_SOURCE</code>). Specifying a source helps avoid errors due to time-dependent message arrival.</li>
                <li><strong>Message Tags</strong>: Tags help distinguish between different messages. The sender assigns a tag using the <code>tag</code> field in <code>MPI_SEND</code>, and the receiver can specify which tag to receive or accept any tag (<code>MPI_ANY_TAG</code>).</li>
            </ul>
            <h3 id="example-nondeterministic-program">Example: Nondeterministic Program</h3>
            <p>The following example demonstrates a nondeterministic MPI program. It implements the symmetric pairwise interaction algorithm, where messages are communicated halfway around a ring and then returned to the originating process.</p>
            <h4 id="c-version">C Version</h4>
            <p>
            <div>
                <pre><code class="language-clike">main(int *argc, char *argv[]) {
    int rnbr, rdest, myid, np;
    float buff[600];
    MPI_Status status;

    MPI_Init(argc, &amp;argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;myid);
    MPI_Comm_size(MPI_COMM_WORLD, &amp;np);
    rnbr = (myid + 1) % np;
    rdest = (myid + np / 2 + 1) % np;

    // Circulate data around ring
    for (int i = 0; i &lt; np / 2; i++) {
        MPI_Send(buff, 600, MPI_FLOAT, rnbr, 1, MPI_COMM_WORLD);
        MPI_Recv(buff, 600, MPI_FLOAT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &amp;status);
    }

    // Return accumulated data to source
    MPI_Send(buff, 300, MPI_FLOAT, rdest, 2, MPI_COMM_WORLD);
    MPI_Recv(buff, 300, MPI_FLOAT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &amp;status);

    MPI_Finalize();
}</code></pre>
            </div>
            </p>
            <p>This program may suffer from nondeterminism because it uses <code>MPI_ANY_SOURCE</code> and <code>MPI_ANY_TAG</code>, which can lead to unexpected message order. To ensure determinism, specific sources and tags should be used where possible.</p>
            <h3 id="mpi-global-operations">MPI Global Operations</h3>
            <p>Parallel algorithms often require coordinated communication among multiple processes. MPI provides specialized collective communication functions to facilitate these operations efficiently. These functions include synchronization, data distribution, and reduction operations.</p>
            <h4 id="key-mpi-collective-communication-functions">Key MPI Collective Communication Functions</h4>
            <p>
            <table>
                <tr>
                    <td><strong>Function</strong></td>
                    <td><strong>Purpose</strong></td>
                    <td><strong>Parameters</strong></td>
                    <td><strong>Returns</strong></td>
                </tr>
                <tr>
                    <td><strong>MPI_BARRIER</strong></td>
                    <td>Synchronizes all processes in a communicator.</td>
                    <td><strong>IN</strong>: <code>comm</code> (communicator handle)</td>
                    <td>None</td>
                </tr>
                <tr>
                    <td><strong>MPI_BCAST</strong></td>
                    <td>Broadcasts data from one root process to all other processes.</td>
                    <td><strong>INOUT</strong>: <code>inbuf</code> (address of the input buffer) <br /> <strong>IN</strong>: <code>incnt</code> (number of elements in input buffer) <br /> <strong>IN</strong>: <code>intype</code> (datatype of input buffer elements) <br /> <strong>IN</strong>: <code>root</code> (process ID of root process) <br /> <strong>IN</strong>: <code>comm</code> (communicator handle)</td>
                    <td>None</td>
                </tr>
                <tr>
                    <td><strong>MPI_GATHER</strong></td>
                    <td>Gathers data from all processes to one root process.</td>
                    <td><strong>IN</strong>: <code>inbuf</code> (address of input buffer) <br /> <strong>IN</strong>: <code>incnt</code> (number of elements sent to each process) <br /> <strong>IN</strong>: <code>intype</code> (datatype of input buffer elements) <br /> <strong>IN</strong>: <code>outcnt</code> (number of elements received from each process) <br /> <strong>IN</strong>: <code>outtype</code> (datatype of output buffer elements) <br /> <strong>IN</strong>: <code>root</code> (process ID of root process) <br /> <strong>IN</strong>: <code>comm</code> (communicator handle)</td>
                    <td><strong>OUT</strong>: <code>outbuf</code> (address of output buffer)</td>
                </tr>
                <tr>
                    <td><strong>MPI_SCATTER</strong></td>
                    <td>Distributes data from one root process to all processes.</td>
                    <td><strong>IN</strong>: <code>inbuf</code> (address of input buffer) <br /> <strong>IN</strong>: <code>incnt</code> (number of elements sent to each process) <br /> <strong>IN</strong>: <code>intype</code> (datatype of input buffer elements) <br /> <strong>IN</strong>: <code>outcnt</code> (number of elements received from each process) <br /> <strong>IN</strong>: <code>outtype</code> (datatype of output buffer elements) <br /> <strong>IN</strong>: <code>root</code> (process ID of root process) <br /> <strong>IN</strong>: <code>comm</code> (communicator handle)</td>
                    <td><strong>OUT</strong>: <code>outbuf</code> (address of output buffer)</td>
                </tr>
                <tr>
                    <td><strong>MPI_REDUCE</strong></td>
                    <td>Combines values from all processes using a specified operation.</td>
                    <td><strong>IN</strong>: <code>inbuf</code> (address of input buffer) <br /> <strong>IN</strong>: <code>count</code> (number of elements in input buffer) <br /> <strong>IN</strong>: <code>type</code> (datatype of input buffer elements) <br /> <strong>IN</strong>: <code>op</code> (reduction operation) <br /> <strong>IN</strong>: <code>root</code> (process ID of root process) <br /> <strong>IN</strong>: <code>comm</code> (communicator handle)</td>
                    <td><strong>OUT</strong>: <code>outbuf</code> (address of output buffer)</td>
                </tr>
                <tr>
                    <td><strong>MPI_ALLREDUCE</strong></td>
                    <td>Similar to MPI_REDUCE but returns the result to all processes.</td>
                    <td><strong>IN</strong>: <code>inbuf</code> (address of input buffer) <br /> <strong>IN</strong>: <code>count</code> (number of elements in input buffer) <br /> <strong>IN</strong>: <code>type</code> (datatype of input buffer elements) <br /> <strong>IN</strong>: <code>op</code> (reduction operation) <br /> <strong>IN</strong>: <code>comm</code> (communicator handle)</td>
                    <td><strong>OUT</strong>: <code>outbuf</code> (address of output buffer)</td>
                </tr>
            </table>
            </p>
            <h4 id="complete-example-program">Complete Example Program</h4>
            <p>
            <div>
                <pre><code class="language-clike">main(int argc, char *argv[]) {
    MPI_Comm com = MPI_COMM_WORLD;
    MPI_Init(&amp;argc, &amp;argv);
    MPI_Comm_size(com, &amp;np);
    MPI_Comm_rank(com, &amp;me);
    
    if (me == 0) {  /* Read problem size at process 0 */
        read_problem_size(&amp;size);
        buff[0] = size;
    }
    /* Global broadcast propagates this data to all processes */
    MPI_Bcast(buff, 1, MPI_INT, 0, com);
    
    /* Extract problem size from buff; allocate space for local data */
    lsize = buff[0] / np;
    local = malloc(lsize + 2);
    
    /* Read input data at process 0; then distribute to processes */
    if (me == 0) { 
        work = malloc(size); 
        read_array(work); 
    }
    MPI_Scatter(work, lsize, MPI_FLOAT, local + 1, lsize, MPI_FLOAT, 0, com);
    
    lnbr = (me + np - 1) % np;  /* Determine my neighbors in ring */
    rnbr = (me + 1) % np;
    globalerr = 99999.0;
    
    while (globalerr &gt; 0.1) {  /* Repeat until termination */
        /* Exchange boundary values with neighbors */
        ls = local + lsize;
        MPI_Send(local + 1, 1, MPI_FLOAT, lnbr, 10, com);
        MPI_Recv(local + 1, 1, MPI_FLOAT, rnbr, 10, com, &amp;status);
        MPI_Send(ls - 2, 1, MPI_FLOAT, rnbr, 20, com);
        MPI_Recv(ls - 1, 1, MPI_FLOAT, lnbr, 20, com, &amp;status);
        compute(local);
        localerr = maxerror(local);  /* Determine local error */
        
        /* Find maximum local error, and replicate in each process */
        MPI_Allreduce(&amp;localerr, &amp;globalerr, 1, MPI_FLOAT, MPI_MAX, com);
    }
    
    /* Collect results at process 0 */
    MPI_Gather(local, lsize, MPI_FLOAT, work, lsize, MPI_FLOAT, 0, com);
    if (me == 0) { 
        write_array(work); 
        free(work); 
    }
    MPI_Finalize();
}</code></pre>
            </div>
            </p>
            <h3 id="finite-difference-problem-global-operations">Finite Difference Problem: Global Operations</h3>
            <p>The following example demonstrates a finite difference problem using global operations. The algorithm requires both nearest-neighbor communication and global communication to detect termination. This example uses MPI_SEND and MPI_RECV for nearest-neighbor communication and four MPI global communication routines.</p>
            <h4 id="communication-operations">Communication Operations</h4>
            <ol>
                <li><strong>MPI_BCAST</strong>: Broadcasts the problem size parameter (<code>size</code>) from process 0 to all processes.</li>
                <li><strong>MPI_SCATTER</strong>: Distributes an input array (<code>work</code>) from process 0 to other processes, so each process receives <code>size/np</code> elements.</li>
                <li><strong>MPI_SEND and MPI_RECV</strong>: Exchanges data (a single floating-point number) with neighbors.</li>
                <li><strong>MPI_ALLREDUCE</strong>: Determines the maximum of a set of <code>localerr</code> values computed at different processes and distributes this maximum value to each process.</li>
                <li><strong>MPI_GATHER</strong>: Accumulates an output array at process 0.</li>
            </ol>
            <h4 id="mpi-implementation">MPI Implementation</h4>
            <p>The following MPI implementation illustrates the use of these communication operations:</p>
            <p>
            <div>
                <pre><code class="language-clike">#include "mpi.h"
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

void read_problem_size(int *size) {
    // Function to read problem size
}

void read_array(float *array) {
    // Function to read input data
}

void write_array(float *array) {
    // Function to write output data
}

void compute(float *local) {
    // Function to perform computation on local data
}

float maxerror(float *local) {
    // Function to compute maximum local error
    return 0.0;
}

int main(int argc, char *argv[]) {
    MPI_Comm com = MPI_COMM_WORLD;
    MPI_Init(&amp;argc, &amp;argv);
    int np, me, size, lsize;
    float buff[1], *work = NULL, *local;
    MPI_Status status;
    int lnbr, rnbr;
    float globalerr, localerr;

    MPI_Comm_size(com, &amp;np);
    MPI_Comm_rank(com, &amp;me);
    
    if (me == 0) {
        read_problem_size(&amp;size);
        buff[0] = size;
    }
    
    MPI_Bcast(buff, 1, MPI_INT, 0, com);
    size = buff[0];
    lsize = size / np;
    local = (float *)malloc((lsize + 2) * sizeof(float));
    
    if (me == 0) {
        work = (float *)malloc(size * sizeof(float));
        read_array(work);
    }
    
    MPI_Scatter(work, lsize, MPI_FLOAT, local + 1, lsize, MPI_FLOAT, 0, com);
    
    lnbr = (me + np - 1) % np;
    rnbr = (me + 1) % np;
    globalerr = 99999.0;
    
    while (globalerr &gt; 0.1) {
        MPI_Send(local + 1, 1, MPI_FLOAT, lnbr, 10, com);
        MPI_Recv(local, 1, MPI_FLOAT, lnbr, 10, com, &amp;status);
        MPI_Send(local + lsize, 1, MPI_FLOAT, rnbr, 20, com);
        MPI_Recv(local + lsize + 1, 1, MPI_FLOAT, rnbr, 20, com, &amp;status);
        
        compute(local);
        localerr = maxerror(local);
        
        MPI_Allreduce(&amp;localerr, &amp;globalerr, 1, MPI_FLOAT, MPI_MAX, com);
    }
    
    MPI_Gather(local + 1, lsize, MPI_FLOAT, work, lsize, MPI_FLOAT, 0, com);
    
    if (me == 0) {
        write_array(work);
        free(work);
    }
    
    free(local);
    MPI_Finalize();
    return 0;
}</code></pre>
            </div>
            </p>
            <h3 id="mpi-modularity">MPI Modularity</h3>
            <p>In parallel programming, modular construction involves combining program components in various ways: sequentially, in parallel, or concurrently. MPI supports modular programming via its communicator mechanism, allowing for the specification of program components that encapsulate internal communication operations and provide a local name space for processes.</p>
            <h4 id="communicators-in-mpi">Communicators in MPI</h4>
            <p>Communicators in MPI specify the process group and context for communication operations, enabling modular program design. Here are key functions for creating and managing communicators:</p>
            <p>
            <table>
                <tr>
                    <td><strong>Function</strong></td>
                    <td><strong>Purpose</strong></td>
                    <td><strong>Parameters</strong></td>
                    <td><strong>Returns</strong></td>
                </tr>
                <tr>
                    <td><strong>MPI_COMM_DUP</strong></td>
                    <td>Creates a new communicator with the same process group but a new context.</td>
                    <td><strong>IN</strong>: <code>comm</code> (communicator handle)</td>
                    <td><strong>OUT</strong>: <code>newcomm</code> (new communicator handle)</td>
                </tr>
                <tr>
                    <td><strong>MPI_COMM_SPLIT</strong></td>
                    <td>Partitions a group into disjoint subgroups.</td>
                    <td><strong>IN</strong>: <code>comm</code> (communicator handle) <br /> <strong>IN</strong>: <code>color</code> (subgroup control, integer) <br /> <strong>IN</strong>: <code>key</code> (process ID control, integer)</td>
                    <td><strong>OUT</strong>: <code>newcomm</code> (new communicator handle)</td>
                </tr>
                <tr>
                    <td><strong>MPI_INTERCOMM_CREATE</strong></td>
                    <td>Creates an intercommunicator linking processes in two groups.</td>
                    <td><strong>IN</strong>: <code>comm</code> (local intracommunicator handle) <br /> <strong>IN</strong>: <code>leader</code> (local leader, integer) <br /> <strong>IN</strong>: <code>peer</code> (peer intracommunicator handle) <br /> <strong>IN</strong>: <code>rleader</code> (process ID of remote leader in <code>peer</code>, integer) <br /> <strong>IN</strong>: <code>tag</code> (tag for communicator setup, integer)</td>
                    <td><strong>OUT</strong>: <code>inter</code> (new intercommunicator handle)</td>
                </tr>
                <tr>
                    <td><strong>MPI_COMM_FREE</strong></td>
                    <td>Destroys a communicator.</td>
                    <td><strong>IN</strong>: <code>comm</code> (communicator handle)</td>
                    <td>None</td>
                </tr>
            </table>
            </p>
            <h4 id="creating-communicators">Creating Communicators</h4>
            <h5>Using MPI_COMM_DUP for Sequential Composition</h5>
            <p>Sequential composition can cause issues if different components use the same message tags. Using different contexts via MPI_COMM_DUP avoids this problem.</p>
            <p>Example:</p>
            <p>
            <div>
                <pre><code class="language-fortran">integer comm, newcomm, ierr
call MPI_COMM_DUP(comm, newcomm, ierr)
call transpose(newcomm, A)
call MPI_COMM_FREE(newcomm, ierr)</code></pre>
            </div>
            </p>
            <h5>Using MPI_COMM_SPLIT for Parallel Composition</h5>
            <p>MPI_COMM_SPLIT partitions processes into subgroups, allowing parallel execution without conflicts.</p>
            <p>Example:</p>
            <p>CODE_BLOCK_PLACEHOLDER
                This splits the communicator into subgroups based on <code>myid % 3</code>.</p>
            <h3 id="communicating-between-groups">Communicating between Groups</h3>
            <p>An intercommunicator created by MPI_INTERCOMM_CREATE links two groups, allowing intergroup communication.</p>
            <p>Example:</p>
            <p>
            <div>
                <pre><code class="language-clike">MPI_Comm comm, newcomm;
int myid, color;
MPI_Comm_rank(comm, &amp;myid);
color = myid % 3;
MPI_Comm_split(comm, color, myid, &amp;newcomm);</code></pre>
            </div>
            </p>
            <h4 id="example-program-intercommunicator">Example Program: Intercommunicator</h4>
            <p>The following program splits processes into two groups and creates an intercommunicator between them.</p>
            <p>
            <div>
                <pre><code class="language-clike">MPI_Comm comm, newcomm, intercomm;
int myid, new_id, np;
MPI_Comm_rank(MPI_COMM_WORLD, &amp;myid);
MPI_Comm_size(MPI_COMM_WORLD, &amp;np);
int color = (myid % 2 == 0) ? 0 : 1;
MPI_Comm_split(MPI_COMM_WORLD, color, myid, &amp;newcomm);
int local_leader = (myid % 2 == 0) ? 0 : 1;
int remote_leader = 1 - local_leader;
MPI_Intercomm_create(newcomm, local_leader, MPI_COMM_WORLD, remote_leader, 0, &amp;intercomm);</code></pre>
            </div>
            </p>
            <h3 id="mpi-derived-datatypes">MPI Derived Datatypes</h3>
            <p>In MPI, derived datatypes allow grouping noncontiguous data elements into a single message, avoiding the need for data copy operations. This mechanism is crucial for efficiently communicating complex data structures, such as rows of a 2D array stored by columns.</p>
            <h4 id="key-functions-for-derived-datatypes">Key Functions for Derived Datatypes</h4>
            <p>
            <table>
                <tr>
                    <td><strong>Function</strong></td>
                    <td><strong>Purpose</strong></td>
                    <td><strong>Parameters</strong></td>
                    <td><strong>Returns</strong></td>
                </tr>
                <tr>
                    <td><strong>MPI_TYPE_CONTIGUOUS</strong></td>
                    <td>Creates a derived datatype from contiguous elements.</td>
                    <td><strong>IN</strong>: <code>count</code> (number of elements, integer â‰¥ 0) <br /> <strong>IN</strong>: <code>oldtype</code> (input datatype handle)</td>
                    <td><strong>OUT</strong>: <code>newtype</code> (output datatype handle)</td>
                </tr>
                <tr>
                    <td><strong>MPI_TYPE_VECTOR</strong></td>
                    <td>Creates a derived datatype from blocks separated by a stride.</td>
                    <td><strong>IN</strong>: <code>count</code> (number of elements, integer â‰¥ 0) <br /> <strong>IN</strong>: <code>blocklen</code> (elements in a block, integer â‰¥ 0) <br /> <strong>IN</strong>: <code>stride</code> (elements between start of each block, integer) <br /> <strong>IN</strong>: <code>oldtype</code> (input datatype handle)</td>
                    <td><strong>OUT</strong>: <code>newtype</code> (output datatype handle)</td>
                </tr>
                <tr>
                    <td><strong>MPI_TYPE_INDEXED</strong></td>
                    <td>Creates a derived datatype with variable indices and sizes.</td>
                    <td><strong>IN</strong>: <code>count</code> (number of blocks, integer â‰¥ 0) <br /> <strong>IN</strong>: <code>blocklens</code> (elements in each block, array of integer â‰¥ 0) <br /> <strong>IN</strong>: <code>indices</code> (displacements for each block, array of integer) <br /> <strong>IN</strong>: <code>oldtype</code> (input datatype handle)</td>
                    <td><strong>OUT</strong>: <code>newtype</code> (output datatype handle)</td>
                </tr>
                <tr>
                    <td><strong>MPI_TYPE_COMMIT</strong></td>
                    <td>Commits a datatype so that it can be used in communication.</td>
                    <td><strong>INOUT</strong>: <code>type</code> (datatype to be committed handle)</td>
                    <td>None</td>
                </tr>
                <tr>
                    <td><strong>MPI_TYPE_FREE</strong></td>
                    <td>Frees a derived datatype.</td>
                    <td><strong>INOUT</strong>: <code>type</code> (datatype to be freed handle)</td>
                    <td>None</td>
                </tr>
            </table>
            </p>
            <h4 id="example-program-contiguous-derived-type">Example Program: Contiguous Derived Type</h4>
            <p>Example of defining a contiguous derived type:</p>
            <p>CODE_BLOCK_PLACEHOLDER
                This is equivalent to:</p>
            <p>
            <div>
                <pre><code class="language-clike">#include "mpi.h"
#include &lt;stdio.h&gt;

int main(int argc, char *argv[]) {
    MPI_Init(&amp;argc, &amp;argv);

    MPI_Comm comm = MPI_COMM_WORLD, newcomm, intercomm;
    int myid, np, color, new_id;
    
    MPI_Comm_rank(comm, &amp;myid);
    MPI_Comm_size(comm, &amp;np);
    
    color = (myid % 2 == 0) ? 0 : 1;
    MPI_Comm_split(comm, color, myid, &amp;newcomm);
    
    int local_leader = 0;
    int remote_leader = (color == 0) ? 1 : 0;
    
    MPI_Intercomm_create(newcomm, local_leader, comm, remote_leader, 0, &amp;intercomm);
    
    if (color == 0) {
        MPI_Send(&amp;myid, 1, MPI_INT, 0, 0, intercomm);
    } else {
        int received;
        MPI_Recv(&amp;received, 1, MPI_INT, 0, 0, intercomm, MPI_STATUS_IGNORE);
        printf("Process %d received message from process %d\n", myid, received);
    }
    
    MPI_Comm_free(&amp;newcomm);
    MPI_Comm_free(&amp;intercomm);
    MPI_Finalize();
    return 0;
}</code></pre>
            </div>
            </p>
            <h4 id="example-program-vector-derived-type">Example Program: Vector Derived Type</h4>
            <p>Example of defining a vector derived type:</p>
            <p>
            <div>
                <pre><code class="language-fortran">integer ierr
call MPI_TYPE_CONTIGUOUS(10, MPI_REAL, tenrealtype, ierr)
call MPI_TYPE_COMMIT(tenrealtype, ierr)
call MPI_SEND(data, 1, tenrealtype, dest, tag, MPI_COMM_WORLD, ierr)
call MPI_TYPE_FREE(tenrealtype, ierr)</code></pre>
            </div>
            </p>
            <p>This is equivalent to:</p>
            <p>
            <div>
                <pre><code class="language-fortran">call MPI_SEND(data, 10, MPI_REAL, dest, tag, MPI_COMM_WORLD, ierr)</code></pre>
            </div>
            </p>
            <h4 id="example-program-finite-difference-stencil">Example Program: Finite Difference Stencil</h4>
            <p>This program uses derived types to communicate the north and south rows and the west and east columns of a Fortran array.</p>
            <p>
            <div>
                <pre><code class="language-clike">float data[1024];
MPI_Datatype floattype;
MPI_Type_vector(10, 1, 32, MPI_FLOAT, &amp;floattype);
MPI_Type_commit(&amp;floattype);
MPI_Send(data, 1, floattype, dest, tag, MPI_COMM_WORLD);
MPI_Type_free(&amp;floattype);</code></pre>
            </div>
            </p>
            <h4 id="example-program-fock-matrix-problem">Example Program: Fock Matrix Problem</h4>
            <p>This example uses MPI_TYPE_INDEXED to send noncontiguous blocks of data.</p>
            <p>
            <div>
                <pre><code class="language-clike">float data[1024], buff[10];
for (int i = 0; i &lt; 10; i++) buff[i] = data[i * 32];
MPI_Send(buff, 10, MPI_FLOAT, dest, tag, MPI_COMM_WORLD);</code></pre>
            </div>
            </p>
            <h3 id="asynchronous-communication">Asynchronous Communication</h3>
            <p>Asynchronous communication is essential when a computation must access elements of a shared data structure in an unstructured manner. MPI provides functions like MPI_IPROBE, MPI_PROBE, and MPI_GET_COUNT to facilitate asynchronous communication.</p>
            <p>
            <table>
                <tr>
                    <td><strong>Function</strong></td>
                    <td><strong>Purpose</strong></td>
                    <td><strong>Parameters</strong></td>
                    <td><strong>Returns</strong></td>
                </tr>
                <tr>
                    <td><strong>MPI_IPROBE</strong></td>
                    <td>Polls for a pending message without receiving it.</td>
                    <td><strong>IN</strong>: <code>source</code> (ID of source process, or <code>MPI_ANY_SOURCE</code>) <br /> <strong>IN</strong>: <code>tag</code> (Message tag, or <code>MPI_ANY_TAG</code>) <br /> <strong>IN</strong>: <code>comm</code> (communicator handle)</td>
                    <td><strong>OUT</strong>: <code>flag</code> (Boolean indicating if a message is available) <br /> <strong>OUT</strong>: <code>status</code> (Status object)</td>
                </tr>
                <tr>
                    <td><strong>MPI_PROBE</strong></td>
                    <td>Blocks until a message is pending, then returns and sets its status argument.</td>
                    <td><strong>IN</strong>: <code>source</code> (ID of source process, or <code>MPI_ANY_SOURCE</code>) <br /> <strong>IN</strong>: <code>tag</code> (Message tag, or <code>MPI_ANY_TAG</code>) <br /> <strong>IN</strong>: <code>comm</code> (communicator handle)</td>
                    <td><strong>OUT</strong>: <code>status</code> (Status object)</td>
                </tr>
                <tr>
                    <td><strong>MPI_GET_COUNT</strong></td>
                    <td>Determines the size of a message.</td>
                    <td><strong>IN</strong>: <code>status</code> (Status variable from receive) <br /> <strong>IN</strong>: <code>datatype</code> (Datatype of receive buffer elements)</td>
                    <td><strong>OUT</strong>: <code>count</code> (Number of data elements in message)</td>
                </tr>
            </table>
            </p>
            <h4 id="example-program-using-asynchronous-communication">Example Program: Using Asynchronous Communication</h4>
            <p>The following code demonstrates how to use MPI_IPROBE, MPI_PROBE, and MPI_GET_COUNT to handle messages from an unknown source containing an unknown number of integers.</p>
            <p>
            <div>
                <pre><code class="language-fortran">integer, parameter :: n = 10, m = 10
real :: array(n, m)
integer :: ierr, rowtype, coltype

! Define row type
call MPI_TYPE_VECTOR(m, 1, n, MPI_REAL, rowtype, ierr)
call MPI_TYPE_COMMIT(rowtype, ierr)

! Define column type
call MPI_TYPE_CONTIGUOUS(n, MPI_REAL, coltype, ierr)
call MPI_TYPE_COMMIT(coltype, ierr)

! Use derived types to send/receive rows and columns
call MPI_SEND(array(1, 1), 1, rowtype, dest, tag, MPI_COMM_WORLD, ierr)
call MPI_RECV(array(1, 1), 1, coltype, source, tag, MPI_COMM_WORLD, ierr)

! Free derived types
call MPI_TYPE_FREE(rowtype, ierr)
call MPI_TYPE_FREE(coltype, ierr)</code></pre>
            </div>
            </p>
            <h4 id="example-program-fock-matrix-construction-with-polling">Example Program: Fock Matrix Construction with Polling</h4>
            <p>This example integrates data and computation tasks into a single process, alternating between checking for pending data requests and performing computation.</p>
            <p>
            <div>
                <pre><code class="language-fortran">integer, parameter :: len = 100
integer :: focktype, ierr, inbuf(len), source

! Define indexed type
call MPI_TYPE_INDEXED(len / 2, inbuf(len / 2 + 1), inbuf(1), MPI_INTEGER, focktype, ierr)
call MPI_TYPE_COMMIT(focktype, ierr)

! Use indexed type to send data
call MPI_SEND(data, 1, focktype, source, MPI_COMM_WORLD, ierr)

! Free indexed type
call MPI_TYPE_FREE(focktype, ierr)</code></pre>
            </div>
            </p>
            <h3 id="best-practices">Best Practices</h3>
            <p>When using MPI, consider the following best practices:</p>
            <ol>
                <li>Minimize communication overhead: Limit the number and size of messages to reduce the time spent on communication.</li>
                <li>Use non-blocking communication when possible: This allows the program to continue executing while messages are being sent or received.</li>
                <li>Use collective communication operations for efficient communication among multiple processes.</li>
                <li>Design your program to scale well with an increasing number of processes, ensuring that each process has enough work to do without waiting for others.</li>
                <li>Implement load balancing to distribute work evenly among processes, avoiding situations where some processes are idle while others are overloaded.</li>
            </ol>
            <h3 id="examples">Examples</h3>
            <h4 id="c-c-mpi">C/C++ MPI</h4>
            <ol>
                <li>Install an MPI implementation, such as OpenMPI or MPICH.</li>
                <li>Compile your C/C++ MPI program using the provided wrapper scripts:</li>
                <li>For C: <code>mpicc mpi_program.c -o mpi_program</code></li>
                <li>For C++: <code>mpiCC mpi_program.cpp -o mpi_program</code></li>
                <li>Run your MPI program using the provided <code>mpiexec</code> or <code>mpirun</code> command:</li>
                <li><code>mpiexec -n &lt;number_of_processes&gt; ./mpi_program</code></li>
                <li><code>mpirun -n &lt;number_of_processes&gt; ./mpi_program</code></li>
            </ol>
            <p>The following C/C++ examples demonstrate different aspects of MPI:</p>
            <ul>
                <li><a href="https://github.com/wesleykendall/mpitutorial/blob/gh-pages/tutorials/mpi-hello-world/code/mpi_hello_world.c">MPI Hello World</a></li>
                <li><a href="https://github.com/wesleykendall/mpitutorial/blob/gh-pages/tutorials/mpi-send-and-receive/code/send_recv.c">MPI Send and Receive</a></li>
                <li><a href="https://github.com/wesleykendall/mpitutorial/blob/gh-pages/tutorials/mpi-broadcast/code/bcast.c">MPI Broadcast</a></li>
                <li><a href="https://github.com/wesleykendall/mpitutorial/blob/gh-pages/tutorials/mpi-scatter-gather-and-allgather/code/scatter_gather.c">MPI Scatter and Gather</a></li>
            </ul>
            <h4 id="python-mpi">Python MPI</h4>
            <ol>
                <li>Install the <code>mpi4py</code> library and an MPI implementation, such as OpenMPI or MPICH.</li>
                <li>Run your Python MPI program using the provided <code>mpiexec</code> or <code>mpirun</code> command:</li>
                <li><code>mpiexec -n &lt;number_of_processes&gt; python mpi_program.py</code></li>
                <li><code>mpirun -n &lt;number_of_processes&gt; python mpi_program.py</code></li>
            </ol>
            <p>The following Python examples demonstrate different aspects of MPI:</p>
            <ul>
                <li><a href="https://mpi4py.readthedocs.io/en/stable/tutorial.html#mpi-hello-world">MPI Hello World</a></li>
                <li><a href="https://mpi4py.readthedocs.io/en/stable/tutorial.html#point-to-point-communication">MPI Send and Receive</a></li>
                <li><a href="https://mpi4py.readthedocs.io/en/stable/tutorial.html#collective-communication">MPI Broadcast</a></li>
                <li><a href="https://mpi4py.readthedocs.io/en/stable/tutorial.html#scatter-and-gather">MPI Scatter and Gather</a></li>
            </ul>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol><a href="#message-passing-interface-mpi-">Message Passing Interface (MPI)</a>
                <ol>
                    <li><a href="#key-concepts-of-mpi">Key Concepts of MPI</a></li>
                    <li><a href="#mpi-programming-model">MPI Programming Model</a></li>
                    <li><a href="#implementing-parallel-algorithms-with-mpi">Implementing Parallel Algorithms with MPI</a></li>
                    <li><a href="#mpi-basics">MPI Basics</a>
                        <ol>
                            <li><a href="#introduction-to-mpi-communicators">Introduction to MPI Communicators</a></li>
                            <li><a href="#mpi-initialization-and-finalization">MPI Initialization and Finalization</a></li>
                            <li><a href="#determining-process-information">Determining Process Information</a></li>
                            <li><a href="#example-program">Example Program</a></li>
                            <li><a href="#execution-and-output">Execution and Output</a></li>
                            <li><a href="#sending-and-receiving-messages">Sending and Receiving Messages</a></li>
                            <li><a href="#example-program-with-mpi_send-and-mpi_recv">Example Program with MPI_SEND and MPI_RECV</a></li>
                        </ol>
                    </li>
                    <li><a href="#language-bindings">Language Bindings</a>
                        <ol>
                            <li><a href="#c-language-binding">C Language Binding</a></li>
                            <li><a href="#fortran-language-binding">Fortran Language Binding</a></li>
                        </ol>
                    </li>
                    <li><a href="#example-pairwise-interactions-algorithm">Example: Pairwise Interactions Algorithm</a>
                        <ol>
                            <li><a href="#c-version">C Version</a></li>
                            <li><a href="#fortran-version">Fortran Version</a></li>
                        </ol>
                    </li>
                    <li><a href="#determinism">Determinism</a>
                        <ol>
                            <li><a href="#introduction-to-determinism">Introduction to Determinism</a></li>
                            <li><a href="#ensuring-determinism">Ensuring Determinism</a></li>
                        </ol>
                    </li>
                    <li><a href="#example-nondeterministic-program">Example: Nondeterministic Program</a>
                        <ol>
                            <li><a href="#c-version">C Version</a></li>
                        </ol>
                    </li>
                    <li><a href="#mpi-global-operations">MPI Global Operations</a>
                        <ol>
                            <li><a href="#key-mpi-collective-communication-functions">Key MPI Collective Communication Functions</a></li>
                            <li><a href="#complete-example-program">Complete Example Program</a></li>
                        </ol>
                    </li>
                    <li><a href="#finite-difference-problem-global-operations">Finite Difference Problem: Global Operations</a>
                        <ol>
                            <li><a href="#communication-operations">Communication Operations</a></li>
                            <li><a href="#mpi-implementation">MPI Implementation</a></li>
                        </ol>
                    </li>
                    <li><a href="#mpi-modularity">MPI Modularity</a>
                        <ol>
                            <li><a href="#communicators-in-mpi">Communicators in MPI</a></li>
                            <li><a href="#creating-communicators">Creating Communicators</a></li>
                        </ol>
                    </li>
                    <li><a href="#communicating-between-groups">Communicating between Groups</a>
                        <ol>
                            <li><a href="#example-program-intercommunicator">Example Program: Intercommunicator</a></li>
                        </ol>
                    </li>
                    <li><a href="#mpi-derived-datatypes">MPI Derived Datatypes</a>
                        <ol>
                            <li><a href="#key-functions-for-derived-datatypes">Key Functions for Derived Datatypes</a></li>
                            <li><a href="#example-program-contiguous-derived-type">Example Program: Contiguous Derived Type</a></li>
                            <li><a href="#example-program-vector-derived-type">Example Program: Vector Derived Type</a></li>
                            <li><a href="#example-program-finite-difference-stencil">Example Program: Finite Difference Stencil</a></li>
                            <li><a href="#example-program-fock-matrix-problem">Example Program: Fock Matrix Problem</a></li>
                        </ol>
                    </li>
                    <li><a href="#asynchronous-communication">Asynchronous Communication</a>
                        <ol>
                            <li><a href="#example-program-using-asynchronous-communication">Example Program: Using Asynchronous Communication</a></li>
                            <li><a href="#example-program-fock-matrix-construction-with-polling">Example Program: Fock Matrix Construction with Polling</a></li>
                        </ol>
                    </li>
                    <li><a href="#best-practices">Best Practices</a></li>
                    <li><a href="#examples">Examples</a>
                        <ol>
                            <li><a href="#c-c-mpi">C/C++ MPI</a></li>
                            <li><a href="#python-mpi">Python MPI</a></li>
                        </ol>
                    </li>
                </ol>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/parallel_and_concurrent_programming/01_basic_terminology.html">Basic Terminology</a></li>
                    <li><a href="https://adamdjellouli.com/articles/parallel_and_concurrent_programming/02_multithreading.html">Multithreading</a></li>
                    <li><a href="https://adamdjellouli.com/articles/parallel_and_concurrent_programming/03_multiprocessing.html">Multiprocessing</a></li>
                    <li><a href="https://adamdjellouli.com/articles/parallel_and_concurrent_programming/04_asynchronous_programming.html">Asynchronous Programming</a></li>
                    <li><a href="https://adamdjellouli.com/articles/parallel_and_concurrent_programming/05_mpi.html">Mpi</a></li>
                    <li><a href="https://adamdjellouli.com/articles/parallel_and_concurrent_programming/06_hardware.html">Hardware</a></li>
                    <li><a href="https://adamdjellouli.com/articles/parallel_and_concurrent_programming/07_evaluating_performance.html">Evaluating Performance</a></li>
                    <li><a href="https://adamdjellouli.com/articles/parallel_and_concurrent_programming/08_designing_parallel_programs.html">Designing Parallel Programs</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>