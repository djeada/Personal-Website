<!DOCTYPE html>

<html lang="en">
<head>
<script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
<meta charset="utf-8"/>
<title>Least Squares Regression</title>
<meta content="Least Squares Regression is a fundamental technique in statistical modeling and data analysis used for fitting a model to observed data." name="description"/>
<meta content="Adam Djellouli" name="author"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet"/>
<link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon"/>
<link href="../../../resources/style.css" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>
<body><nav aria-label="Main navigation">
<a class="logo" href="https://adamdjellouli.com">
<img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG"/>
</a>
<input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox"/>
<ul aria-labelledby="navbar-toggle" role="menu">
<li role="menuitem">
<a href="../../../index.html" title="Go to Home Page"> Home </a>
</li>
<li role="menuitem">
<a class="active" href="../../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
</li>
<li role="menuitem">
<a href="../../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
</li>
<li role="menuitem">
<a href="../../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
</li>
<li role="menuitem">
<a href="../../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
</li>
<li>
<script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
<div class="gcse-search"></div>
</li>
<li>
<button aria-label="Toggle dark mode" id="dark-mode-button"></button>
</li>
</ul>
</nav>
<div id="article-wrapper"><article-section id="article-body">
<p style="text-align: right;"><i>Last modified: December 25, 2025</i></p>
<p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
<h2 id="least-squares-regression">Least Squares Regression</h2>
<p>Least Squares Regression is a fundamental technique in statistical modeling and data analysis used for fitting a model to observed data. The primary goal is to find a set of parameters that minimize the discrepancies (residuals) between the modelâ€™s predictions and the actual observed data. The "least squares" criterion is chosen because it leads to convenient mathematical properties and closed-form solutions, particularly for linear models.</p>
<p>In its simplest form, least squares regression is applied to <strong>linear regression</strong>, where we assume a linear relationship between a set of input variables (features) $X$ and an output variable $Y$. More generally, it can be extended to polynomial regression and multiple linear regression with multiple input variables. Because of its simplicity, transparency, and relative mathematical convenience, least squares remains one of the most widely used techniques in data analysis.</p>
<p><img alt="output(31)" src="https://github.com/user-attachments/assets/3777f998-f72c-43a1-a8a9-8ecad0e82b1f"/></p>
<h3 id="mathematical-formulation">Mathematical Formulation</h3>
<p>Given a matrix of features $X \in \mathbb{R}^{m \times n}$ (with $m$ observations and $n$ features), and a vector of target variables $Y \in \mathbb{R}^m$, we seek a coefficient vector $\beta \in \mathbb{R}^n$ that best fits the data in the sense of minimizing the sum of squared residuals. If we include a column of ones in $X$ to represent the intercept term, $\beta$ naturally includes the intercept as well.</p>
<p>We model:</p>
<p>$$\hat{Y} = X \beta$$</p>
<p><strong>Objective</strong>: Minimize the Residual Sum of Squares (RSS):</p>
<p>$$RSS(\beta) = \| Y - X\beta \|_2^2 = (Y - X\beta)^\top (Y - X\beta)$$</p>
<p>The goal is to find $\beta$ that solves:</p>
<p>$$\min_\beta \| Y - X\beta \|_2^2$$</p>
<p>By setting the gradient of this objective with respect to $\beta$ to zero, we obtain the <strong>Normal Equation</strong>:</p>
<p>$$X^\top X \beta = X^\top Y$$</p>
<p>Provided $X^\top X$ is invertible, we have a closed-form solution:</p>
<p>$$\beta = (X^\top X)^{-1} X^\top Y$$</p>
<p>This $\beta$ is the least squares estimate of the coefficient vector, ensuring that the fitted line (or hyperplane, in the multi-dimensional case) is the best fit in the least squares sense.</p>
<h3 id="derivation">Derivation</h3>
<p>I. <strong>Set up the Problem</strong>:</p>
<p>Suppose we have observations ${ (x_i, y_i) }_{i=1}^m$, where $x_i \in \mathbb{R}^n$ is a vector of features for the $i$-th observation and $y_i$ is the response. We assume a linear model:</p>
<p>$$\hat{y}<em>i = \sum</em>{j=1}^n \beta_j x_{ij} = x_i^\top \beta,$$
or in matrix form:</p>
<p>$$\hat{Y} = X\beta,$$
where $X$ is the $m \times n$ matrix with rows $x_i^\top$, and $Y \in \mathbb{R}^m$ is the vector of observed responses.</p>
<p>II. <strong>Defining the Error to Minimize</strong>:</p>
<p>We define the residuals as:</p>
<p>$$r = Y - X\beta$$</p>
<p>The objective is to minimize:</p>
<p>$$RSS(\beta) = r^\top r = (Y - X\beta)^\top (Y - X\beta)$$</p>
<p>III. <strong>Finding the Minimum</strong>:</p>
<p>To minimize with respect to $\beta$, take the gradient and set it to zero:</p>
<p>$$\frac{\partial RSS}{\partial \beta} = -2X^\top(Y - X\beta) = 0$$</p>
<p>This implies:</p>
<p>$$X^\top Y - X^\top X \beta = 0 \implies X^\top X \beta = X^\top Y$$</p>
<p>IV. <strong>Solving the Normal Equation</strong>:</p>
<p>If $X^\top X$ is invertible:</p>
<p>$$\beta = (X^\top X)^{-1} X^\top Y$$</p>
<p>This formula provides a closed-form solution for the ordinary least squares estimator $\beta$.</p>
<h3 id="algorithm-steps">Algorithm Steps</h3>
<p>I. <strong>Data Preparation</strong>:</p>
<ul>
<li>Construct your design matrix $X$ by stacking the observations row-wise.  </li>
<li>Each row corresponds to one observation and each column corresponds to one feature.  </li>
<li>Often, a column of ones is added to incorporate the intercept term.</li>
<li>Construct the response vector $Y$ from the observed target values.</li>
</ul>
<p>II. <strong>Compute Matrices</strong>:</p>
<p>Compute $X^\top X$ and $X^\top Y$.</p>
<p>III. <strong>Check Invertibility</strong>:</p>
<ul>
<li>Ensure $X^\top X$ is invertible (or use a pseudo-inverse if not).</li>
<li>If $X^\top X$ is not invertible, it may be due to multicollinearity. Consider removing or combining features, or use regularization methods (like Ridge or Lasso).</li>
</ul>
<p>IV. <strong>Solve for $\beta$</strong>:</p>
<p>$$\beta = (X^\top X)^{-1} X^\top Y$$</p>
<p>V. <strong>Use the Model for Prediction</strong>:</p>
<p>For a new input $x_{\text{new}}$, predict:</p>
<p>$$\hat{y}<em>{\text{new}} = x</em>{\text{new}}^\top \beta$$</p>
<h3 id="example">Example</h3>
<p><strong>Given Data Points</strong>: $(x,y)$ = $(1,1), (2,2), (3,2)$.</p>
<p><strong>Step-by-step</strong>:</p>
<p>I. Add an intercept term:</p>
<p>$$X = \begin{bmatrix}
1 &amp; 1 \\
1 &amp; 2 \\
1 &amp; 3 \\
\end{bmatrix}$$ </p>
<p>$$Y=\begin{bmatrix}1 \ 2 \ 2\end{bmatrix}$$</p>
<p>II. Compute:</p>
<p>$$X^\top X = \begin{bmatrix} 3 &amp; 6 \ 6 &amp;14 \end{bmatrix}$$</p>
<p>$$X^\top Y = \begin{bmatrix} 5 \ 12 \end{bmatrix}$$</p>
<p>III. Invert $X^\top X$:</p>
<p>$$(X^\top X)^{-1} = \begin{bmatrix} 2 &amp; -1 \ -1 &amp; 0.5 \end{bmatrix}$$</p>
<p>IV. Compute $\beta$:</p>
<p>$$\beta = (X^\top X)^{-1}X^\top Y = \begin{bmatrix} 0.5 \ 0.5 \end{bmatrix}$$</p>
<p>Thus, the fitted line is:</p>
<p>$$\hat{y} = 0.5 + 0.5x$$</p>
<h3 id="advantages">Advantages</h3>
<ul>
<li><strong>Closed-Form Solution</strong>: Provides an explicit formula for the optimal parameters, enabling direct interpretation.</li>
<li><strong>Efficient for Small Problems</strong>: Works well with relatively small datasets and few features.</li>
<li><strong>Foundational Method</strong>: Forms the basis for many advanced regression techniques and regularized models.</li>
</ul>
<h3 id="limitations">Limitations</h3>
<ul>
<li><strong>Assumes Linearity</strong>: The method presupposes a linear relationship between features and output.</li>
<li><strong>Sensitive to Outliers</strong>: Squared errors emphasize large errors more heavily, making the model sensitive to outliers.</li>
<li><strong>Invertibility Issues</strong>: If $X^\top X$ is not invertible, the standard formula fails. Issues like multicollinearity require either dropping features, transformations, or using regularized regression variants.</li>
</ul>
</article-section><div id="table-of-contents"><h2>Table of Contents</h2><ol><a href="#least-squares-regression">Least Squares Regression</a><ol><li><a href="#mathematical-formulation">Mathematical Formulation</a></li><li><a href="#derivation">Derivation</a></li><li><a href="#algorithm-steps">Algorithm Steps</a></li><li><a href="#example">Example</a></li><li><a href="#advantages">Advantages</a></li><li><a href="#limitations">Limitations</a></li></ol></ol><div id="related-articles"><h2>Related Articles</h2><ol><li>Root and Extrema Finding<ol><li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/bisection_method.html">Bisection Method</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/golden_ratio_search.html">Golden Ratio Search</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/gradient_descent.html">Gradient Descent</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/newtons_method.html">Newtons Method</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/relaxation_method.html">Relaxation Method</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/root_finding.html">Root Finding</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/secant_method.html">Secant Method</a></li></ol></li><li>Systems of Equations<ol><li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/gauss_seidel.html">Gauss Seidel</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/gaussian_elimination.html">Gaussian Elimination</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/inverse_matrix.html">Inverse Matrix</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/jacobi_method.html">Jacobi Method</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/lu_decomposition.html">Lu Decomposition</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/systems_of_equations.html">Systems of Equations</a></li></ol></li><li>Differentiation<ol><li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/backward_difference.html">Backward Difference</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/central_difference.html">Central Difference</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/differentiation.html">Differentiation</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/forward_difference.html">Forward Difference</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/taylor_series.html">Taylor Series</a></li></ol></li><li>Integration<ol><li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/integration_introduction.html">Integration Introduction</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/midpoint_rule.html">Midpoint Rule</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/monte_carlo.html">Monte Carlo</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/simpsons_rule.html">Simpsons Rule</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/trapezoidal_rule.html">Trapezoidal Rule</a></li></ol></li><li>Matrices<ol><li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/eigen_value_decomposition.html">Eigen Value Decomposition</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/eigenvalues_and_eigenvectors.html">Eigenvalues and Eigenvectors</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/matrix_methods.html">Matrix Methods</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/power_method.html">Power Method</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/qr_method.html">Qr Method</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/singular_value_decomposition.html">Singular Value Decomposition</a></li></ol></li><li>Regression<ol><li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/cubic_spline_interpolation.html">Cubic Spline Interpolation</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/gaussian_interpolation.html">Gaussian Interpolation</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/interpolation.html">Interpolation</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/lagrange_polynomial_interpolation.html">Lagrange Polynomial Interpolation</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/least_squares.html">Least Squares</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/linear_interpolation.html">Linear Interpolation</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/newton_polynomial.html">Newton Polynomial</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/regression.html">Regression</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/thin_plate_spline_interpolation.html">Thin Plate Spline Interpolation</a></li></ol></li><li>Ordinary Differential Equations<ol><li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/eulers_method.html">Eulers Method</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/heuns_method.html">Heuns Method</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/ordinary_differential_equations.html">Ordinary Differential Equations</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/picards_method.html">Picards Method</a></li><li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/runge_kutta.html">Runge Kutta</a></li></ol></li></ol></div></div></div><footer>
<div class="footer-columns">
<div class="footer-column">
<img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png"/>
</div>
<div class="footer-column">
<h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
<p>Thank you for visiting my personal website. All content here is free to use, but please remember to be respectful and avoid any misuse of the site. If youâ€™d like to get in touch, feel free to reach out via my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or connect with me on <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have technical questions or ideas to share. Wishing you all the best and a fantastic life ahead!</p>
</div>
<div class="footer-column">
<h2>Follow me</h2>
<ul class="social-media">
<li>
<a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
</a>YouTube
                </li>
<li>
<a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
</a>LinkedIn
                </li>
<li>
<a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
</a>Instagram
                </li>
<li>
<a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
</a>Github
                </li>
</ul>
</div>
</div>
<div>
<p id="copyright">
            Â© Adam Djellouli. All rights reserved.
        </p>
</div>
<script>
        document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
    </script>
<script src="../../../app.js"></script>
</footer></body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script></html>