<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Gaussian Interpolation</title>
    <meta content="Gaussian Interpolation, often associated with Gauss‚Äôs forward and backward interpolation formulas, is a technique that refines the approach of polynomial interpolation when data points are equally spaced." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <div class="article-action-buttons"><button class="btn-suggest-edit" title="Suggest Edit">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M4 21h4l11-11-4-4L4 17v4z" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button><button class="btn-create-issue" title="Create Issue">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"></circle>
                        <line stroke="currentColor" stroke-width="2" x1="12" x2="12" y1="8" y2="12"></line>
                        <circle cx="12" cy="16" fill="currentColor" r="1"></circle>
                    </svg>
                </button><button class="btn-download" title="Download">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M12 5v14m0 0l-6-6m6 6l6-6" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button></div>
            <p style="text-align: right;"><i>Last modified: June 01, 2025</i></p>
            <p style="text-align: right;"><i>This article is written in: üá∫üá∏</i></p>
            <h2 id="gaussian-interpolation">Gaussian Interpolation</h2>
            <p><strong>Gaussian Interpolation</strong>, often associated with <strong>Gauss‚Äôs forward and backward interpolation formulas</strong>, is a technique that refines the approach of polynomial interpolation when data points are equally spaced. Instead of using the Newton forward or backward interpolation formulas directly from one end of the data interval, Gaussian interpolation centers the interpolation around a midpoint of the data set. This approach can provide better accuracy when the point at which we need to interpolate lies somewhere in the "interior" of the given data points rather than near the boundaries.</p>
            <p>In essence, Gaussian interpolation is a variant of Newton‚Äôs divided difference interpolation but employs a "central" reference point and finite differences structured around a central node. By choosing a midpoint as a reference and using appropriately shifted indices, Gaussian interpolation formulas often yield more stable and accurate approximations for values near the center of the data set.</p>
            <p>Imagine you have a set of equally spaced points and corresponding function values:</p>
            <p><img alt="output(29)" src="https://github.com/user-attachments/assets/074c2f58-7d0a-44ac-b12f-cb43c9417bfc" /></p>
            <p>Newton‚Äôs forward or backward interpolation builds a polynomial starting from one end (like x_0 or x_n). Gaussian interpolation, however, selects a point near the center of the interval, say x_m (the midpoint), and builds the interpolation polynomial outward from this center. This symmetric approach can lead to a polynomial that better represents the function near that central area, potentially reducing error.</p>
            <h3 id="mathematical-formulation">Mathematical Formulation</h3>
            <p>Assume we have <strong>equally spaced abscissae</strong></p>
            <p>$$
                x_0,x_1,x_2,\dots ,x_n, \qquad
                h = x_{i+1}-x_i (\text{constant step})
                $$</p>
            <p>Let the ‚Äúmid-table‚Äô‚Äô node be $x_m$ where</p>
            <p>$$
                m =
                \begin{cases}
                \dfrac{n}{2}, &amp; n\ \text{even},\\
                \dfrac{n-1}{2}, &amp; n\ \text{odd}
                \end{cases}
                $$</p>
            <p>Shifted argument</p>
            <p>To measure how far the interpolation point $x$ sits from the centre we introduce the dimensionless variable</p>
            <p>$$
                \boxed{t=\dfrac{x-x_m}{h}}
                $$</p>
            <p>Difference operators near the centre</p>
            <p><em>Forward</em> and <em>backward</em> first differences are</p>
            <p>$$
                \Delta y_i = y_{i+1}-y_i
                $$</p>
            <p>$$
                \nabla y_i = y_i-y_{i-1}
                $$</p>
            <p>with higher‚Äêorder differences generated recursively, e.g.</p>
            <p>$$\Delta^2 y_i = \Delta(\Delta y_i)=y_{i+2}-2y_{i+1}+y_i$$</p>
            <p>For a central scheme we evaluate these differences on the rows immediately <strong>adjacent to</strong> $x_m$:</p>
            <p>$$
                \Delta y_{m-1}, \Delta y_m, \Delta^2 y_{m-1}, \nabla y_{m+1},\ldots
                $$</p>
            <p>so that every term in the polynomial is as symmetric about $x_m$ as possible.</p>
            <p>Gauss central formulas</p>
            <p>
            <table>
                <tr>
                    <td>Variant</td>
                    <td>Interpolating polynomial</td>
                </tr>
                <tr>
                    <td><strong>Gauss forward</strong><br />(row just <strong>left</strong> of the centre)</td>
                    <td>$\displaystyle f(x)\approx y_m + t\Delta y_{m-1} + \frac{t(t-1)}{2!}\Delta^{2} y_{m-1} + \frac{t(t+1)(t-1)}{3!}\Delta^{3} y_{m-2} + \cdots$</td>
                </tr>
                <tr>
                    <td><strong>Gauss backward</strong><br />(row just <strong>right</strong> of the centre)</td>
                    <td>$\displaystyle f(x)\approx y_m + t\nabla y_{m+1} + \frac{t(t+1)}{2!}\nabla^{2} y_{m+1} + \frac{t(t+1)(t-1)}{3!}\nabla^{3} y_{m+2} + \cdots$</td>
                </tr>
            </table>
            </p>
            <ul>
                <li>The factorial products $t(t!\pm!1)(t!\mp!1)\ldots$ mirror the binomial coefficients that arise when the Newton forward/backward polynomial is <strong>re-indexed</strong> so that $x_m$ is treated as the origin.</li>
                <li>Each successive term draws its difference from one step farther away, preserving symmetry and minimising round-off error when $x$ lies near the mid-range.</li>
            </ul>
            <p>By recasting Newton‚Äôs formulas about the central node and writing everything in powers of $t$, Gauss‚Äôs forward/backward polynomials provide a more accurate‚Äîand numerically stable‚Äîinterpolant whenever the desired $x$ is closer to the middle of the tabulated data than to either end.</p>
            <h3 id="derivation">Derivation</h3>
            <p>I. <strong>Equally-spaced data and difference operators</strong></p>
            <p>Nodes and spacing</p>
            <p>$$
                x_i = x_0 + ih,\qquad h = x_{i+1}-x_i (\text{constant}),\qquad i=0,\dots n
                $$</p>
            <p>Function values</p>
            <p>$$
                y_i = f(x_i)
                $$</p>
            <p>Forward and backward differences</p>
            <p>$$
                \Delta y_i = y_{i+1}-y_i
                $$</p>
            <p>$$
                \nabla y_i = y_i-y_{i-1}
                $$</p>
            <p>with higher orders obtained recursively, e.g.</p>
            <p>$$
                \Delta^{2}y_i = \Delta(\Delta y_i)=y_{i+2}-2y_{i+1}+y_i, \quad
                \nabla^{2}y_i = \nabla(\nabla y_i)=y_i-2y_{i-1}+y_{i-2}
                $$</p>
            <p>and so on.</p>
            <p>II. <strong>Choosing the central origin and defining the reduced argument</strong></p>
            <p>Pick the index</p>
            <p>$$
                m =
                \begin{cases}
                n/2, &amp; n\text{ even}
                (n-1)/2, &amp; n\text{ odd},
                \end{cases}
                \qquad\Longrightarrow\qquad x_m\approx\text{mid-table}
                $$</p>
            <p>Introduce the dimensionless distance of the target point $x$ from the centre:</p>
            <p>$$
                t = \dfrac{x-x_m}{h}.
                $$</p>
            <blockquote>
                <p>$t=0$ at the centre, $t=\pm1$ exactly one grid step away, etc.</p>
            </blockquote>
            <p>III. <strong>Re-expanding Newton‚Äôs forward/backward polynomial about $x_m$</strong></p>
            <p>Start from Newton‚Äôs forward series with base row $x_{m-1}$ (one step left of the centre):</p>
            <p>$$
                f(x) = y_{m-1} + p\Delta y_{m-1} + \frac{p(p-1)}{2!}\Delta^{2}y_{m-1} + \frac{p(p-1)(p-2)}{3!}\Delta^{3}y_{m-1} + \cdots
                $$</p>
            <p>where $p = \dfrac{x-x_{m-1}}{h}=t+1$.</p>
            <p>Rewrite every occurrence of $p$ in terms of $t$.</p>
            <p>Because $p = t+1$,</p>
            <p>$$
                \begin{aligned}
                p &amp;= t+1,\\
                p(p-1) &amp;= (t+1)t, \\
                p(p-1)(p-2) &amp;= (t+1)t(t-1),\quad\text{etc.}
                \end{aligned}
                $$</p>
            <p>Shift the constant term from $y_{m-1}$ to $y_m$.</p>
            <p>Note that $y_m = y_{m-1} + \Delta y_{m-1}$.</p>
            <p>Substitute $y_{m-1}=y_m-\Delta y_{m-1}$ into the series and regroup.</p>
            <p>After cancelling like terms one obtains the <strong>Gauss forward central formula</strong></p>
            <p>$$
                \boxed{f(x)\approx y_m + t\Delta y_{m-1} + \frac{t(t-1)}{2!}\Delta^2 y_{m-1} + \frac{t(t+1)(t-1)}{3!}\Delta^3 y_{m-2} + \cdots}
                $$</p>
            <p>Analogous manipulation starting from Newton‚Äôs backward series one row to the right of the centre ($x_{m+1}$) yields</p>
            <p>$$
                \boxed{f(x)\approx y_m + t\nabla y_{m+1} + \frac{t(t+1)}{2!}\nabla^2 y_{m+1} + \frac{t(t+1)(t-1)}{3!}\nabla^3 y_{m+2} + \cdots}
                $$</p>
            <p>These are precisely <strong>Gauss‚Äôs forward and backward central-difference polynomials</strong>.</p>
            <p>IV. <strong>Why the factorial products look symmetric</strong></p>
            <ul>
                <li>Each coefficient in the forward (or backward) series is now a <strong>central factorial</strong> such as $t(t-1), t(t+1)(t-1),\ldots$. These arise automatically when you substitute $p=t\pm1$ and regroup.</li>
                <li>The difference rows used‚Äî$\Delta y_{m-1}, \Delta^{2}y_{m-1}, \Delta^{3}y_{m-2}, \ldots$ on the left, or $\nabla y_{m+1}, \nabla^{2}y_{m+1}, \ldots$ on the right‚Äîstep out symmetrically from the centre, so the truncated polynomial minimises the error for any $x$ with $|t|\lesssim 1$ (i.e.\ near the middle of the table).</li>
                <li>In the limit $t\to0$ both polynomials reduce to $f(x_m)$ as expected; as $|t|$ approaches 1 they smoothly match Newton‚Äôs ordinary forward/backward formulas, ensuring continuity across the entire tabulated interval.</li>
            </ul>
            <h3 id="algorithm-steps">Algorithm Steps</h3>
            <p><strong>Input</strong></p>
            <p>Equally spaced nodes and ordinates</p>
            <p>$${(x_i,y_i)}{i=0}^{n}, y_i=f(x_i)$$</p>
            <p>with step $h=x{i+1}-x_i$ and a target abscissa $x$ that lies inside the table.</p>
            <p>I. <strong>Identify the central reference row</strong></p>
            <p>Locate the mid-index**</p>
            <p>$$
                m=
                \begin{cases}
                n/2, &amp; n\ \text{even}
                (n-1)/2, &amp; n\ \text{odd}
                \end{cases}
                $$</p>
            <p>Hence $x_m$ is as close as possible to the table‚Äôs midpoint.</p>
            <p>Reduced argument</p>
            <p>$$
                t=\dfrac{x-x_m}{h}.
                $$</p>
            <p><strong>Decide forward vs. backward form</strong></p>
            <ul>
                <li>If $t&lt;0$<em> (the target lies to the </em><em>left</em><em> of the centre) ‚Üí use the </em><em>Gauss‚Äìforward</em>* polynomial;</li>
                <li>if $t&gt;0$<em> (to the </em><em>right</em><em>) ‚Üí use </em><em>Gauss‚Äìbackward</em>*.</li>
            </ul>
            <p>II. <strong>Build the central-difference table</strong></p>
            <p>Set up the usual forward-difference triangle for $y_0,y_1,\dots ,y_n$.</p>
            <p>Extract the rows you will need:</p>
            <p>
            <table>
                <tr>
                    <td>order</td>
                    <td>forward form uses</td>
                    <td>backward form uses</td>
                </tr>
                <tr>
                    <td>1st</td>
                    <td>$\Delta y_{m-1}$</td>
                    <td>$\nabla y_{m+1}$</td>
                </tr>
                <tr>
                    <td>2nd</td>
                    <td>$\Delta^{2}y_{m-1}$</td>
                    <td>$\nabla^{2}y_{m+1}$</td>
                </tr>
                <tr>
                    <td>3rd</td>
                    <td>$\Delta^{3}y_{m-2}$</td>
                    <td>$\nabla^{3}y_{m+2}$</td>
                </tr>
                <tr>
                    <td>‚Ä¶</td>
                    <td>‚Ä¶</td>
                    <td>‚Ä¶</td>
                </tr>
            </table>
            </p>
            <p>Only as many orders as you intend to keep are required.</p>
            <p>III. <strong>Insert $t$ and the differences into Gauss‚Äôs formula</strong></p>
            <p>Forward ( $t&lt;0$ )</p>
            <p>$$
                f(x)\approx
                y_m
                +t\Delta y_{m-1}
                +\frac{t(t-1)}{2!}\Delta^{2}y_{m-1}
                +\frac{t(t+1)(t-1)}{3!}\Delta^{3}y_{m-2}
                +\cdots
                $$</p>
            <p>Backward ( $t&gt;0$ )</p>
            <p>$$
                f(x)\approx
                y_m
                +t\nabla y_{m+1}
                +\frac{t(t+1)}{2!}\nabla^{2}y_{m+1}
                +\frac{t(t+1)(t-1)}{3!}\nabla^{3}y_{m+2}
                +\cdots
                $$</p>
            <p>IV. <strong>Accumulate the series to the desired order</strong></p>
            <p>Evaluate the terms sequentially and keep a running sum.</p>
            <p>Stopping criterion options:</p>
            <ul>
                <li>truncate after a pre-chosen order $k$;</li>
                <li>or stop when $|\text{next term}|&lt;\varepsilon$ for a tolerance $\varepsilon$.</li>
            </ul>
            <p>The resulting sum is the interpolated value $\displaystyle \hat f(x)$.</p>
            <blockquote>
                <p><strong>Accuracy tip:</strong> for points with $|t|\le 1$ the first three or four terms usually give error of the same order as the fourth or fifth finite difference, so adding higher orders seldom pays off unless the data are extremely smooth.</p>
            </blockquote>
            <p><strong>Output</strong></p>
            <p>$$
                \boxed{ \hat f(x)=\text{sum obtained in Step IV}}
                $$</p>
            <p>This algorithm preserves every assumption explicitly (equal spacing, central index, choice of $t$) and shows exactly where each quantity enters the computation.</p>
            <h3 id="example">Example</h3>
            <p><strong>Given data (step $h=1$)</strong></p>
            <p>
            <table>
                <tr>
                    <td>$i$</td>
                    <td>$x_i$</td>
                    <td>$y_i=f(x_i)$</td>
                </tr>
                <tr>
                    <td>0</td>
                    <td>0</td>
                    <td>2.0</td>
                </tr>
                <tr>
                    <td>1</td>
                    <td>1</td>
                    <td>3.5</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td>2</td>
                    <td>5.0</td>
                </tr>
                <tr>
                    <td>3</td>
                    <td>3</td>
                    <td>5.8</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td>4</td>
                    <td>6.0</td>
                </tr>
            </table>
            </p>
            <p>We wish to interpolate $f(1.5)$</p>
            <p>I. <strong>Choose the central row and reduced argument</strong></p>
            <ul>
                <li>Mid-index $m=2 -&gt; x_m = 2$.</li>
                <li>Reduced distance from the centre</li>
            </ul>
            <p>$$
                t=\frac{x-x_m}{h}= \frac{1.5-2}{1}= -0.5.
                $$</p>
            <p>Because $t&lt;0$ the <strong>Gauss-forward</strong> (left-hand) polynomial is appropriate.</p>
            <p>II. <strong>Construct the needed central differences</strong></p>
            <p>Forward‚Äêdifference table (only rows required by the formula are shown):</p>
            <p>
            <table>
                <tr>
                    <td>order</td>
                    <td>symbol</td>
                    <td>value</td>
                </tr>
                <tr>
                    <td>0</td>
                    <td>$y_m$</td>
                    <td>$5.0$</td>
                </tr>
                <tr>
                    <td>1</td>
                    <td>$\Delta y_{m-1}=y_2-y_1$</td>
                    <td>$1.5$</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td>$\Delta^{2}y_{m-1}= \Delta y_{2}-\Delta y_{1}=0.8-1.5$</td>
                    <td>$-0.7$</td>
                </tr>
                <tr>
                    <td>3</td>
                    <td>$\Delta^{3}y_{m-2}= \Delta^{2}y_{1}-\Delta^{2}y_{0}= -0.7-0$</td>
                    <td>$-0.7$</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td>$\Delta^{4}y_{m-2}= \Delta^{3}y_{1}-\Delta^{3}y_{0}=0.1-(-0.7)$</td>
                    <td>$0.8$</td>
                </tr>
            </table>
            </p>
            <p>Full differences: $\Delta y_0=1.5,\ \Delta y_1=1.5,\ \Delta y_2=0.8,\ \Delta y_3=0.2$;</p>
            <p>$\Delta^{2}y_0=0,\ \Delta^{2}y_1=-0.7,\ \Delta^{2}y_2=-0.6$;</p>
            <p>$\Delta^{3}y_0=-0.7,\ \Delta^{3}y_1=0.1$.</p>
            <p>III. <strong>Insert $t$ and differences into the Gauss-forward series</strong></p>
            <p>$$
                f(x)\approx y_m + t\Delta y_{m-1} + \frac{t(t-1)}{2!}\Delta^2y_{m-1} + \frac{t(t+1)(t-1)}{3!}\Delta^3y_{m-2} + \frac{t(t+1)(t-1)(t-2)}{4!}\Delta^4y_{m-2}
                $$</p>
            <p>Plug in $t=-0.5$ and the table values:</p>
            <p>
            <table>
                <tr>
                    <td>term</td>
                    <td>numerical value</td>
                </tr>
                <tr>
                    <td>$y_m$</td>
                    <td>$5.0000$</td>
                </tr>
                <tr>
                    <td>$t\Delta y_{m-1}=(-0.5)(1.5)$</td>
                    <td>$-0.7500$</td>
                </tr>
                <tr>
                    <td>$\dfrac{t(t-1)}{2}\Delta^{2}y_{m-1}= \dfrac{(-0.5)(-1.5)}{2}(-0.7)$</td>
                    <td>$-0.2625$</td>
                </tr>
                <tr>
                    <td>$\dfrac{t(t+1)(t-1)}{6}\Delta^{3}y_{m-2}= \dfrac{(-0.5)(0.5)(-1.5)}{6}(-0.7)$</td>
                    <td>$-0.0438$</td>
                </tr>
                <tr>
                    <td>$\dfrac{t(t+1)(t-1)(t-2)}{24}\Delta^{4}y_{m-2}= \dfrac{(-0.5)(0.5)(-1.5)(-2.5)}{24}(0.8)$</td>
                    <td>$-0.0313$</td>
                </tr>
            </table>
            </p>
            <p>IV. <strong>Accumulate the series</strong></p>
            <p>Up to 3rd-order term:</p>
            <p>$$
                f(1.5)\approx5.0000-0.7500-0.2625-0.0438 = 3.9437
                $$</p>
            <p>Including the 4th-order term:</p>
            <p>$$
                f(1.5)\approx3.9437-0.0313= \boxed{3.9124}.
                $$</p>
            <p>Adding still higher orders would change the value by only a few $10^{-3}$ here, so
                $f(1.5)\approx3.91$ is a good Gaussian-interpolated estimate based on the given table.</p>
            <blockquote>
                <p><strong>Check:</strong> The estimate lies between the tabulated $f(1)=3.5$ and $f(2)=5.0$, closer to the latter‚Äîas expected for $x=1.5$.</p>
            </blockquote>
            <h3 id="advantages">Advantages</h3>
            <ul>
                <li><strong>Higher accuracy near the table centre</strong> ‚Äì because the polynomial is built sym¬≠metrically around the midpoint, truncation error is smaller whenever the target $x$ lies roughly one step to either side of that centre, often outperforming Newton‚Äôs end-based forward/backward formulas.</li>
                <li><strong>Better numerical stability</strong> ‚Äì central factorial terms such as $t(t!\pm!1)$ keep successive coefficients similar in magnitude, reducing round-off amplification compared with the large binomial factors that appear when you measure everything from an end point.</li>
                <li><strong>Seamless extension of Newton‚Äôs schemes</strong> ‚Äì the table of ordinary forward differences can be reused; you merely ‚Äúfold‚Äù it around the middle row and read off the required $\Delta$ or $\nabla$ values, so implementation effort is modest if a difference table already exists.</li>
            </ul>
            <h3 id="disadvantages">Disadvantages</h3>
            <ul>
                <li><strong>Requires equal spacing</strong> ‚Äì the classic Gauss forward/backward formulas rely on a constant step $h$; with uneven $x_i$ the method does not apply without re-derivation or switching to divided-difference polynomials.</li>
                <li><strong>Extra bookkeeping</strong> ‚Äì one must identify the correct central row, decide forward vs. backward form, and track which difference (e.g.\ $\Delta^{3}y_{m-2}$ or $\nabla^{3}y_{m+2}$) feeds each term; this is more fiddly than the straight left-to-right pattern of Newton‚Äôs forward series.</li>
                <li><strong>No particular benefit away from the centre</strong> ‚Äì if the target $x$ is closer to an end of the table $(|t|\gg1)$, Gauss‚Äôs central formula reverts in effect to the ordinary Newton formulas but with more complicated indexing, so you gain little (and may lose stability if you retain unnecessary terms).
                    t, there may be no significant advantage over standard methods.</li>
            </ul>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol><a href="#gaussian-interpolation">Gaussian Interpolation</a>
                <ol>
                    <li><a href="#mathematical-formulation">Mathematical Formulation</a></li>
                    <li><a href="#derivation">Derivation</a></li>
                    <li><a href="#algorithm-steps">Algorithm Steps</a></li>
                    <li><a href="#example">Example</a></li>
                    <li><a href="#advantages">Advantages</a></li>
                    <li><a href="#disadvantages">Disadvantages</a></li>
                </ol>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li>Root and Extrema Finding<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/bisection_method.html">Bisection Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/golden_ratio_search.html">Golden Ratio Search</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/gradient_descent.html">Gradient Descent</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/newtons_method.html">Newtons Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/relaxation_method.html">Relaxation Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/root_finding.html">Root Finding</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/secant_method.html">Secant Method</a></li>
                        </ol>
                    </li>
                    <li>Systems of Equations<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/gauss_seidel.html">Gauss Seidel</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/gaussian_elimination.html">Gaussian Elimination</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/inverse_matrix.html">Inverse Matrix</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/jacobi_method.html">Jacobi Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/lu_decomposition.html">Lu Decomposition</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/systems_of_equations.html">Systems of Equations</a></li>
                        </ol>
                    </li>
                    <li>Differentiation<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/backward_difference.html">Backward Difference</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/central_difference.html">Central Difference</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/differentiation.html">Differentiation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/forward_difference.html">Forward Difference</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/taylor_series.html">Taylor Series</a></li>
                        </ol>
                    </li>
                    <li>Integration<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/integration_introduction.html">Integration Introduction</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/midpoint_rule.html">Midpoint Rule</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/monte_carlo.html">Monte Carlo</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/simpsons_rule.html">Simpsons Rule</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/trapezoidal_rule.html">Trapezoidal Rule</a></li>
                        </ol>
                    </li>
                    <li>Matrices<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/eigen_value_decomposition.html">Eigen Value Decomposition</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/eigenvalues_and_eigenvectors.html">Eigenvalues and Eigenvectors</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/matrix_methods.html">Matrix Methods</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/power_method.html">Power Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/qr_method.html">Qr Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/singular_value_decomposition.html">Singular Value Decomposition</a></li>
                        </ol>
                    </li>
                    <li>Regression<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/cubic_spline_interpolation.html">Cubic Spline Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/gaussian_interpolation.html">Gaussian Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/interpolation.html">Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/lagrange_polynomial_interpolation.html">Lagrange Polynomial Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/least_squares.html">Least Squares</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/linear_interpolation.html">Linear Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/newton_polynomial.html">Newton Polynomial</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/regression.html">Regression</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/thin_plate_spline_interpolation.html">Thin Plate Spline Interpolation</a></li>
                        </ol>
                    </li>
                    <li>Ordinary Differential Equations<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/eulers_method.html">Eulers Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/heuns_method.html">Heuns Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/ordinary_differential_equations.html">Ordinary Differential Equations</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/partial_differential_equations.html">Partial Differential Equations</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/picards_method.html">Picards Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/runge_kutta.html">Runge Kutta</a></li>
                        </ol>
                    </li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                ¬© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../../app.js"></script>
    </footer>
    <div id="pdf-spinner-overlay">
        <div class="spinner"></div>
    </div>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>