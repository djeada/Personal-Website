<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Regression Analysis</title>
    <meta content="Regression analysis and curve fitting are important tools in statistics, econometrics, engineering, and modern machine-learning pipelines." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper"><article-section id="article-body">
            <div class="article-action-buttons"><button class="btn-suggest-edit" title="Suggest Edit">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M4 21h4l11-11-4-4L4 17v4z" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button><button class="btn-create-issue" title="Create Issue">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"></circle>
                        <line stroke="currentColor" stroke-width="2" x1="12" x2="12" y1="8" y2="12"></line>
                        <circle cx="12" cy="16" fill="currentColor" r="1"></circle>
                    </svg>
                </button><button class="btn-download" title="Download">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M12 5v14m0 0l-6-6m6 6l6-6" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button></div>
            <p style="text-align: right;"><i>Last modified: May 04, 2025</i></p>
            <p style="text-align: right;"><i>This article is written in: 🇺🇸</i></p>
            <h2 id="regression-analysis">Regression Analysis</h2>
            <p>Regression analysis and curve fitting are important tools in statistics, econometrics, engineering, and modern machine-learning pipelines. At their core they seek a deterministic (or probabilistic) mapping
                $\widehat f: \mathcal X \longrightarrow \mathcal Y$
                that minimizes a suitably chosen loss function with respect to a sample of observations
                $\mathcal D = {(\mathbf x_1,y_1),\dots,(\mathbf x_N,y_N)}\subseteq \mathcal X\times\mathcal Y.$</p>
            <p>A <strong>regression problem</strong> is typically posed under the additive error model</p>
            <p>$$
                y_i = f_*(\mathbf{x}_i) + \varepsilon_i,
                \qquad
                \mathbb{E}[\varepsilon_i \mid \mathbf{x}_i] = 0,
                \qquad
                \mathrm{Var}(\varepsilon_i) = \sigma^2.
                $$</p>
            <p>where $f_*$ is an (unknown) deterministic function and $(\varepsilon_i)$ are random errors. The analyst’s objective is to construct an estimator $\widehat f$ (or equivalently to estimate a parameter vector $\widehat{\boldsymbol\theta}$ specifying $\widehat f$) such that some notion of risk—mean-squared error, negative log-likelihood, predictive log-loss, etc.—is minimized.</p>
            <p>
            <table>
                <tr>
                    <td>Symbol</td>
                    <td>Meaning</td>
                </tr>
                <tr>
                    <td>$N$</td>
                    <td>sample size (number of observations)</td>
                </tr>
                <tr>
                    <td>$p$</td>
                    <td>number of predictors (features)</td>
                </tr>
                <tr>
                    <td>$\mathbf X \in \mathbb R^{N\times p}$</td>
                    <td>design / model matrix whose $i$-th row is $\mathbf x_i^\top$</td>
                </tr>
                <tr>
                    <td>$\mathbf y = (y_1,\dots,y_N)^\top$</td>
                    <td>vector of responses</td>
                </tr>
                <tr>
                    <td>$\boldsymbol\beta\in\mathbb R^{p}$</td>
                    <td>vector of unknown regression coefficients</td>
                </tr>
                <tr>
                    <td>$\widehat{\boldsymbol\beta}$</td>
                    <td>estimator of $\boldsymbol\beta$</td>
                </tr>
                <tr>
                    <td>$\mathbf r=\mathbf y-\mathbf X\widehat{\boldsymbol\beta}$</td>
                    <td>vector of residuals</td>
                </tr>
                <tr>
                    <td>$\lVert \cdot \rVert_2$</td>
                    <td>Euclidean ($\ell_2$) norm</td>
                </tr>
            </table>
            </p>
            <h3 id="curve-fitting">Curve Fitting</h3>
            <p>Curve fitting emphasizes the geometrical problem of approximating a cloud of points by a parametric curve or surface. The archetypal formulation is <strong>polynomial least-squares</strong>: given scalar inputs $x_i\in\mathbb R$, fit an $m$-degree polynomial</p>
            <p>$P_m(x)=\sum_{k=0}^{m} a_k x^{k}\quad (\boldsymbol a\in\mathbb R^{m+1})$</p>
            <p>by minimizing the <strong>sum-of-squares loss</strong></p>
            <p>$$
                S(\mathbf{a})
                = \sum_{i=1}^N \bigl(P_m(x_i) - y_i\bigr)^2.
                $$</p>
            <p>In matrix form let $\mathbf V\in\mathbb R^{N\times(m+1)}$ be the <em>Vandermonde matrix</em> with $V_{ik}=x_i^{k}$ and $\mathbf a=(a_0,\dots,a_m)^\top$. The normal equations read
                $\mathbf V^{\top}\mathbf V\,\widehat{\mathbf a}=\mathbf V^{\top}\mathbf y.$</p>
            <p>Provided $\mathbf V^{\top}\mathbf V$ is nonsingular (which fails if $m \ge N$ or data are collinear), the minimizer is uniquely given by
                $\widehat{\mathbf a}=(\mathbf V^{\top}\mathbf V)^{-1}\mathbf V^{\top}\mathbf y.$</p>
            <p><img alt="curve_fitting" src="https://github.com/djeada/Numerical-Methods/assets/37275728/03a26675-9baa-4557-92fb-2ab86c9d7b7c" /></p>
            <blockquote>
                <p><strong>Remark (Overfitting and Regularisation).</strong>
                    High-degree polynomials can interpolate noisy data yet extrapolate disastrously. Ridge ($\ell_2$) or Lasso ($\ell_1$) penalties enforce smoothness or sparsity:</p>
            </blockquote>
            <p>$$
                S_\lambda(a) = \lVert V\,a - y\rVert_2^2 + \lambda\,\lVert a\rVert_q^q,\quad q\in{1,2}
                $$</p>
            <blockquote>
                <p>Closed-form solutions exist for $q=2$; for $q=1$ one must resort to convex optimisation.</p>
            </blockquote>
            <p>Other classical curve-fitting families include <strong>splines</strong>, <strong>B-splines</strong>, <strong>Bezier curves</strong>, <strong>wavelet bases</strong>, and <strong>kernel smoothers</strong> (e.g. Nadaraya–Watson). Each trades parametric flexibility against interpretability and computational cost.</p>
            <h3 id="regression-analysis">Regression Analysis</h3>
            <p>In modern statistics, <strong>regression</strong> refers to modeling the conditional mean</p>
            <p>$$
                \mathbb{E}[\,y\mid x\,] = \mu(x;\,\beta),
                $$</p>
            <p>where $\mu(\cdot;\beta)$ is a known function (link) indexed by parameters $\beta$. Given i.i.d. samples $(x_i,y_i)$, our goal is to estimate $\beta$.</p>
            <h4 id="linear-model">Linear Model</h4>
            <p>If</p>
            <p>$$
                \mu(x;\beta) = x^\top \beta,
                $$</p>
            <p>the model is <strong>linear</strong> in the parameters. Writing the data matrix $X$ and response vector $y$, the OLS estimator solves</p>
            <p>$$
                \hat\beta = \arg\min_{\beta}\,\|\,y - X\beta\|_2^2.
                $$</p>
            <p>When $\mathrm{rank}(X)=p$, the closed-form solution is</p>
            <p>$$
                \hat\beta = (X^\top X)^{-1}X^\top y.
                $$</p>
            <p><strong>Gauss–Markov Theorem.</strong> If $\mathrm{Cov}(\varepsilon)=\sigma^2I$, then among all linear unbiased estimators $\tilde\beta = Cy$ with $CX=I$, OLS has the smallest variance:</p>
            <p>$$
                \mathrm{Var}(\tilde\beta) - \mathrm{Var}(\hat\beta) \succeq 0.
                $$</p>
            <h4 id="generalized-linear-model-glm-">Generalized Linear Model (GLM)</h4>
            <p>For responses in the exponential family (e.g.\ Bernoulli, Poisson), we introduce a <strong>link</strong> $g$ so that</p>
            <p>$$
                g\bigl(\mu(x)\bigr) = x^\top \beta.
                $$</p>
            <p>For instance, in logistic regression $g(\mu)=\log\bigl(\mu/(1-\mu)\bigr)$. Parameters are found by maximizing the likelihood</p>
            <p>$$
                \hat\beta = \arg\max_{\beta}\prod_{i=1}^{N} f\bigl(y_i;\,\mu(x_i;\beta)\bigr),
                $$</p>
            <p>using Fisher scoring or Newton methods.</p>
            <h4 id="nonlinear-least-squares-nls-">Nonlinear Least Squares (NLS)</h4>
            <p>When $\mu(x;\beta)$ is nonlinear in $\beta$ (e.g.\ Michaelis–Menten: $\mu(x;V,K)=Vx/(K+x)$), we minimize</p>
            <p>$$
                S(\beta) = \sum_{i=1}^N \bigl(y_i - \mu(x_i;\beta)\bigr)^2.
                $$</p>
            <p>This loss is generally non-convex; standard solvers include Levenberg–Marquardt or trust-region algorithms.</p>
            <h3 id="concepts-in-regression">Concepts in Regression</h3>
            <p>
            <table>
                <tr>
                    <td>Concept</td>
                    <td>Formal Definition</td>
                </tr>
                <tr>
                    <td><strong>Parameter Estimation</strong></td>
                    <td>$\displaystyle \hat\theta = \arg\min_{\theta}\,\mathcal L(\theta)$ where $\mathcal L$ is least‐squares or negative log‐likelihood.</td>
                </tr>
                <tr>
                    <td><strong>Fitted Values</strong></td>
                    <td>$\displaystyle \hat y_i = \mu(\mathbf x_i;\,\hat\theta)$</td>
                </tr>
                <tr>
                    <td><strong>Residuals</strong></td>
                    <td>$\displaystyle r_i = y_i - \hat y_i$</td>
                </tr>
                <tr>
                    <td></td>
                    <td>$\displaystyle \hat\varepsilon_i = \frac{r_i}{1 - h_{ii}}$, with $h_{ii}$ the $i$th diagonal of the hat matrix.</td>
                </tr>
                <tr>
                    <td><strong>Loss / Error</strong></td>
                    <td>$\displaystyle \mathrm{RSS} = \sum_i r_i^2$</td>
                </tr>
                <tr>
                    <td></td>
                    <td>$\displaystyle -\sum_i \bigl[y_i\log\hat y_i + (1-y_i)\log(1-\hat y_i)\bigr]$</td>
                </tr>
                <tr>
                    <td><strong>Risk</strong></td>
                    <td>$\displaystyle R(\hat f) = \mathbb{E}\bigl[\mathcal L(\hat f(\mathbf x),y)\bigr]$, empirical risk minimisation replaces $\mathbb{E}$ by the sample mean.</td>
                </tr>
                <tr>
                    <td><strong>Goodness-of-Fit</strong></td>
                    <td>$\displaystyle R^2 = 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}},\quad \mathrm{TSS} = \sum_i (y_i - \bar y)^2$</td>
                </tr>
                <tr>
                    <td></td>
                    <td>$\displaystyle \bar R^2 = 1 - (1 - R^2)\,\frac{N-1}{N-p-1}$</td>
                </tr>
                <tr>
                    <td></td>
                    <td>$\displaystyle \mathrm{AIC} = 2k - 2\log\hat L$</td>
                </tr>
                <tr>
                    <td></td>
                    <td>$\displaystyle \mathrm{BIC} = k\log N - 2\log\hat L$</td>
                </tr>
                <tr>
                    <td><strong>Inference</strong></td>
                    <td>$\displaystyle z_j = \frac{\hat\beta_j}{\widehat{\mathrm{se}}(\hat\beta_j)} \approx N(0,1)$</td>
                </tr>
                <tr>
                    <td></td>
                    <td>$\displaystyle 2(\ell_1 - \ell_0)\sim \chi^2_{\text{df}}$</td>
                </tr>
                <tr>
                    <td><strong>Prediction Interval</strong></td>
                    <td>$\displaystyle \hat y_0 \pm t_{N-p,\,1-\alpha/2}\,\hat\sigma\sqrt{1 + \mathbf x_0^\top (X^\top X)^{-1}\mathbf x_0}$</td>
                </tr>
            </table>
            </p>
            <h3 id="types-of-regression-methods">Types of Regression Methods</h3>
            <ol>
                <li><strong>Ordinary Least Squares (OLS)</strong> – Closed-form, BLUE under Gauss–Markov conditions.</li>
                <li><strong>Ridge Regression</strong> – Penalised least-squares with $\lambda|\boldsymbol\beta|_2^2$; solution $\widehat{\boldsymbol\beta}=(\mathbf X^{\top}\mathbf X+\lambda\mathbf I)^{-1}\mathbf X^{\top}\mathbf y$.</li>
                <li><strong>Lasso &amp; Elastic Net</strong> – $\ell_1$ and mixed $\ell_1+\ell_2$ penalties promoting sparsity; solved by coordinate descent or LARS.</li>
                <li><strong>Generalised Linear Models (GLM)</strong> – Logistic, probit, Poisson; estimated by iteratively re-weighted least squares.</li>
                <li><strong>Non-linear Regression (NLS)</strong> – Use gradient-based optimisers; asymptotic theory requires identifiability and regularity.</li>
                <li><strong>Robust Regression</strong> – M-estimators with Huber or Tukey bisquare $\rho$-functions; minimises $\sum_{i}\rho(r_i/\hat\sigma)$.</li>
                <li><strong>Quantile Regression</strong> – Minimises asymmetric absolute loss $\sum_{i}\rho_\tau(r_i)$ with $\rho_\tau(u)=u(\tau-\mathbb 1_{u&lt;0})$.</li>
                <li><strong>Bayesian Regression</strong> – Places prior $p(\boldsymbol\beta)$, outputs posterior $p(\boldsymbol\beta\mid\mathbf y)\propto L(\boldsymbol\beta),p(\boldsymbol\beta)$; predictive distribution integrates over posterior.</li>
            </ol>
            <blockquote>
                <p><strong>Computational Note.</strong> High-dimensional ($p\gg N$) problems demand numerical linear-algebra tricks: Woodbury identity, iterative conjugate gradient, stochastic gradient descent (SGD), or variance-reduced methods (SVRG, SAGA).</p>
            </blockquote>
            <h3 id="worked-examples">Worked Examples</h3>
            <h4 id="example-1-ols-in-matrix-form">Example 1 – OLS in Matrix Form</h4>
            <p>We have $N=5$ observations ${(x_i,y_i)}$ and wish to fit</p>
            <p>$$
                y_i = \beta_0 + \beta_1 x_i + \varepsilon_i,\qquad
                \varepsilon_i\sim\text{mean }0.
                $$</p>
            <p>We stack the data as</p>
            <p>$$
                X =
                \begin{bmatrix}
                1 &amp; 0.8\\
                1 &amp; 1.2\\
                1 &amp; 1.9\\
                1 &amp; 2.4\\
                1 &amp; 3.0
                \end{bmatrix},
                \qquad
                y =
                \begin{bmatrix}
                1.2\\
                1.9\\
                3.1\\
                3.9\\
                5.1
                \end{bmatrix}.
                $$</p>
            <p>The OLS estimator is</p>
            <p>$$
                \hat\beta
                = (X^\top X)^{-1}\,X^\top y.
                $$</p>
            <p>Compute</p>
            <p>$$
                X^\top X
                = \begin{bmatrix}
                5 &amp; 9.30\\
                9.30&amp;20.45
                \end{bmatrix},
                \quad
                X^\top y
                = \begin{bmatrix}
                15.20\\
                33.79
                \end{bmatrix}.
                $$</p>
            <p>Hence</p>
            <p>$$
                \hat\beta
                = \begin{pmatrix}\hat\beta_0\\hat\beta_1\end{pmatrix}
                \approx
                \begin{pmatrix}-0.236, 1.751\end{pmatrix}.
                $$</p>
            <p>The fitted line is</p>
            <p>$$
                \hat y = -0.236 + 1.751\,x.
                $$</p>
            <p>To assess fit, let $\bar y=15.20/5=3.04$. Then</p>
            <p>$$
                R^2
                = 1 - \frac{\sum_i (y_i - \hat y_i)^2}{\sum_i (y_i - \bar y)^2}
                \approx 0.998.
                $$</p>
            <h4 id="example-2-logistic-regression-mle-derivatives">Example 2 – Logistic Regression, MLE Derivatives</h4>
            <p>For binary data $y_i\in{0,1}$ the log-likelihood is</p>
            <p>$$
                \ell(\beta) = \sum_{i=1}^N \bigl[y_i\,x_i^\top \beta - \log\bigl(1 + e^{x_i^\top \beta}\bigr)\bigr].
                $$</p>
            <p>Gradient and Hessian:</p>
            <p>$$
                \nabla\ell(\beta) = X^\top (y - \pi),
                \quad
                \pi = (1 + e^{-X\beta})^{-1},
                $$</p>
            <p>$$
                \nabla^2\ell(\beta) = -\,X^\top \mathrm{diag}\bigl(\pi \circ (1 - \pi)\bigr)\,X
                \preceq0.
                $$</p>
            <p>Newton iteration: $\boldsymbol\beta^{(t+1)}=\boldsymbol\beta^{(t)}-(\nabla^2\ell)^{-1}\nabla\ell$.</p>
            <h3 id="applications">Applications</h3>
            <ul>
                <li><strong>Finance &amp; Econometrics</strong> – Capital asset pricing (CAPM), term-structure models, volatility forecasting (GARCH regression), default-probability prediction.</li>
                <li><strong>Healthcare &amp; Epidemiology</strong> – Survival analysis (Cox proportional hazards), dose-response curves, genome-wide association studies (GWAS) via penalised regression.</li>
                <li><strong>Engineering</strong> – System identification, Kalman-filter regressions, fatigue-life modelling.</li>
                <li><strong>Marketing &amp; A/B Testing</strong> – Uplift modelling, mixed-effect regressions for hierarchical data.</li>
                <li><strong>Machine Learning Pipelines</strong> – Feature engineering baseline, stacking/blending meta-learners, interpretability audits.</li>
            </ul>
            <h3 id="limitations-pitfalls">Limitations &amp; Pitfalls</h3>
            <p>I. <strong>Model Misspecification:</strong></p>
            <p>When $f_*(\mathbf{x})$ lies outside the chosen hypothesis class, estimators remain biased even as $N \to \infty$.</p>
            <p>II. <strong>Violation of IID:</strong></p>
            <p>Autocorrelated or clustered errors require GLS or “sandwich” covariance estimators.</p>
            <p>III. <strong>Heteroscedasticity:</strong></p>
            <p>If $Var(\varepsilon_i \mid \mathbf{x}_i) = \sigma_i^2$, the usual OLS variance formula is invalid; use White’s (HC) estimators instead.</p>
            <p>IV. <strong>Multicollinearity:</strong></p>
            <p>Near–linear dependence among columns of $X$ inflates $Var(\hat\beta_j)$; ridge regression can shrink the condition number.</p>
            <p>V. <strong>High Use &amp; Outliers:</strong></p>
            <p>Cook’s distance</p>
            <p>$$D_i = \frac{r_i^2,h_{ii}}{p,\hat\sigma^2,(1 - h_{ii})^2}$$</p>
            <p>identifies influential points; strong M–estimators mitigate their effect.</p>
            <p>VI. <strong>Overfitting / High Variance:</strong></p>
            <p>Cross-validation, information criteria, or Bayesian model averaging help choose model complexity.</p>
            <p>VII. <strong>External Validity:</strong></p>
            <p>Regression learns the conditional mean on $\mathcal{D}$; distribution shifts (covariate shift, concept drift) break prediction accuracy.</p>
            <p>VIII. <strong>Causal Inference vs. Prediction:</strong></p>
            <p>Regression coefficients are not causal unless confounding is addressed (e.g.\ via instrumental variables, RCTs, or DAG-based adjustment).</p>
            <h3 id="further-reading">Further Reading</h3>
            <ol>
                <li>Seber, G. A. F., &amp; Lee, A. J. <em>Linear Regression Analysis</em>, 2e, Wiley (2003).</li>
                <li>Hastie, T., Tibshirani, R., &amp; Friedman, J. <em>The Elements of Statistical Learning</em>, 2e, Springer (2009).</li>
                <li>McCullagh, P., &amp; Nelder, J. <em>Generalized Linear Models</em>, 2e, Chapman &amp; Hall (1989).</li>
                <li>Kennedy, P. <em>A Guide to Econometrics</em>, 7e, Wiley-Blackwell (2008).</li>
            </ol>
        </article-section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol><a href="#regression-analysis">Regression Analysis</a>
                <ol>
                    <li><a href="#curve-fitting">Curve Fitting</a></li>
                    <li><a href="#regression-analysis">Regression Analysis</a>
                        <ol>
                            <li><a href="#linear-model">Linear Model</a></li>
                            <li><a href="#generalized-linear-model-glm-">Generalized Linear Model (GLM)</a></li>
                            <li><a href="#nonlinear-least-squares-nls-">Nonlinear Least Squares (NLS)</a></li>
                        </ol>
                    </li>
                    <li><a href="#concepts-in-regression">Concepts in Regression</a></li>
                    <li><a href="#types-of-regression-methods">Types of Regression Methods</a></li>
                    <li><a href="#worked-examples">Worked Examples</a>
                        <ol>
                            <li><a href="#example-1-ols-in-matrix-form">Example 1 – OLS in Matrix Form</a></li>
                            <li><a href="#example-2-logistic-regression-mle-derivatives">Example 2 – Logistic Regression, MLE Derivatives</a></li>
                        </ol>
                    </li>
                    <li><a href="#applications">Applications</a></li>
                    <li><a href="#limitations-pitfalls">Limitations &amp; Pitfalls</a></li>
                    <li><a href="#further-reading">Further Reading</a></li>
                </ol>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li>Root and Extrema Finding<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/bisection_method.html">Bisection Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/golden_ratio_search.html">Golden Ratio Search</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/gradient_descent.html">Gradient Descent</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/newtons_method.html">Newtons Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/relaxation_method.html">Relaxation Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/root_finding.html">Root Finding</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/secant_method.html">Secant Method</a></li>
                        </ol>
                    </li>
                    <li>Systems of Equations<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/gauss_seidel.html">Gauss Seidel</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/gaussian_elimination.html">Gaussian Elimination</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/inverse_matrix.html">Inverse Matrix</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/jacobi_method.html">Jacobi Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/lu_decomposition.html">Lu Decomposition</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/systems_of_equations.html">Systems of Equations</a></li>
                        </ol>
                    </li>
                    <li>Differentiation<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/backward_difference.html">Backward Difference</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/central_difference.html">Central Difference</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/differentiation.html">Differentiation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/forward_difference.html">Forward Difference</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/taylor_series.html">Taylor Series</a></li>
                        </ol>
                    </li>
                    <li>Integration<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/integration_introduction.html">Integration Introduction</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/midpoint_rule.html">Midpoint Rule</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/monte_carlo.html">Monte Carlo</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/simpsons_rule.html">Simpsons Rule</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/trapezoidal_rule.html">Trapezoidal Rule</a></li>
                        </ol>
                    </li>
                    <li>Matrices<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/eigen_value_decomposition.html">Eigen Value Decomposition</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/eigenvalues_and_eigenvectors.html">Eigenvalues and Eigenvectors</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/matrix_methods.html">Matrix Methods</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/power_method.html">Power Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/qr_method.html">Qr Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/singular_value_decomposition.html">Singular Value Decomposition</a></li>
                        </ol>
                    </li>
                    <li>Regression<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/cubic_spline_interpolation.html">Cubic Spline Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/gaussian_interpolation.html">Gaussian Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/interpolation.html">Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/lagrange_polynomial_interpolation.html">Lagrange Polynomial Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/least_squares.html">Least Squares</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/linear_interpolation.html">Linear Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/newton_polynomial.html">Newton Polynomial</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/regression.html">Regression</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/thin_plate_spline_interpolation.html">Thin Plate Spline Interpolation</a></li>
                        </ol>
                    </li>
                    <li>Ordinary Differential Equations<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/eulers_method.html">Eulers Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/heuns_method.html">Heuns Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/ordinary_differential_equations.html">Ordinary Differential Equations</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/partial_differential_equations.html">Partial Differential Equations</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/picards_method.html">Picards Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/runge_kutta.html">Runge Kutta</a></li>
                        </ol>
                    </li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>Thank you for visiting my personal website. All content here is free to use, but please remember to be respectful and avoid any misuse of the site. If you’d like to get in touch, feel free to reach out via my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or connect with me on <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have technical questions or ideas to share. Wishing you all the best and a fantastic life ahead!</p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                © Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../../app.js"></script>
    </footer>
    <div id="pdf-spinner-overlay">
        <div class="spinner"></div>
    </div>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>