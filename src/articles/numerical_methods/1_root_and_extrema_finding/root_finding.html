<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Root-Finding in Numerical Methods</title>
    <meta content="Root-finding algorithms aim to solve equations of the form" name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <div class="article-action-buttons"><button class="btn-suggest-edit" title="Suggest Edit">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M4 21h4l11-11-4-4L4 17v4z" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button><button class="btn-create-issue" title="Create Issue">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"></circle>
                        <line stroke="currentColor" stroke-width="2" x1="12" x2="12" y1="8" y2="12"></line>
                        <circle cx="12" cy="16" fill="currentColor" r="1"></circle>
                    </svg>
                </button><button class="btn-download" title="Download">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M12 5v14m0 0l-6-6m6 6l6-6" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button></div>
            <p style="text-align: right;"><i>Last modified: August 19, 2020</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="root-finding-in-numerical-methods">Root-Finding in Numerical Methods</h2>
            <p>Root-finding algorithms aim to solve equations of the form</p>
            <p>$$f(x) = 0$$</p>
            <p>for a scalar (or vector) variable $x$. Although linear problems $\,ax + b = 0\,$ admit the straightforward solution $x = -b/a$, many practical applications involve <strong>nonlinear</strong> equationsâ€”polynomial or transcendentalâ€”where closed-form solutions may be unavailable or difficult to find. Consequently, numerical methods are indispensable for approximating roots.</p>
            <h3 id="relevance-and-applications">Relevance and Applications</h3>
            <ul>
                <li><strong>Root-finding</strong> is used in engineering to determine operating points and equilibrium solutions in mechanical, electrical, or chemical systems. </li>
                <li>In <strong>physics</strong>, root-finding helps solve boundary value problems, locate energy eigenstates, or find intersection points in scattering problems. </li>
                <li>In <strong>computer science and machine learning</strong>, root-finding is involved in optimizing objective functions, which can reduce to solving (\nabla f(x) = 0). </li>
                <li><strong>Financial applications</strong> include calculating implied volatilities or internal rates of return, often involving transcendental equations that require root-finding. </li>
                <li>Solving <strong>nonlinear systems</strong> in higher dimensions, such as (\mathbf{f}(\mathbf{x}) = \mathbf{0}), extends 1D root-finding methods (e.g., Newtonâ€™s method generalizes to Newtonâ€“Raphson for multiple dimensions). </li>
            </ul>
            <h3 id="basic-concepts">Basic Concepts</h3>
            <h4 id="root">Root</h4>
            <p>A <strong>root</strong> $r$ of a function $f$ is a solution to</p>
            <p>$$f(r) = 0.$$</p>
            <p>Depending on $f$:</p>
            <ol>
                <li><strong>Polynomial functions</strong> $f(x) = a_n x^n + \dots + a_0$ may have multiple real or complex roots. </li>
                <li><strong>Transcendental functions</strong> (e.g., $\sin x$, $\exp x$, $\log x$) may have infinitely many roots or require special methods. </li>
                <li><strong>Generalized exponents</strong> or radicals ($x^\pi, x^{3/5}, \sqrt{x}, \ldots$) introduce domain constraints or branch cuts.</li>
            </ol>
            <h4 id="brackets-and-the-intermediate-value-theorem">Brackets and the Intermediate Value Theorem</h4>
            <p>A <strong>bracket</strong> $[a,b]$ is an interval such that</p>
            <p>$$f(a)\,f(b) &lt; 0.$$</p>
            <p>Provided $f$ is <strong>continuous</strong> on $[a,b]$, the <strong>Intermediate Value Theorem (IVT)</strong> guarantees there is at least one root in $[a,b]$. This property underpins <strong>bracketing methods</strong>, ensuring that each iteration can reduce the interval size while preserving a sign change that encloses a root.</p>
            <blockquote>
                <p><strong>Note</strong>: If $f$ is not continuous, sign changes do not necessarily imply roots in that interval.</p>
            </blockquote>
            <h4 id="convergence">Convergence</h4>
            <p>An iterative method generates a sequence of approximations ${x_k}$. If $x_k \to r$ as $k \to \infty$ (for some root $r$), we say the method <strong>converges</strong> to $r$. The <strong>rate of convergence</strong> is an important property:</p>
            <ul>
                <li><strong>Linear convergence</strong>: $\lvert x_{k+1} - r\rvert \approx C \,\lvert x_k - r\rvert$ for some $C &lt; 1$. </li>
                <li><strong>Quadratic convergence</strong>: $\lvert x_{k+1} - r\rvert \approx K \,\lvert x_k - r\rvert^2$. </li>
            </ul>
            <p>For example, Bisection typically has <strong>linear convergence</strong>, while Newtonâ€™s Method can exhibit <strong>quadratic convergence</strong> (under favorable conditions).</p>
            <h4 id="tolerance">Tolerance</h4>
            <p>A <strong>tolerance</strong> $\varepsilon$ is chosen so that when</p>
            <p>$$\lvert x_{k+1} - x_k\rvert \,&lt;\, \varepsilon \quad\text{or}\quad \lvert f(x_k)\rvert \,&lt;\, \delta,$$
                we consider the root sufficiently approximated and stop. The choice of $\varepsilon$ and $\delta$ depends on the applicationâ€™s precision needs.</p>
            <h3 id="common-root-finding-methods">Common Root-Finding Methods</h3>
            <p>Root-finding methods can be broadly classified into <strong>bracketing</strong> (guaranteed convergence under certain assumptions) and <strong>open</strong> methods (faster convergence but risk of divergence). Some methods combine both aspects.</p>
            <h4 id="bracketing-methods">Bracketing Methods</h4>
            <h5>Bisection Method</h5>
            <ul>
                <li>Start with an interval $[a_0, b_0]$ where $f(a_0)f(b_0) &lt; 0$ to ensure the presence of a root. </li>
                <li>Calculate the midpoint $m = \frac{a_k + b_k}{2}$ of the current interval. </li>
                <li>If $f(a_k)f(m) &lt; 0$, update the interval to $[a_{k+1}, b_{k+1}] = [a_k, m]$. </li>
                <li>If $f(a_k)f(m) \geq 0$, update the interval to $[a_{k+1}, b_{k+1}] = [m, b_k]$. </li>
                <li>The method halves the interval length in each iteration, leading to linear convergence with a ratio of 0.5. </li>
                <li>After $n$ steps, the bracket size reduces to $\frac{b_0 - a_0}{2^n}$. </li>
            </ul>
            <h5>False Position (Regula Falsi)</h5>
            <ul>
                <li>Select an interval $[a, b]$ such that $f(a)f(b) &lt; 0$, ensuring the presence of a root in the interval. </li>
                <li>Compute the interpolated point $x_{\text{interp}} = \frac{a f(b) - b f(a)}{f(b) - f(a)}$ using linear interpolation between $(a, f(a))$ and $(b, f(b))$. </li>
                <li>Evaluate $f(x_{\text{interp}})$ to check which subinterval contains the root. </li>
                <li>If $f(a)f(x_{\text{interp}}) &lt; 0$, update the interval to $[a, x_{\text{interp}}]$. </li>
                <li>If $f(a)f(x_{\text{interp}}) \geq 0$, update the interval to $[x_{\text{interp}}, b]$. </li>
                <li>Repeat the computation of $x_{\text{interp}}$ and interval updates until the stopping condition is met. </li>
                <li>Stop when $|b - a|$ or $|f(x_{\text{interp}})|$ is smaller than the predefined tolerance. </li>
                <li>The final result is the root approximation $x_{\text{interp}}$ with accuracy determined by the chosen tolerance. </li>
            </ul>
            <h4 id="open-methods">Open Methods</h4>
            <p>Open methods rely on derivative information or estimates, and do not require an initial bracket. However, they can fail to converge if poorly chosen initial guesses or pathological function behavior occurs.</p>
            <h5>Newtonâ€™s Method</h5>
            <ul>
                <li>Start with an initial guess $x_0$ for the root of $f(x) = 0$. </li>
                <li>Compute the next approximation using the formula $x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}$. </li>
                <li>At each step, approximate $f(x)$ by the tangent line at $x_k$ and solve for the root of this tangent. </li>
                <li>The method exhibits <strong>quadratic convergence</strong> near simple roots when $f'(r) \neq 0$, as the error reduces quadratically with each iteration. </li>
                <li>The convergence rate satisfies $\lim_{k \to \infty} \frac{|x_{k+1} - r|}{|x_k - r|^2} = \text{constant}$. </li>
                <li>The method requires the derivative $f'(x)$, which must be computed or approximated if unavailable. </li>
                <li>If $f'(x_k) \approx 0$, the method can diverge or produce large, inaccurate jumps. </li>
                <li>Without a guaranteed bracket, the method may fail if the initial guess $x_0$ is too far from the actual root. </li>
            </ul>
            <h5>Secant Method</h5>
            <ul>
                <li>The <strong>idea</strong> is to approximate $f'(x_k)$ using a finite difference, resulting in the formula $f'(x_k) \approx \frac{f(x_k) - f(x_{k-1})}{x_k - x_{k-1}}$. </li>
                <li>The iterative formula is derived as $x_{k+1} = x_k - f(x_k) \frac{x_k - x_{k-1}}{f(x_k) - f(x_{k-1})}$, replacing the derivative in Newton's method. </li>
                <li>The method typically exhibits <strong>superlinear convergence</strong> with a rate close to $1.618$, which is the golden ratio. </li>
                <li>This approach is <strong>useful</strong> because it avoids the need for an explicit derivative function, relying instead on previous function evaluations. </li>
            </ul>
            <h4 id="combination-methods">Combination Methods</h4>
            <ul>
                <li>Start with a bracket $[a_0, b_0]$ such that $f(a_0)f(b_0) &lt; 0$, ensuring a root exists within the interval. </li>
                <li>Evaluate $f(a)$ and $f(b)$, and initialize $c = b$ as the last valid root approximation. </li>
                <li>At each iteration, compute an interpolation candidate $x_{\text{interp}}$ using either secant or inverse quadratic interpolation based on the most recent points. </li>
                <li>Check if $x_{\text{interp}}$ falls within the bracket $[a, b]$ and provides sufficient progress toward the root. </li>
                <li>If $x_{\text{interp}}$ is valid and improves the approximation, update the bracket to $[a, x_{\text{interp}}]$ or $[x_{\text{interp}}, b]$ based on the sign of $f(x_{\text{interp}})$. </li>
                <li>If $x_{\text{interp}}$ fails or is outside the bracket, perform a bisection step by setting the midpoint of the bracket as the next approximation. </li>
                <li>Update $c$ to the most recent approximation, ensuring $c$ always represents the last reliable approximation of the root. </li>
                <li>Repeat the process until the stopping condition is met, typically when the interval size or $f(c)$ is below a predefined tolerance. </li>
                <li>The final result is the approximation $c$, which satisfies the convergence criteria. </li>
            </ul>
            <h3 id="choosing-a-root-finding-method">Choosing a Root-Finding Method</h3>
            <ul>
                <li><strong>Continuity</strong> ensures that a bracketing method can reliably locate a root, as the existence of a sign change guarantees a solution within the interval. </li>
                <li><strong>Differentiability</strong> allows for use of methods like Newton's, which rely on first derivatives to predict the root. If the derivative is unavailable, alternative methods such as secant or bisection are better options. </li>
                <li><strong>Multiplicity of roots</strong> poses challenges for Newtonâ€™s method; when a root is not simple (e.g., $f(r) = 0$ but $f'(r) = 0$), the method can converge very slowly or fail entirely. </li>
                <li><strong>Bracketing methods</strong> like bisection are robust because they systematically narrow the interval containing the root, but their linear convergence makes them slower for high-precision needs. </li>
                <li><strong>Open methods</strong> such as Newton's or secant can converge much faster, often at a superlinear or quadratic rate, but they require good initial guesses to avoid divergence or incorrect results. </li>
                <li><strong>Computational cost</strong> can escalate when functions or their derivatives are complex to evaluate, making derivative-free methods like secant attractive as they avoid calculating derivatives directly. However, they may require more iterations to achieve the same accuracy. </li>
                <li><strong>Hybrid approaches</strong> combine reliability and speed by starting with a bracketing method like bisection to secure the root interval, then transitioning to faster methods like Newton's or secant once near the solution. </li>
                <li><strong>Combination methods</strong> like Brentâ€™s dynamically adapt between bracketing and open techniques, providing both robustness of convergence and faster refinement when conditions are favorable. </li>
            </ul>
            <h4 id="example-flow-of-decision">Example Flow of Decision</h4>
            <ul>
                <li>Determine if $f(x)$ has an easily detectable sign change. </li>
                <li>If a sign change is present, use bracketing methods like bisection or false position to isolate the root. </li>
                <li>If no sign change is found, scan the domain or apply domain knowledge to guess a valid interval. </li>
                <li>Assess if $f'(x)$ is analytically or numerically cheap to compute. </li>
                <li>If derivatives are easy to compute, use Newtonâ€™s method for faster local convergence. </li>
                <li>If derivatives are not available or are expensive, use derivative-free methods like secant. </li>
                <li>Decide if reliability is a key priority for the solution. </li>
                <li>If reliability is critical, select robust methods like Brentâ€™s, which combine bracketing and open steps. </li>
            </ul>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol><a href="#root-finding-in-numerical-methods">Root-Finding in Numerical Methods</a>
                <ol>
                    <li><a href="#relevance-and-applications">Relevance and Applications</a></li>
                    <li><a href="#basic-concepts">Basic Concepts</a>
                        <ol>
                            <li><a href="#root">Root</a></li>
                            <li><a href="#brackets-and-the-intermediate-value-theorem">Brackets and the Intermediate Value Theorem</a></li>
                            <li><a href="#convergence">Convergence</a></li>
                            <li><a href="#tolerance">Tolerance</a></li>
                        </ol>
                    </li>
                    <li><a href="#common-root-finding-methods">Common Root-Finding Methods</a>
                        <ol>
                            <li><a href="#bracketing-methods">Bracketing Methods</a></li>
                            <li><a href="#open-methods">Open Methods</a></li>
                            <li><a href="#combination-methods">Combination Methods</a></li>
                        </ol>
                    </li>
                    <li><a href="#choosing-a-root-finding-method">Choosing a Root-Finding Method</a>
                        <ol>
                            <li><a href="#example-flow-of-decision">Example Flow of Decision</a></li>
                        </ol>
                    </li>
                </ol>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li>Root and Extrema Finding<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/bisection_method.html">Bisection Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/golden_ratio_search.html">Golden Ratio Search</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/gradient_descent.html">Gradient Descent</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/newtons_method.html">Newtons Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/relaxation_method.html">Relaxation Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/root_finding.html">Root Finding</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/secant_method.html">Secant Method</a></li>
                        </ol>
                    </li>
                    <li>Systems of Equations<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/gauss_seidel.html">Gauss Seidel</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/gaussian_elimination.html">Gaussian Elimination</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/inverse_matrix.html">Inverse Matrix</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/jacobi_method.html">Jacobi Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/lu_decomposition.html">Lu Decomposition</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/systems_of_equations.html">Systems of Equations</a></li>
                        </ol>
                    </li>
                    <li>Differentiation<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/backward_difference.html">Backward Difference</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/central_difference.html">Central Difference</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/differentiation.html">Differentiation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/forward_difference.html">Forward Difference</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/taylor_series.html">Taylor Series</a></li>
                        </ol>
                    </li>
                    <li>Integration<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/integration_introduction.html">Integration Introduction</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/midpoint_rule.html">Midpoint Rule</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/monte_carlo.html">Monte Carlo</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/simpsons_rule.html">Simpsons Rule</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/trapezoidal_rule.html">Trapezoidal Rule</a></li>
                        </ol>
                    </li>
                    <li>Matrices<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/eigen_value_decomposition.html">Eigen Value Decomposition</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/eigenvalues_and_eigenvectors.html">Eigenvalues and Eigenvectors</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/matrix_methods.html">Matrix Methods</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/power_method.html">Power Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/qr_method.html">Qr Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/singular_value_decomposition.html">Singular Value Decomposition</a></li>
                        </ol>
                    </li>
                    <li>Regression<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/cubic_spline_interpolation.html">Cubic Spline Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/gaussian_interpolation.html">Gaussian Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/interpolation.html">Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/lagrange_polynomial_interpolation.html">Lagrange Polynomial Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/least_squares.html">Least Squares</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/linear_interpolation.html">Linear Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/newton_polynomial.html">Newton Polynomial</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/regression.html">Regression</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/thin_plate_spline_interpolation.html">Thin Plate Spline Interpolation</a></li>
                        </ol>
                    </li>
                    <li>Ordinary Differential Equations<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/eulers_method.html">Eulers Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/heuns_method.html">Heuns Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/ordinary_differential_equations.html">Ordinary Differential Equations</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/partial_differential_equations.html">Partial Differential Equations</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/picards_method.html">Picards Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/runge_kutta.html">Runge Kutta</a></li>
                        </ol>
                    </li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All content here is free to use,
                    but please remember to be respectful and avoid any misuse of the site.
                    If youâ€™d like to get in touch, feel free to reach out via my
                    <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a>
                    or connect with me on
                    <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a>
                    if you have technical questions or ideas to share.
                    Wishing you all the best and a fantastic life ahead!
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../../app.js"></script>
    </footer>
    <div id="pdf-spinner-overlay">
        <div class="spinner"></div>
    </div>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>