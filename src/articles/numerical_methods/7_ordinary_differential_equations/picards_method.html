<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Picard's Iteration Method</title>
    <meta content="Picard's method, alternatively known as the method of successive approximations, is a tool primarily used for solving initial-value problems for first-order ordinary differential equations (ODEs)." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <div class="article-action-buttons"><button class="btn-suggest-edit" title="Suggest Edit">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M4 21h4l11-11-4-4L4 17v4z" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button><button class="btn-create-issue" title="Create Issue">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"></circle>
                        <line stroke="currentColor" stroke-width="2" x1="12" x2="12" y1="8" y2="12"></line>
                        <circle cx="12" cy="16" fill="currentColor" r="1"></circle>
                    </svg>
                </button><button class="btn-download" title="Download">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M12 5v14m0 0l-6-6m6 6l6-6" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button></div>
            <p style="text-align: right;"><i>Last modified: April 25, 2025</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="picard-s-iteration-method">Picard's Iteration Method</h2>
            <p>Picard's method, alternatively known as the method of successive approximations, is a tool primarily used for solving initial-value problems for first-order ordinary differential equations (ODEs). The approach hinges on an iterative process that approximates the solution of an ODE. Though this method is notably simple and direct, it can sometimes converge slowly, depending on the specifics of the problem at hand.</p>
            <h3 id="mathematical-formulation-of-picard-s-method">Mathematical Formulation of Picard's Method</h3>
            <p>Picard's method originates from the idea of expressing the solution to an ODE as an infinite series. We then approximate this solution by truncating the series after a certain number of terms. Given a first-order ODE of the general form $y' = f(x, y)$, complemented by the initial condition $y(x_0) = y_0$, we can express the iterative formula for Picard's method as follows:</p>
            <p>$$y_{n+1}(x) = y_0 + \int_{x_0}^{x} f(t, y_n(t)) dt$$</p>
            <h3 id="derivation">Derivation</h3>
            <p>Picard's method of successive approximations is based on the principle of transforming the differential equation into an equivalent integral equation, which can then be solved iteratively. Let's take a look at how it's derived.</p>
            <p>Consider a first-order ordinary differential equation (ODE):</p>
            <p>$$ y'(t) = f(t, y(t)),$$</p>
            <p>with an initial condition: </p>
            <p>$$ y(t_0) = y_0$$</p>
            <p>The main idea of Picard's method is to rewrite the ODE as an equivalent integral equation by integrating both sides with respect to $t$ over the interval $[t_0, t]$:</p>
            <p>$$\int_{t_0}^t y'(t) dt = \int_{t_0}^t f(t, y(t)) dt$$</p>
            <p>The left-hand side of the equation becomes $y(t) - y(t_0)$ by the fundamental theorem of calculus, which we can rewrite using our initial condition to obtain:</p>
            <p>$$y(t) = y_0 + \int_{t_0}^t f(s, y(s)) ds$$</p>
            <p>Now we can see the integral equation form of the initial value problem. </p>
            <p>To solve this equation using Picard's method, we generate a sequence of functions ${y_n(t)}$ iteratively as:</p>
            <p>$$y_{n+1}(t) = y_0 + \int_{t_0}^t f(s, y_n(s)) ds,$$</p>
            <p>where $y_0(t)$ is the initial guess (often taken as the constant function $y_0(t) = y_0$).</p>
            <p>Each approximation $y_{n+1}(t)$ is defined in terms of the previous approximation $y_n(t)$, creating an iterative process which can be repeated until a desired level of accuracy is achieved.</p>
            <h3 id="algorithm-steps">Algorithm steps</h3>
            <ol>
                <li>Start with an initial approximation $y_0(x)$.</li>
                <li>Calculate the next approximation $y_{n+1}(x) = y_0 + \int_{x_0}^{x} f(t, y_n(t)) dt$.</li>
                <li>Repeat step 2 for a pre-determined number of iterations $n$ or until the approximation of the solution meets the required accuracy.</li>
            </ol>
            <h3 id="example-application">Example Application</h3>
            <p>Consider the first-order ODE</p>
            <p>$$y' = x + y, \quad y(0) = 1$$</p>
            <p>We will use <strong>Picard iteration</strong> (also called the <strong>method of successive approximations</strong>) to approximate the solution. Recall that if $y$ solves the initial value problem, then $y$ must satisfy the integral equation</p>
            <p>$$y(x) = y(0) + \int_{0}^{x} \bigl(t + y(t)\bigr) dt$$</p>
            <p>Because $y(0) = 1$, this becomes</p>
            <p>$$y(x) = 1 + \int_{0}^{x} \bigl(t + y(t)\bigr) dt$$</p>
            <h4 id="step-1-choose-an-initial-guess">Step 1: Choose an initial guess</h4>
            <p>A simple choice for the initial approximation is a constant function that satisfies the initial condition:
                $$y_0(x) = 1$$</p>
            <h4 id="step-2-form-the-iteration">Step 2: Form the iteration</h4>
            <p>Define the $(n+1)$-th approximation $y_{n+1}(x)$ by</p>
            <p>$$y_{n+1}(x) = 1 + \int_{0}^{x} \bigl(t + y_n(t)\bigr) dt$$</p>
            <h4 id="step-3-compute-the-first-few-iterates">Step 3: Compute the first few iterates</h4>
            <p>I. <strong>First iterate $\mathbf{y_1}$:</strong></p>
            <p>Starting from $y_0(t) = 1$,
                $$y_1(x) = 1 + \int_0^x \bigl(t + y_0(t)\bigr) dt = 1 + \int_0^x \bigl(t + 1\bigr) dt$$</p>
            <p>Compute the integral:</p>
            <p>$$\int_0^x \bigl(t + 1\bigr) dt = \left[\tfrac{1}{2}t^2 + t\right]_{t=0}^{t=x} = \tfrac{1}{2}x^2 + x$$</p>
            <p>Hence</p>
            <p>$$y_1(x) = 1 + \left(\tfrac{1}{2}x^2 + x\right) = 1 + x + \tfrac{1}{2}x^2$$</p>
            <p>II. <strong>Second iterate $\mathbf{y_2}$:</strong></p>
            <p>Now substitute $y_1(t) = 1 + t + \tfrac{1}{2}t^2$ into the integral:</p>
            <p>$$y_2(x) = 1 + \int_0^x \bigl(t + y_1(t)\bigr) dt = 1 + \int_0^x \Bigl(t + \bigl(1 + t + \tfrac{1}{2}t^2\bigr)\Bigr) dt = 1 + \int_0^x \bigl(1 + 2t + \tfrac{1}{2}t^2\bigr) dt$$
                Compute this integral:
                $$\int_0^x \bigl(1 + 2t + \tfrac{1}{2}t^2\bigr) dt = \left[ t + t^2 + \tfrac{1}{6}t^3\right]_{t=0}^{t=x} = x + x^2 + \tfrac{1}{6}x^3$$</p>
            <p>Therefore,</p>
            <p>$$y_2(x) = 1 + \bigl(x + x^2 + \tfrac{1}{6}x^3\bigr) = 1 + x + x^2 + \tfrac{1}{6}x^3$$</p>
            <p>In principle, you continue in this mannerâ€”substituting $y_n(x)$ back into the integral equationâ€”to obtain $y_{n+1}(x)$. You either stop when you get a satisfactory approximation or when you reach your maximum number of iterations.</p>
            <h4 id="observing-a-pattern">Observing a Pattern</h4>
            <p>Notice how each iteration adds higher-order polynomial terms in $x$. In fact, this process is <strong>building the power-series expansion</strong> of the true solution around $x=0$. More and more terms (with increasing powers of $x$) appear as you iterate.</p>
            <h4 id="the-exact-solution">The Exact Solution</h4>
            <p>Although Picard iteration is a good way to <em>approximate</em> or <em>numerically</em> solve the ODE, we can also solve it directly by standard methods for first-order linear ODEs. Indeed, rewriting</p>
            <p>$$y' - y = x$$</p>
            <p>the integrating factor is $e^{-x}$. Multiplying the equation by $e^{-x}$ gives</p>
            <p>$$\frac{d}{dx}\bigl(e^{-x}y\bigr) = x e^{-x}$$</p>
            <p>Integrate both sides from $0$ to $x$:</p>
            <p>$$e^{-x} y(x) - e^{-0} y(0) = \int_0^x t e^{-t} dt$$</p>
            <p>Since $y(0) = 1$, the left-hand side is $e^{-x} y(x) - 1$. The right-hand side can be computed by integration by parts (or looked up in a table):</p>
            <p>$$\int_0^x t e^{-t} dt = \bigl[-(t+1)e^{-t}\bigr]_{0}^{x} = - (x+1) e^{-x} + 1$$</p>
            <p>Hence,</p>
            <p>$$e^{-x}y(x) - 1 = - (x+1) e^{-x} + 1 \quad\Longrightarrow\quad e^{-x}y(x) = 2 - (x+1)e^{-x}$$</p>
            <p>Multiply both sides by $e^{x}$ to solve for $y(x)$:</p>
            <p>$$y(x) = 2 e^x - (x+1)$$</p>
            <p>A quick check: at $x=0$, $y(0) = 2\cdot 1 - (0+1) = 1$, matching the initial condition.</p>
            <h4 id="connecting-back-to-the-iterates">Connecting Back to the Iterates</h4>
            <p>If you <strong>expand the exact solution</strong> $y(x) = 2e^x - (x+1)$ in a Maclaurin series around $x=0$,</p>
            <p>$$2e^x = 2\Bigl(1 + x + \tfrac{x^2}{2!} + \tfrac{x^3}{3!} + \cdots\Bigr) = 2 + 2x + x^2 + \tfrac{x^3}{3} + \cdots$$</p>
            <p>$$2e^x - (x+1) = (2-1) + (2x - x) + x^2 + \tfrac{x^3}{3} + \cdots = 1 + x + x^2 + \tfrac{x^3}{3} + \cdots$$</p>
            <p>Picard iteration naturally generates the <strong>partial sums</strong> of this series:</p>
            <p>$$y_1(x) = 1 + x + \tfrac{1}{2}x^2, \quad y_2(x) = 1 + x + x^2 + \tfrac{1}{6}x^3, \quad y_3(x) = 1 + x + x^2 + \tfrac{1}{3}x^3 + \cdots,$$</p>
            <p>and so on. Each iterate gets you closer to the true (infinite series) solution; numerically, you see that the coefficients of each power of $x$ converge to those in the exact expansion.</p>
            <p>In this example, the partial sums $y_0, y_1, y_2, \cdots$ quickly converge to the <strong>exact solution</strong> </p>
            <p>$$y(x) = 2 e^x - (x+1)$$</p>
            <p>If one needs a purely numerical approach (especially for more complicated $f(x,y)$ ), continuing the iteration until it stabilizes (or up to a certain tolerance/number of terms) provides a valid approximate solution.</p>
            <h3 id="advantages">Advantages</h3>
            <ul>
                <li>Picard's method provides a <strong>straightforward iterative process</strong> for solving ODEs and is particularly useful in theoretical contexts for proving the existence and uniqueness of solutions. </li>
                <li>The <strong>series-based solution</strong> derived from Picard's method allows for a clear understanding of the solution's structure and behavior in mathematical terms. </li>
                <li>It can serve as a foundation for teaching <strong>iterative techniques</strong> and for introducing students to numerical and analytical solution methods. </li>
                <li>The method is well-suited for equations where both the function $f(x, y)$ and its <strong>integral</strong> are smooth, ensuring convergence in many standard cases. </li>
            </ul>
            <h3 id="limitations">Limitations</h3>
            <ul>
                <li>Each iteration of <strong>Picard's method</strong> involves evaluating an integral, which becomes computationally demanding for complicated or multidimensional functions. </li>
                <li><strong>Convergence</strong> is not guaranteed for functions $f(x, y)$ that are discontinuous, unbounded, or violate Lipschitz conditions in the region of interest. </li>
                <li>The method tends to converge slowly for <strong>stiff or nonlinear ODEs</strong>, making it less practical for such problems compared to other numerical methods. </li>
                <li>Applying Picard's method can be impractical for ODEs requiring <strong>high precision</strong>, as the number of iterations to achieve accuracy grows significantly. </li>
                <li>The approach is not suitable for equations where <strong>numerical solutions</strong> are desired quickly, as alternative methods like Runge-Kutta or finite difference techniques are often more efficient. </li>
            </ul>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol><a href="#picard-s-iteration-method">Picard's Iteration Method</a>
                <ol>
                    <li><a href="#mathematical-formulation-of-picard-s-method">Mathematical Formulation of Picard's Method</a></li>
                    <li><a href="#derivation">Derivation</a></li>
                    <li><a href="#algorithm-steps">Algorithm steps</a></li>
                    <li><a href="#example-application">Example Application</a>
                        <ol>
                            <li><a href="#step-1-choose-an-initial-guess">Step 1: Choose an initial guess</a></li>
                            <li><a href="#step-2-form-the-iteration">Step 2: Form the iteration</a></li>
                            <li><a href="#step-3-compute-the-first-few-iterates">Step 3: Compute the first few iterates</a></li>
                            <li><a href="#observing-a-pattern">Observing a Pattern</a></li>
                            <li><a href="#the-exact-solution">The Exact Solution</a></li>
                            <li><a href="#connecting-back-to-the-iterates">Connecting Back to the Iterates</a></li>
                        </ol>
                    </li>
                    <li><a href="#advantages">Advantages</a></li>
                    <li><a href="#limitations">Limitations</a></li>
                </ol>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li>Root and Extrema Finding<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/bisection_method.html">Bisection Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/golden_ratio_search.html">Golden Ratio Search</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/gradient_descent.html">Gradient Descent</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/newtons_method.html">Newtons Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/relaxation_method.html">Relaxation Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/root_finding.html">Root Finding</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/secant_method.html">Secant Method</a></li>
                        </ol>
                    </li>
                    <li>Systems of Equations<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/gauss_seidel.html">Gauss Seidel</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/gaussian_elimination.html">Gaussian Elimination</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/inverse_matrix.html">Inverse Matrix</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/jacobi_method.html">Jacobi Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/lu_decomposition.html">Lu Decomposition</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/systems_of_equations.html">Systems of Equations</a></li>
                        </ol>
                    </li>
                    <li>Differentiation<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/backward_difference.html">Backward Difference</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/central_difference.html">Central Difference</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/differentiation.html">Differentiation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/forward_difference.html">Forward Difference</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/taylor_series.html">Taylor Series</a></li>
                        </ol>
                    </li>
                    <li>Integration<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/integration_introduction.html">Integration Introduction</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/midpoint_rule.html">Midpoint Rule</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/monte_carlo.html">Monte Carlo</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/simpsons_rule.html">Simpsons Rule</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/trapezoidal_rule.html">Trapezoidal Rule</a></li>
                        </ol>
                    </li>
                    <li>Matrices<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/eigen_value_decomposition.html">Eigen Value Decomposition</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/eigenvalues_and_eigenvectors.html">Eigenvalues and Eigenvectors</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/matrix_methods.html">Matrix Methods</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/power_method.html">Power Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/qr_method.html">Qr Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/singular_value_decomposition.html">Singular Value Decomposition</a></li>
                        </ol>
                    </li>
                    <li>Regression<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/cubic_spline_interpolation.html">Cubic Spline Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/gaussian_interpolation.html">Gaussian Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/interpolation.html">Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/lagrange_polynomial_interpolation.html">Lagrange Polynomial Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/least_squares.html">Least Squares</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/linear_interpolation.html">Linear Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/newton_polynomial.html">Newton Polynomial</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/regression.html">Regression</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/thin_plate_spline_interpolation.html">Thin Plate Spline Interpolation</a></li>
                        </ol>
                    </li>
                    <li>Ordinary Differential Equations<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/eulers_method.html">Eulers Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/heuns_method.html">Heuns Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/ordinary_differential_equations.html">Ordinary Differential Equations</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/partial_differential_equations.html">Partial Differential Equations</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/picards_method.html">Picards Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/runge_kutta.html">Runge Kutta</a></li>
                        </ol>
                    </li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../../app.js"></script>
    </footer>
    <div id="pdf-spinner-overlay">
        <div class="spinner"></div>
    </div>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>