<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Gaussian Elimination</title>
    <meta content="Gaussian elimination is a fundamental algorithmic procedure in linear algebra used to solve systems of linear equations, find matrix inverses, and determine the rank of matrices." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>Last modified: January 27, 2025</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="gaussian-elimination">Gaussian Elimination</h2>
            <p>Gaussian elimination is a fundamental algorithmic procedure in linear algebra used to solve systems of linear equations, find matrix inverses, and determine the rank of matrices. The procedure systematically applies elementary row operations to transform a given matrix into an upper-triangular form (row echelon form), from which the solution to the system (if it exists and is unique) can be readily determined by back substitution.</p>
            <p>From a conceptual viewpoint, Gaussian elimination provides a structured approach to eliminating unknowns step-by-step. Geometrically, each linear equation represents a hyperplane in $n$-dimensional space, and the solution of the system corresponds to the intersection point(s) of these hyperplanes. Gaussian elimination successively "clears out" the variables, enabling a direct path to the solution (or revealing inconsistencies or infinite solution sets if they exist).</p>
            <p><img alt="gaussian_elimination" src="https://github.com/user-attachments/assets/54011276-5a17-4666-8fbf-91d92ee9c30e" /></p>
            <h3 id="mathematical-formulation">Mathematical Formulation</h3>
            <p>Consider a system of $n$ linear equations with $n$ unknowns:</p>
            <p>$$A\mathbf{x} = \mathbf{b},$$
                where</p>
            <p>$$A = \begin{bmatrix}
                a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
                a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
                \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
                a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nn}
                \end{bmatrix}
                $$</p>
            <p>$$
                \mathbf{x} = \begin{bmatrix} x_1 \ x_2 \ \cdots \ x_n \end{bmatrix}
                $$</p>
            <p>$$
                \mathbf{b} = \begin{bmatrix} b_1 \ b_2 \ \cdots \ b_n \end{bmatrix}
                $$</p>
            <p>We form the augmented matrix $[A|\mathbf{b}]$:</p>
            <p>$$
                [A|\mathbf{b}] = \begin{bmatrix}
                a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} &amp; b_1 \\
                a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} &amp; b_2 \\
                \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
                a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nn} &amp; b_n
                \end{bmatrix}
                $$</p>
            <p>The goal of Gaussian elimination is to perform a series of row operations to transform $[A|\mathbf{b}]$ into an upper-triangular form:</p>
            <p>$$[U|\mathbf{c}] = \begin{bmatrix}
                u_{11} &amp; u_{12} &amp; \cdots &amp; u_{1n} &amp; c_1 \\
                0 &amp; u_{22} &amp; \cdots &amp; u_{2n} &amp; c_2 \\
                \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
                0 &amp; 0 &amp; \cdots &amp; u_{nn} &amp; c_n
                \end{bmatrix}$$</p>
            <p>where $U$ is an upper-triangular matrix. Once in this form, the solution $\mathbf{x}$ can be found by back substitution.</p>
            <h3 id="derivation">Derivation</h3>
            <p>The derivation of Gaussian elimination closely mirrors the logic of systematic elimination of variables from a set of equations:</p>
            <p>I. <strong>Elimination of $x_1$ from equations 2 through $n$</strong>: </p>
            <p>Suppose the first pivot (the leading element in the first row) is $a_{11}$. By using row operations, we can eliminate the $x_1$-term from all equations below the first. This is achieved by subtracting suitable multiples of the first row from subsequent rows.</p>
            <p>II. <strong>Elimination of $x_2$ from equations 3 through $n$</strong>: </p>
            <p>After the first step, the second row now has a leading coefficient (pivot) in the second column. Using this pivot, we eliminate the $x_2$-term from all equations below the second.</p>
            <p>III. <strong>Continue this process</strong> until the last pivot $a_{nn}$ (in the $n$-th row) is in place. If at any stage a pivot is zero, we interchange rows (partial pivoting) to bring a nonzero pivot into the pivot position.</p>
            <p>The end result is an upper-triangular system that can be solved starting from the last equation and moving upward (back substitution).</p>
            <h3 id="algorithm-steps">Algorithm Steps</h3>
            <p><strong>Input</strong>: An augmented matrix $[A|\mathbf{b}]$ representing the system $A\mathbf{x} = \mathbf{b}$.</p>
            <p><strong>Output</strong>: A solution vector $\mathbf{x}$ if it exists, or detection of no solution or infinitely many solutions.</p>
            <p><strong>Step-by-Step Procedure</strong>:</p>
            <p>I. <strong>Form the augmented matrix</strong>:</p>
            <p>$$[A|\mathbf{b}] = \begin{bmatrix}
                a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} &amp; b_1 \\
                a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} &amp; b_2 \\
                \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
                a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nn} &amp; b_n
                \end{bmatrix}$$</p>
            <p>II. <strong>Forward Elimination</strong> (to reach upper-triangular form):</p>
            <p>For $i = 1$ to $n$:</p>
            <p>I. <strong>Partial Pivoting (if desired)</strong>: Find the row (pivotRow) below (and including) the current row $i$ that has the largest absolute value in column $i$. Swap the current row $i$ with pivotRow to reduce numerical instability.</p>
            <p>II. <strong>Pivot Normalization</strong>: Divide the entire $i$-th row by $a_{ii}$ (the pivot) to make the pivot element equal to 1.</p>
            <p>III. <strong>Elimination</strong>: For each row $j &gt; i$, subtract $a_{ji}$ times the $i$-th row from the $j$-th row to make the elements below the pivot zero.</p>
            <p>After these steps, the matrix is in row echelon form:</p>
            <p>$$[U|\mathbf{c}] = \begin{bmatrix}
                1 &amp; * &amp; \cdots &amp; * &amp; * \\
                0 &amp; 1 &amp; \cdots &amp; * &amp; * \\
                \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
                0 &amp; 0 &amp; \cdots &amp; 1 &amp; *
                \end{bmatrix}$$</p>
            <p>where $*$ represents arbitrary numbers obtained during the process.</p>
            <p>III. <strong>Back Substitution</strong>:</p>
            <p>Starting from the last equation:</p>
            <p>For $i = n$ down to 1:</p>
            <p>$$x_i = c_i - \sum_{k=i+1}^{n} u_{ik} x_k.$$</p>
            <p>Since $u_{ii}=1$ after normalization, $x_i$ can be directly computed. (If not normalized, divide by $u_{ii}$.)</p>
            <p>This process yields the solution vector $\mathbf{x}$.</p>
            <h3 id="example">Example</h3>
            <p><strong>Given System</strong>:</p>
            <p>$$\begin{aligned}
                2x + y - z &amp;= 8, \\
                -3x - y + 2z &amp;= -11, \\
                -2x + y + 2z &amp;= -3.
                \end{aligned}$$</p>
            <p>I. <strong>Augmented Matrix</strong>:</p>
            <p>$$[A|\mathbf{b}] = \begin{bmatrix}
                2 &amp; 1 &amp; -1 &amp; 8 \\
                -3 &amp; -1 &amp; 2 &amp; -11 \\
                -2 &amp; 1 &amp; 2 &amp; -3
                \end{bmatrix}$$</p>
            <p>II. <strong>Forward Elimination</strong>:</p>
            <ul>
                <li>Pivot in first row is $a_{11} = 2$. Normalize the first row by dividing by 2:</li>
            </ul>
            <p>$$\begin{bmatrix}
                1 &amp; 0.5 &amp; -0.5 &amp; 4 \\
                -3 &amp; -1 &amp; 2 &amp; -11 \\
                -2 &amp; 1 &amp; 2 &amp; -3
                \end{bmatrix}$$</p>
            <ul>
                <li>Eliminate $x$-terms in row 2 and row 3 using row 1:</li>
                <li>For row 2: Add 3 times row 1:</li>
            </ul>
            <p>$$(-3)R_1 + R_2 \to R_2 \implies
                R_2 = \begin{bmatrix}
                0 &amp; 0.5 &amp; 0.5 &amp; 1
                \end{bmatrix}$$</p>
            <ul>
                <li>For row 3: Add 2 times row 1:</li>
            </ul>
            <p>$$(2)R_1 + R_3 \to R_3 \implies
                R_3 = \begin{bmatrix}
                0 &amp; 2 &amp; 1 &amp; 5
                \end{bmatrix}
                $$</p>
            <p>Now the matrix is:</p>
            <p>$$\begin{bmatrix}
                1 &amp; 0.5 &amp; -0.5 &amp; 4 \\
                0 &amp; 0.5 &amp; 0.5 &amp; 1 \\
                0 &amp; 2 &amp; 1 &amp; 5
                \end{bmatrix}
                $$</p>
            <ul>
                <li>Next pivot is $a_{22} = 0.5$. Normalize the second row by dividing by 0.5:</li>
            </ul>
            <p>$$\begin{bmatrix}
                1 &amp; 0.5 &amp; -0.5 &amp; 4 \\
                0 &amp; 1 &amp; 1 &amp; 2 \\
                0 &amp; 2 &amp; 1 &amp; 5
                \end{bmatrix}
                $$</p>
            <ul>
                <li>Eliminate below the second pivot:</li>
            </ul>
            <p>For row 3: subtract 2 times row 2 from row 3:</p>
            <p>$$R_3 - 2R_2 \implies R_3 = \begin{bmatrix}
                0 &amp; 0 &amp; -1 &amp; 1
                \end{bmatrix}
                $$</p>
            <p>Now the matrix is in upper-triangular form:</p>
            <p>$$[U|\mathbf{c}] = \begin{bmatrix}
                1 &amp; 0.5 &amp; -0.5 &amp; 4 \\
                0 &amp; 1 &amp; 1 &amp; 2 \\
                0 &amp; 0 &amp; -1 &amp; 1
                \end{bmatrix}
                $$</p>
            <p>III. <strong>Back Substitution</strong>:</p>
            <ul>
                <li>From the last equation: $-1 \cdot z = 1 \implies z = -1$.</li>
                <li>Substitute $z = -1$ into second equation:</li>
            </ul>
            <p>$y + 1(-1) = 2 \implies y = 3$.</p>
            <ul>
                <li>Substitute $y = 3, z = -1$ into first equation: </li>
            </ul>
            <p>$x + 0.5(3) -0.5(-1) = 4 \implies x + 1.5 + 0.5 = 4 \implies x = 2$.</p>
            <p>The solution is $\mathbf{x} = (2,\, 3,\, -1)^\top$.</p>
            <h3 id="advantages">Advantages</h3>
            <ul>
                <li>Gaussian elimination has <strong>general applicability</strong> to any $n \times n$ system of linear equations, and it can also identify cases where no solution or infinitely many solutions exist in non-square systems.</li>
                <li>The method serves as a <strong>foundation for advanced techniques</strong> like LU decomposition and QR decomposition, forming a building block for many numerical algorithms.</li>
                <li>It can be used to <strong>determine the rank of a matrix</strong> and to compute the inverse of a matrix when applicable by applying the procedure to the augmented matrix $[A | I]$.</li>
            </ul>
            <h3 id="limitations">Limitations</h3>
            <ul>
                <li><strong>Numerical instability</strong> is a concern due to round-off errors, particularly without pivoting. Partial or full pivoting mitigates this issue but may require additional computational steps.</li>
                <li>The methodâ€™s <strong>computational cost</strong>, proportional to $O(n^3)$ operations, can become prohibitive for large-scale systems, making it less efficient than iterative methods for such cases.</li>
                <li>A <strong>zero pivot element</strong> halts the process unless row interchanges are performed. Pivoting is necessary to avoid division by zero and to maintain algorithmic stability.</li>
            </ul>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol><a href="#gaussian-elimination">Gaussian Elimination</a>
                <ol>
                    <li><a href="#mathematical-formulation">Mathematical Formulation</a></li>
                    <li><a href="#derivation">Derivation</a></li>
                    <li><a href="#algorithm-steps">Algorithm Steps</a></li>
                    <li><a href="#example">Example</a></li>
                    <li><a href="#advantages">Advantages</a></li>
                    <li><a href="#limitations">Limitations</a></li>
                </ol>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li>Root and Extrema Finding<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/bisection_method.html">Bisection Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/golden_ratio_search.html">Golden Ratio Search</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/gradient_descent.html">Gradient Descent</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/newtons_method.html">Newtons Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/relaxation_method.html">Relaxation Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/root_finding.html">Root Finding</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/1_root_and_extrema_finding/secant_method.html">Secant Method</a></li>
                        </ol>
                    </li>
                    <li>Systems of Equations<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/gauss_seidel.html">Gauss Seidel</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/gaussian_elimination.html">Gaussian Elimination</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/inverse_matrix.html">Inverse Matrix</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/jacobi_method.html">Jacobi Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/lu_decomposition.html">Lu Decomposition</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/2_systems_of_equations/systems_of_equations.html">Systems of Equations</a></li>
                        </ol>
                    </li>
                    <li>Differentiation<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/backward_difference.html">Backward Difference</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/central_difference.html">Central Difference</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/differentiation.html">Differentiation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/forward_difference.html">Forward Difference</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/3_differentiation/taylor_series.html">Taylor Series</a></li>
                        </ol>
                    </li>
                    <li>Integration<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/integration_introduction.html">Integration Introduction</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/midpoint_rule.html">Midpoint Rule</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/monte_carlo.html">Monte Carlo</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/simpsons_rule.html">Simpsons Rule</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/4_integration/trapezoidal_rule.html">Trapezoidal Rule</a></li>
                        </ol>
                    </li>
                    <li>Matrices<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/eigen_value_decomposition.html">Eigen Value Decomposition</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/eigenvalues_and_eigenvectors.html">Eigenvalues and Eigenvectors</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/matrix_methods.html">Matrix Methods</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/power_method.html">Power Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/qr_method.html">Qr Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/5_matrices/singular_value_decomposition.html">Singular Value Decomposition</a></li>
                        </ol>
                    </li>
                    <li>Regression<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/cubic_spline_interpolation.html">Cubic Spline Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/gaussian_interpolation.html">Gaussian Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/interpolation.html">Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/lagrange_polynomial_interpolation.html">Lagrange Polynomial Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/least_squares.html">Least Squares</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/linear_interpolation.html">Linear Interpolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/newton_polynomial.html">Newton Polynomial</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/regression.html">Regression</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/6_regression/thin_plate_spline_interpolation.html">Thin Plate Spline Interpolation</a></li>
                        </ol>
                    </li>
                    <li>Ordinary Differential Equations<ol>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/eulers_method.html">Eulers Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/heuns_method.html">Heuns Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/ordinary_differential_equations.html">Ordinary Differential Equations</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/partial_differential_equations.html">Partial Differential Equations</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/picards_method.html">Picards Method</a></li>
                            <li><a href="https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/runge_kutta.html">Runge Kutta</a></li>
                        </ol>
                    </li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../../app.js"></script>
    </footer>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>