<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Neural Networks Introduction</title>
    <meta content="Neural networks represent a cornerstone in the field of machine learning, drawing inspiration from neurological processes within the human brain." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>Last modified: June 15, 2024</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="neural-networks-introduction">Neural Networks Introduction</h2>
            <p>Neural networks represent a cornerstone in the field of machine learning, drawing inspiration from neurological processes within the human brain. These networks excel in processing complex datasets with numerous features, transcending traditional methods like logistic regression in both scalability and efficiency. Particularly, logistic regression can become computationally intensive and less practical with a high-dimensional feature space, often necessitating the selection of a feature subset, which might compromise the model's accuracy.</p>
            <p>A neural network comprises an intricate architecture of neurons, analogous to the brain's neural cells, linked through synaptic connections. These connections facilitate the transmission and processing of information across the network. The basic structure of a neural network includes:</p>
            <ul>
                <li><strong>Input Layer:</strong> Serves as the entry point for data into the network.</li>
                <li><strong>Hidden Layers:</strong> Intermediate layers that perform computations and feature transformations.</li>
                <li><strong>Output Layer:</strong> Produces the final output of the network.</li>
            </ul>
            <p>Each neuron in these layers applies a weighted sum to its inputs, followed by a nonlinear activation function. The weights of these connections are represented by the matrix $\theta$, while the bias term is denoted as $x_0$. These parameters are learned during the training process, enabling the network to capture complex patterns and relationships within the data.</p>
            <h3 id="mathematical-representation">Mathematical Representation</h3>
            <p>Consider a neural network with $L$ layers, each layer $l$ having $s_l$ units (excluding the bias unit). The activation of unit $i$ in layer $l$ is denoted as $a_i^{(l)}$. The activation function applied at each neuron is usually a nonlinear function like the sigmoid or ReLU function. The cost function for a neural network, often a function like cross-entropy or mean squared error, is minimized during the training process to adjust the weights $\theta$.</p>
            <h3 id="significance-in-computer-vision">Significance in Computer Vision</h3>
            <p>Neural networks particularly shine in domains like computer vision, where data often involves high-dimensional input spaces. For instance, an image with a resolution of 50x50 pixels, considering only grayscale values, translates to 2,500 features. If we incorporate color channels (RGB), the feature space expands to 7,500 dimensions. Such high-dimensional data is unmanageable for traditional algorithms but is aptly handled by neural networks through feature learning and dimensionality reduction techniques.</p>
            <h3 id="neuroscience-inspiration">Neuroscience Inspiration</h3>
            <p>The inception of neural networks was heavily influenced by the desire to replicate the brain's learning mechanism. This fascination led to the development and evolution of various neural network architectures through the decades. A notable hypothesis in neuroscience suggests that the brain might utilize a universal learning algorithm, adaptable across different sensory inputs and functions. This adaptability is exemplified in experiments where rerouting sensory nerves (e.g., optic to auditory) results in the corresponding cortical area adapting to process the new form of input, a concept that echoes in the design of flexible and adaptive neural networks.</p>
            <h3 id="model-representation-i">Model Representation I</h3>
            <h4 id="neuron-model-in-biology">Neuron Model in Biology</h4>
            <p>In biological terms, a neuron consists of three main parts:</p>
            <ul>
                <li><strong>Cell Body:</strong> Central part of a neuron where inputs are aggregated.</li>
                <li><strong>Dendrites:</strong> Input wires which receive signals from other neurons.</li>
                <li><strong>Axon:</strong> Output wire that transmits signals to other neurons.</li>
            </ul>
            <p>Biological neurons process signals through a combination of electrical and chemical means, sending impulses (or spikes) along the axon as a response to input stimuli received through dendrites.</p>
            <p><img alt="Biological Neuron Diagram" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/neuron.png" /></p>
            <h4 id="artificial-neural-network-neuron-representation">Artificial Neural Network: Neuron Representation</h4>
            <p>In artificial neural networks, the neuron or 'node' functions similarly to its biological counterpart but in a simplified and abstracted manner:</p>
            <ul>
                <li><strong>Inputs:</strong> Received through 'input wires', akin to dendrites.</li>
                <li><strong>Computation:</strong> Performed by a logistic unit within the neuron.</li>
                <li><strong>Output:</strong> Sent down 'output wires', similar to the axon.</li>
            </ul>
            <p>Each neuron in an artificial neural network processes its inputs using a weighted sum and an activation function. The result is then passed on to subsequent neurons or to the output layer.</p>
            <p><img alt="Artificial Neuron Diagram" src="https://user-images.githubusercontent.com/37275728/201517992-cdc14304-2af9-4821-bcae-71caa1a62663.png" /></p>
            <h4 id="mathematical-model-of-a-neuron">Mathematical Model of a Neuron</h4>
            <p>Consider a neuron with inputs represented as a vector $x$, where $x_0$ is the bias unit:</p>
            <p>$$
                x = \begin{bmatrix}
                x_{0} \\
                x_{1} \\
                x_2 \\
                x_3
                \end{bmatrix}
                $$</p>
            <p>And the corresponding weights of the neuron are denoted by $\theta$:</p>
            <p>$$
                \theta = \begin{bmatrix}
                \theta_{0} \\
                \theta_{1} \\
                \theta_2 \\
                \theta_3
                \end{bmatrix}
                $$</p>
            <p>In this representation, $x_0$ is the bias unit that helps in shifting the activation function, and $\theta$ represents the weights of the model.</p>
            <h4 id="layers-in-a-neural-network">Layers in a Neural Network</h4>
            <ul>
                <li><strong>Input Layer:</strong> The initial layer that receives input data.</li>
                <li><strong>Hidden Layers:</strong> Intermediate layers where data transformations occur. The activations within these layers are not directly observable.</li>
                <li><strong>Output Layer:</strong> Produces the final output based on the computations performed by the network.</li>
            </ul>
            <p>The connectivity pattern in a neural network typically involves each neuron in one layer being connected to all neurons in the subsequent layer.</p>
            <p><img alt="Hidden Layer Representation" src="https://user-images.githubusercontent.com/37275728/201517995-ff2af22c-ea22-4be9-9bfc-b7e6c771d69c.png" /></p>
            <h4 id="activation-and-output-computation">Activation and Output Computation</h4>
            <p>The activation $a^{(2)}_i$ of the $i^{th}$ neuron in the 2nd layer is calculated as a function $g$ (such as the sigmoid function) of a weighted sum of inputs:</p>
            <p>$$
                a^{(2)}<em>1 = g(\Theta^{(1)}</em>{10}x_0+\Theta^{(1)}<em>{11}x_1+\Theta^{(1)}</em>{12}x_2+\Theta^{(1)}_{13}x_3)
                $$</p>
            <p>$$
                a^{(2)}<em>2 = g(\Theta^{(1)}</em>{20}x_0+\Theta^{(1)}<em>{21}x_1+\Theta^{(1)}</em>{22}x_2+\Theta^{(1)}_{23}x_3)
                $$</p>
            <p>$$
                a^{(2)}<em>3 = g(\Theta^{(1)}</em>{30}x_0+\Theta^{(1)}<em>{31}x_1+\Theta^{(1)}</em>{32}x_2+\Theta^{(1)}_{33}x_3)
                $$</p>
            <p>The hypothesis function $h_{\Theta}(x)$ for a neural network is the output of the network, which in turn is the activation of the output layer's neurons:</p>
            <p>$$
                h_{\Theta}(x) = g(\Theta^{(2)}<em>{10}a^{(2)}_0+\Theta^{(2)}</em>{11}a^{(2)}<em>1+\Theta^{(2)}</em>{12}a^{(2)}<em>2+\Theta^{(2)}</em>{13}a^{(2)}_3)
                $$</p>
            <h3 id="model-representation-in-neural-networks-ii">Model Representation in Neural Networks II</h3>
            <p>Neural networks process large amounts of data, necessitating efficient computation methods. Vectorization is a key technique used to achieve this efficiency. It allows for the simultaneous computation of multiple operations, significantly speeding up the training and inference processes in neural networks.</p>
            <h4 id="defining-vectorized-terms">Defining Vectorized Terms</h4>
            <p>To illustrate vectorization, consider the computation of the activation for neurons in a layer. The activation of the $i^{th}$ neuron in layer 2, $a^{(2)}_i$, is based on a linear combination of inputs followed by a nonlinear activation function $g$ (e.g., sigmoid function). This can be represented as:</p>
            <p>$$z^{(2)}<em>i = \Theta^{(1)}</em>{i0}x_0+\Theta^{(1)}<em>{i1}x_1+\Theta^{(1)}</em>{i2}x_2+\Theta^{(1)}_{i3}x_3$$</p>
            <p>Hence, the activation $a^{(2)}_i$ is given by:</p>
            <p>$$a^{(2)}_i = g(z^{(2)}_i)$$</p>
            <h4 id="vector-representation">Vector Representation</h4>
            <p>Inputs and activations can be represented as vectors:</p>
            <p>$$
                x = \begin{bmatrix}
                x_{0} \\
                x_{1} \\
                x_2 \\
                x_3
                \end{bmatrix}
                $$</p>
            <p>$$
                z^{(2)} = \begin{bmatrix}
                z^{(2)}_1 \\
                z^{(2)}_2 \\
                z^{(2)}_3
                \end{bmatrix}
                $$</p>
            <ul>
                <li><strong>$z^{(2)}$:</strong> Vector representing the linear combinations for each neuron in layer 2.</li>
                <li><strong>$a^{(2)}$:</strong> Vector representing the activations for each neuron in layer 2, calculated by applying $g()$ to each element of $z^{(2)}$.</li>
                <li><strong>Hidden Layers:</strong> Middle layers in the network, which transform the inputs in a non-linear way.</li>
                <li><strong>Forward Propagation:</strong> The process where input values are fed forward through the network, from input to output layer, producing the final result.</li>
            </ul>
            <h4 id="architectural-flexibility">Architectural Flexibility</h4>
            <p>Neural network architectures can vary in complexity:</p>
            <ul>
                <li><strong>Variation in Node Count:</strong> The number of neurons in each layer can be adjusted based on the complexity of the task.</li>
                <li><strong>Number of Layers:</strong> Additional layers can be added to create deeper networks, which are capable of learning more complex patterns.</li>
            </ul>
            <p><img alt="Multi-Layer Neural Network" src="https://user-images.githubusercontent.com/37275728/201517998-e5f9f245-a6f1-4aed-8a58-fcb0178f38c4.png" /></p>
            <p>In the above example, layer 2 has three hidden units, and layer 3 has two hidden units. By adjusting the number of layers and nodes, neural networks can model complex nonlinear hypotheses, enabling them to tackle a wide range of problems from simple linear classification to complex tasks in computer vision and natural language processing.</p>
            <h3 id="neural-networks-for-logical-functions-and-and-xnor">Neural Networks for Logical Functions: AND and XNOR</h3>
            <h4 id="the-and-function">The AND Function</h4>
            <p>The AND function is a fundamental logical operation that outputs true only if both inputs are true. In neural networks, this can be modeled using a single neuron with appropriate weights and a bias.</p>
            <p><img alt="AND Function Graphical Representation" src="https://user-images.githubusercontent.com/37275728/201518002-72b41fb7-ca3f-4612-aa65-c34f58138737.png" /></p>
            <p>Let's define the bias unit as $x_0 = 1$. We can represent the weights for the AND function in the vector $\Theta^{(1)}_1$:</p>
            <p>$$
                \Theta^{(1)}_1 = \begin{bmatrix}
                -30 \\
                20 \\
                20
                \end{bmatrix}
                $$</p>
            <p>The hypothesis for the AND function, using a sigmoid activation function $g$, is then:</p>
            <p>$$
                h_{\Theta}(x) = g(-30 \cdot 1 + 20 \cdot x_1 + 20 \cdot x_2)
                $$</p>
            <p>The sigmoid function $g(z)$ maps any real number to the $(0, 1)$ interval, effectively acting as an activation function for the neuron.</p>
            <p><img alt="Sigmoid Function Graph" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/sigmoid.png" /></p>
            <h4 id="the-xnor-function">The XNOR Function</h4>
            <p>The XNOR (exclusive-NOR) function is another logical operation that outputs true if both inputs are either true or false.</p>
            <p><img alt="XNOR Function Graphical Representation" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/xnor.png" /></p>
            <p>Unlike the AND function, constructing an XNOR function requires more than one neuron because it is a non-linear function. A neural network with at least one hidden layer containing multiple neurons is necessary to model the XNOR function. The network would typically combine basic logical functions like AND, OR, and NOT in its layers to replicate the XNOR behavior.</p>
            <p>In these examples, the neural network uses a weighted combination of inputs to activate a neuron. The weights (in $\Theta$) and bias terms determine how the neuron responds to different input combinations. For binary classification tasks like AND and XNOR, the sigmoid function works well because it outputs values close to 0 or 1, analogous to the binary nature of these logical operations.</p>
            <h2 id="reference">Reference</h2>
            <p>These notes are based on the free video lectures offered by Stanford University, led by Professor Andrew Ng. These lectures are part of the renowned Machine Learning course available on Coursera. For more information and to access the full course, visit the <a href="https://www.coursera.org/learn/machine-learning">Coursera course page</a>.</p>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#neural-networks-introduction">Neural Networks Introduction</a>
                    <ol>
                        <li><a href="#mathematical-representation">Mathematical Representation</a></li>
                        <li><a href="#significance-in-computer-vision">Significance in Computer Vision</a></li>
                        <li><a href="#neuroscience-inspiration">Neuroscience Inspiration</a></li>
                        <li><a href="#model-representation-i">Model Representation I</a>
                            <ol>
                                <li><a href="#neuron-model-in-biology">Neuron Model in Biology</a></li>
                                <li><a href="#artificial-neural-network-neuron-representation">Artificial Neural Network: Neuron Representation</a></li>
                                <li><a href="#mathematical-model-of-a-neuron">Mathematical Model of a Neuron</a></li>
                                <li><a href="#layers-in-a-neural-network">Layers in a Neural Network</a></li>
                                <li><a href="#activation-and-output-computation">Activation and Output Computation</a></li>
                            </ol>
                        </li>
                        <li><a href="#model-representation-in-neural-networks-ii">Model Representation in Neural Networks II</a>
                            <ol>
                                <li><a href="#defining-vectorized-terms">Defining Vectorized Terms</a></li>
                                <li><a href="#vector-representation">Vector Representation</a></li>
                                <li><a href="#architectural-flexibility">Architectural Flexibility</a></li>
                            </ol>
                        </li>
                        <li><a href="#neural-networks-for-logical-functions-and-and-xnor">Neural Networks for Logical Functions: AND and XNOR</a>
                            <ol>
                                <li><a href="#the-and-function">The AND Function</a></li>
                                <li><a href="#the-xnor-function">The XNOR Function</a></li>
                            </ol>
                        </li>
                    </ol>
                </li>
                <li><a href="#reference">Reference</a></li>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/01_introduction_to_machine_learning.html">Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/02_linear_regression.html">Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/03_review_of_linear_algebra.html">Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/04_linear_regression_multiple_variables.html">Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/06_logistic_regression.html">Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/07_regularization.html">Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/08_neural_networks_representation.html">Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/09_neural_networks_learning.html">Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/10_applying_machine_learning_advice.html">Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/11_machine_learning_system_design.html">Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/12_support_vector_machines.html">Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/13_clustering.html">Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/14_dimensionality_reduction.html">Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/15_anomaly_detection.html">Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/16_recommendation_systems.html">Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/17_large_scale_machine_learning.html">Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/18_photo_ocr.html">Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>