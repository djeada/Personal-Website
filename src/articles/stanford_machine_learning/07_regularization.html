<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Regularization</title>
    <meta content="Regularization is a technique used to prevent overfitting in machine learning models, ensuring they perform well not only on the training data but also on new, unseen data." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>Last modified: January 27, 2025</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="regularization">Regularization</h2>
            <p>Regularization is a technique used to prevent overfitting in machine learning models, ensuring they perform well not only on the training data but also on new, unseen data.</p>
            <h3 id="overfitting-in-machine-learning">Overfitting in Machine Learning</h3>
            <ul>
                <li><strong>Issue</strong>: A model might fit the training data too closely, capturing noise rather than the underlying pattern.</li>
                <li><strong>Effect</strong>: Poor performance on new data.</li>
                <li><strong>Manifestation in Regression</strong>: Occurs when using higher-degree polynomials which results in a high variance hypothesis.</li>
            </ul>
            <h4 id="example-overfitting-in-logistic-regression">Example: Overfitting in Logistic Regression</h4>
            <p><img alt="overfitting_logistic_regression" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/overfitting_logistic_regression.png" /></p>
            <h3 id="regularization-in-cost-function">Regularization in Cost Function</h3>
            <p>Regularization works by adding a penalty term to the cost function that penalizes large coefficients, thereby reducing the complexity of the model.</p>
            <h4 id="regularization-in-linear-regression">Regularization in Linear Regression</h4>
            <ul>
                <li><strong>Regularized Cost Function</strong>:</li>
            </ul>
            <p>$$
                \min \frac{1}{2m} \left[ \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{j=1}^{m} \theta_j^2 \right]
                $$</p>
            <ul>
                <li><strong>Penalization</strong>: Large values of $\theta_3$ and $\theta_4$ are penalized, leading to simpler models.</li>
            </ul>
            <p><img alt="optimization_regularization" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/optimization_regularization.png" /></p>
            <h3 id="regularization-parameter-lambda-">Regularization Parameter: $\lambda$</h3>
            <ul>
                <li><strong>Role of $\lambda$</strong>: Controls the trade-off between fitting the training set well and keeping the model simple (smaller parameter values).</li>
                <li><strong>Selection</strong>: Automated methods can be used to choose an appropriate $\lambda$.</li>
            </ul>
            <h3 id="modifying-gradient-descent">Modifying Gradient Descent</h3>
            <p>The gradient descent algorithm can be adjusted to include the regularization term:</p>
            <p>I. For $\theta_0$ (no regularization):</p>
            <p>$$
                \frac{\partial}{\partial \theta_0} J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})x_0^{(i)}
                $$</p>
            <p>II. For $\theta_j$ ($j \geq 1$):</p>
            <p>$$
                \frac{\partial}{\partial \theta_j} J(\theta) = \left( \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j
                $$</p>
            <h3 id="regularized-linear-regression">Regularized Linear Regression</h3>
            <p>Regularized linear regression incorporates a regularization term in the cost function and its optimization to control model complexity and prevent overfitting.</p>
            <h4 id="gradient-descent-with-regularization">Gradient Descent with Regularization</h4>
            <p>To optimize the regularized linear regression model using gradient descent, the algorithm is adjusted as follows:</p>
            <p>
            <div>
                <pre><code class="language-shell">while not converged:
  for j in [0, ..., n]:
      Î¸_j := Î¸_j - Î± [ \frac{1}{m} \sum_{i=1}^{m}(h_{Î¸}(x^{(i)}) + y^{(i)})x_j^{(i)} + \frac{Î»}{m} Î¸_j ]</code></pre>
            </div>
            </p>
            <p>Here is the Python code to demonstrate regularization in linear regression, including the regularized cost function and gradient descent with regularization. This example uses numpy to implement the regularized linear regression model:</p>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np

def hypothesis(X, theta):
    return np.dot(X, theta)

def compute_cost(X, y, theta, lambda_reg):
    m = len(y)
    h = hypothesis(X, theta)
    cost = (1 / (2 * m)) * (np.sum((h - y) ** 2) + lambda_reg * np.sum(theta[1:] ** 2))
    return cost

def gradient_descent(X, y, theta, alpha, lambda_reg, num_iters):
    m = len(y)
    cost_history = np.zeros(num_iters)

    for iter in range(num_iters):
        h = hypothesis(X, theta)
        theta[0] = theta[0] - alpha * (1 / m) * np.sum((h - y) * X[:, 0])
        for j in range(1, len(theta)):
            theta[j] = theta[j] - alpha * ((1 / m) * np.sum((h - y) * X[:, j]) + (lambda_reg / m) * theta[j])
        
        cost_history[iter] = compute_cost(X, y, theta, lambda_reg)

    return theta, cost_history

# Example usage with mock data
np.random.seed(42)
X = np.random.rand(10, 2)  # Feature matrix (10 examples, 2 features)
y = np.random.rand(10)     # Target values

# Adding a column of ones to X for the intercept term (theta_0)
X = np.hstack((np.ones((X.shape[0], 1)), X))

# Initial parameters
theta = np.random.randn(X.shape[1])
alpha = 0.01  # Learning rate
lambda_reg = 0.1  # Regularization parameter
num_iters = 1000  # Number of iterations

# Perform gradient descent with regularization
theta, cost_history = gradient_descent(X, y, theta, alpha, lambda_reg, num_iters)

print("Optimized parameters:", theta)
print("Final cost:", cost_history[-1])</code></pre>
            </div>
            </p>
            <h4 id="regularization-with-the-normal-equation">Regularization with the Normal Equation</h4>
            <p>In the normal equation approach for regularized linear regression, the optimal $Î¸$ is computed as follows:</p>
            <p><img alt="regularized_normal_equation" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/regularized_normal_equation.png" /></p>
            <p>The equation includes an additional term $Î»I$ to the matrix being inverted, ensuring regularization is accounted for in the solution.</p>
            <p>Here is the Python code to implement regularized linear regression using the normal equation:</p>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np

def regularized_normal_equation(X, y, lambda_reg):
    m, n = X.shape
    I = np.eye(n)
    I[0, 0] = 0  # Do not regularize the bias term (theta_0)
    
    theta = np.linalg.inv(X.T @ X + lambda_reg * I) @ X.T @ y
    return theta

# Example usage with mock data
np.random.seed(42)
X = np.random.rand(10, 2)  # Feature matrix (10 examples, 2 features)
y = np.random.rand(10)     # Target values

# Adding a column of ones to X for the intercept term (theta_0)
X = np.hstack((np.ones((X.shape[0], 1)), X))

# Regularization parameter
lambda_reg = 0.1

# Compute the optimal parameters using the regularized normal equation
theta = regularized_normal_equation(X, y, lambda_reg)

print("Optimized parameters using regularized normal equation:", theta)</code></pre>
            </div>
            </p>
            <h3 id="regularized-logistic-regression">Regularized Logistic Regression</h3>
            <p>The cost function for logistic regression with regularization is:</p>
            <p>$$
                J(Î¸) = \frac{1}{m} \sum_{i=1}^{m}[-y^{(i)}\log(h_{Î¸}(x^{(i)})) - (1-y^{(i)})\log(1 - h_{Î¸}(x^{(i)}))] + \frac{Î»}{2m}\sum_{j=1}^{n}Î¸_j^2
                $$</p>
            <h4 id="gradient-of-the-cost-function">Gradient of the Cost Function</h4>
            <p>The gradient is defined for each parameter $Î¸_j$:</p>
            <p>I. For $j = 0$ (no regularization on $Î¸_0$):</p>
            <p>$$
                \frac{\partial}{\partial Î¸_0} J(Î¸) = \frac{1}{m} \sum_{i=1}^{m} (h_{Î¸}(x^{(i)}) - y^{(i)})x_j^{(i)}
                $$</p>
            <p>II. For $j â‰¥ 1$ (includes regularization):</p>
            <p>$$
                \frac{\partial}{\partial Î¸_j} J(Î¸) = ( \frac{1}{m} \sum_{i=1}^{m} (h_{Î¸}(x^{(i)}) - y^{(i)})x_j^{(i)} ) + \frac{Î»}{m}Î¸_j
                $$</p>
            <h4 id="optimization">Optimization</h4>
            <p>For both linear and logistic regression, the gradient descent algorithm is updated to include regularization:</p>
            <p>
            <div>
                <pre><code class="language-shell">while not converged:
  for j in [0, ..., n]:
      Î¸_j := Î¸_j - Î± [ \frac{1}{m} \sum_{i=1}^{m}(h_{Î¸}(x^{(i)}) + y^{(i)})x_j^{(i)} + \frac{Î»}{m} Î¸_j ]</code></pre>
            </div>
            </p>
            <p>The key difference in logistic regression lies in the hypothesis function $h_{Î¸}(x)$, which is based on the logistic (sigmoid) function.</p>
            <p>Here is the Python code to implement regularized logistic regression using gradient descent:</p>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np
from scipy.special import expit  # Sigmoid function

def sigmoid(z):
    return expit(z)

def compute_cost(X, y, theta, lambda_reg):
    m = len(y)
    h = sigmoid(np.dot(X, theta))
    cost = (1 / m) * np.sum(-y * np.log(h) - (1 - y) * np.log(1 - h)) + (lambda_reg / (2 * m)) * np.sum(theta[1:] ** 2)
    return cost

def gradient_descent(X, y, theta, alpha, lambda_reg, num_iters):
    m = len(y)
    cost_history = np.zeros(num_iters)

    for iter in range(num_iters):
        h = sigmoid(np.dot(X, theta))
        error = h - y
        
        theta[0] = theta[0] - alpha * (1 / m) * np.sum(error * X[:, 0])
        for j in range(1, len(theta)):
            theta[j] = theta[j] - alpha * ((1 / m) * np.sum(error * X[:, j]) + (lambda_reg / m) * theta[j])
        
        cost_history[iter] = compute_cost(X, y, theta, lambda_reg)

    return theta, cost_history

# Example usage with mock data
np.random.seed(42)
X = np.random.rand(10, 2)  # Feature matrix (10 examples, 2 features)
y = np.random.randint(0, 2, 10)  # Binary target values

# Adding a column of ones to X for the intercept term (theta_0)
X = np.hstack((np.ones((X.shape[0], 1)), X))

# Initial parameters
theta = np.random.randn(X.shape[1])
alpha = 0.01  # Learning rate
lambda_reg = 0.1  # Regularization parameter
num_iters = 1000  # Number of iterations

# Perform gradient descent with regularization
theta, cost_history = gradient_descent(X, y, theta, alpha, lambda_reg, num_iters)

print("Optimized parameters:", theta)
print("Final cost:", cost_history[-1])</code></pre>
            </div>
            </p>
            <h2 id="reference">Reference</h2>
            <p>These notes are based on the free video lectures offered by Stanford University, led by Professor Andrew Ng. These lectures are part of the renowned Machine Learning course available on Coursera. For more information and to access the full course, visit the <a href="https://www.coursera.org/learn/machine-learning">Coursera course page</a>.</p>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#regularization">Regularization</a>
                    <ol>
                        <li><a href="#overfitting-in-machine-learning">Overfitting in Machine Learning</a>
                            <ol>
                                <li><a href="#example-overfitting-in-logistic-regression">Example: Overfitting in Logistic Regression</a></li>
                            </ol>
                        </li>
                        <li><a href="#regularization-in-cost-function">Regularization in Cost Function</a>
                            <ol>
                                <li><a href="#regularization-in-linear-regression">Regularization in Linear Regression</a></li>
                            </ol>
                        </li>
                        <li><a href="#regularization-parameter-lambda-">Regularization Parameter: $\lambda$</a></li>
                        <li><a href="#modifying-gradient-descent">Modifying Gradient Descent</a></li>
                        <li><a href="#regularized-linear-regression">Regularized Linear Regression</a>
                            <ol>
                                <li><a href="#gradient-descent-with-regularization">Gradient Descent with Regularization</a></li>
                                <li><a href="#regularization-with-the-normal-equation">Regularization with the Normal Equation</a></li>
                            </ol>
                        </li>
                        <li><a href="#regularized-logistic-regression">Regularized Logistic Regression</a>
                            <ol>
                                <li><a href="#gradient-of-the-cost-function">Gradient of the Cost Function</a></li>
                                <li><a href="#optimization">Optimization</a></li>
                            </ol>
                        </li>
                    </ol>
                </li>
                <li><a href="#reference">Reference</a></li>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/01_introduction_to_machine_learning.html">Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/02_linear_regression.html">Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/03_review_of_linear_algebra.html">Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/04_linear_regression_multiple_variables.html">Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/06_logistic_regression.html">Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/07_regularization.html">Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/08_neural_networks_representation.html">Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/09_neural_networks_learning.html">Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/10_applying_machine_learning_advice.html">Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/11_machine_learning_system_design.html">Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/12_support_vector_machines.html">Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/13_clustering.html">Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/14_dimensionality_reduction.html">Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/15_anomaly_detection.html">Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/16_recommendation_systems.html">Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/17_large_scale_machine_learning.html">Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/18_photo_ocr.html">Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>