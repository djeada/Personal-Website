<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Regularization</title>
    <meta content="Regularization is a technique used to prevent overfitting in machine learning models, ensuring they perform well not only on the training data but also on new, unseen data." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>Last modified: June 15, 2024</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="regularization">Regularization</h2>
            <p>Regularization is a technique used to prevent overfitting in machine learning models, ensuring they perform well not only on the training data but also on new, unseen data.</p>
            <h3 id="overfitting-in-machine-learning">Overfitting in Machine Learning</h3>
            <ul>
                <li><strong>Issue</strong>: A model might fit the training data too closely, capturing noise rather than the underlying pattern.</li>
                <li><strong>Effect</strong>: Poor performance on new data.</li>
                <li><strong>Manifestation in Regression</strong>: Occurs when using higher-degree polynomials which results in a high variance hypothesis.</li>
            </ul>
            <h4 id="example-overfitting-in-logistic-regression">Example: Overfitting in Logistic Regression</h4>
            <p><img alt="overfitting_logistic_regression" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/overfitting_logistic_regression.png" /></p>
            <h3 id="regularization-in-cost-function">Regularization in Cost Function</h3>
            <p>Regularization works by adding a penalty term to the cost function that penalizes large coefficients, thereby reducing the complexity of the model.</p>
            <h4 id="regularization-in-linear-regression">Regularization in Linear Regression</h4>
            <ul>
                <li><strong>Regularized Cost Function</strong>:</li>
            </ul>
            <p>$$
                \min \frac{1}{2m} \left[ \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{j=1}^{m} \theta_j^2 \right]
                $$</p>
            <ul>
                <li><strong>Penalization</strong>: Large values of $\theta_3$ and $\theta_4$ are penalized, leading to simpler models.</li>
            </ul>
            <p><img alt="optimization_regularization" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/optimization_regularization.png" /></p>
            <h3 id="regularization-parameter-lambda-">Regularization Parameter: $\lambda$</h3>
            <ul>
                <li><strong>Role of $\lambda$</strong>: Controls the trade-off between fitting the training set well and keeping the model simple (smaller parameter values).</li>
                <li><strong>Selection</strong>: Automated methods can be used to choose an appropriate $\lambda$.</li>
            </ul>
            <h3 id="modifying-gradient-descent">Modifying Gradient Descent</h3>
            <p>The gradient descent algorithm can be adjusted to include the regularization term:</p>
            <p>I. For $\theta_0$ (no regularization):</p>
            <p>$$
                \frac{\partial}{\partial \theta_0} J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})x_0^{(i)}
                $$</p>
            <p>II. For $\theta_j$ ($j \geq 1$):</p>
            <p>$$
                \frac{\partial}{\partial \theta_j} J(\theta) = \left( \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j
                $$</p>
            <h3 id="regularized-linear-regression">Regularized Linear Regression</h3>
            <p>Regularized linear regression incorporates a regularization term in the cost function and its optimization to control model complexity and prevent overfitting.</p>
            <h4 id="gradient-descent-with-regularization">Gradient Descent with Regularization</h4>
            <p>To optimize the regularized linear regression model using gradient descent, the algorithm is adjusted as follows:</p>
            <p>
            <div>
                <pre><code class="language-shell">while not converged:
  for j in [0, ..., n]:
      Î¸_j := Î¸_j - Î± [ \frac{1}{m} \sum_{i=1}^{m}(h_{Î¸}(x^{(i)}) + y^{(i)})x_j^{(i)} + \frac{Î»}{m} Î¸_j ]</code></pre>
            </div>
            </p>
            <h4 id="regularization-with-the-normal-equation">Regularization with the Normal Equation</h4>
            <p>In the normal equation approach for regularized linear regression, the optimal $Î¸$ is computed as follows:</p>
            <p><img alt="regularized_normal_equation" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/regularized_normal_equation.png" /></p>
            <p>The equation includes an additional term $Î»I$ to the matrix being inverted, ensuring regularization is accounted for in the solution.</p>
            <h3 id="regularized-logistic-regression">Regularized Logistic Regression</h3>
            <p>The cost function for logistic regression with regularization is:</p>
            <p>$$
                J(Î¸) = \frac{1}{m} \sum_{i=1}^{m}[-y^{(i)}\log(h_{Î¸}(x^{(i)})) - (1-y^{(i)})\log(1 - h_{Î¸}(x^{(i)}))] + \frac{Î»}{2m}\sum_{j=1}^{n}Î¸_j^2
                $$</p>
            <h4 id="gradient-of-the-cost-function">Gradient of the Cost Function</h4>
            <p>The gradient is defined for each parameter $Î¸_j$:</p>
            <p>I. For $j = 0$ (no regularization on $Î¸_0$):</p>
            <p>$$
                \frac{\partial}{\partial Î¸_0} J(Î¸) = \frac{1}{m} \sum_{i=1}^{m} (h_{Î¸}(x^{(i)}) - y^{(i)})x_j^{(i)}
                $$</p>
            <p>II. For $j â‰¥ 1$ (includes regularization):</p>
            <p>$$
                \frac{\partial}{\partial Î¸_j} J(Î¸) = ( \frac{1}{m} \sum_{i=1}^{m} (h_{Î¸}(x^{(i)}) - y^{(i)})x_j^{(i)} ) + \frac{Î»}{m}Î¸_j
                $$</p>
            <h4 id="optimization">Optimization</h4>
            <p>For both linear and logistic regression, the gradient descent algorithm is updated to include regularization:</p>
            <p>
            <div>
                <pre><code class="language-shell">while not converged:
  for j in [0, ..., n]:
      Î¸_j := Î¸_j - Î± [ \frac{1}{m} \sum_{i=1}^{m}(h_{Î¸}(x^{(i)}) + y^{(i)})x_j^{(i)} + \frac{Î»}{m} Î¸_j ]</code></pre>
            </div>
            </p>
            <p>The key difference in logistic regression lies in the hypothesis function $h_{Î¸}(x)$, which is based on the logistic (sigmoid) function.</p>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol><a href="#regularization">Regularization</a>
                <ol>
                    <li><a href="#overfitting-in-machine-learning">Overfitting in Machine Learning</a>
                        <ol>
                            <li><a href="#example-overfitting-in-logistic-regression">Example: Overfitting in Logistic Regression</a></li>
                        </ol>
                    </li>
                    <li><a href="#regularization-in-cost-function">Regularization in Cost Function</a>
                        <ol>
                            <li><a href="#regularization-in-linear-regression">Regularization in Linear Regression</a></li>
                        </ol>
                    </li>
                    <li><a href="#regularization-parameter-lambda-">Regularization Parameter: $\lambda$</a></li>
                    <li><a href="#modifying-gradient-descent">Modifying Gradient Descent</a></li>
                    <li><a href="#regularized-linear-regression">Regularized Linear Regression</a>
                        <ol>
                            <li><a href="#gradient-descent-with-regularization">Gradient Descent with Regularization</a></li>
                            <li><a href="#regularization-with-the-normal-equation">Regularization with the Normal Equation</a></li>
                        </ol>
                    </li>
                    <li><a href="#regularized-logistic-regression">Regularized Logistic Regression</a>
                        <ol>
                            <li><a href="#gradient-of-the-cost-function">Gradient of the Cost Function</a></li>
                            <li><a href="#optimization">Optimization</a></li>
                        </ol>
                    </li>
                </ol>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/01_introduction_to_machine_learning.html">Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/02_linear_regression.html">Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/03_review_of_linear_algebra.html">Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/04_linear_regression_multiple_variables.html">Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/06_logistic_regression.html">Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/07_regularization.html">Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/08_neural_networks_representation.html">Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/09_neural_networks_learning.html">Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/10_applying_machine_learning_advice.html">Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/11_machine_learning_system_design.html">Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/12_support_vector_machines.html">Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/13_clustering.html">Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/14_dimensionality_reduction.html">Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/15_anomaly_detection.html">Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/16_recommendation_systems.html">Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/17_large_scale_machine_learning.html">Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/18_photo_ocr.html">Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>