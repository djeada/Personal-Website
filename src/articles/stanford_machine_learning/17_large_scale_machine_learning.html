<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Large Scale Machine Learning</title>
    <meta content="Training machine learning models on large datasets poses significant challenges due to the computational intensity involved." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <div class="article-action-buttons"><button class="btn-suggest-edit" title="Suggest Edit">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M4 21h4l11-11-4-4L4 17v4z" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button><button class="btn-create-issue" title="Create Issue">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"></circle>
                        <line stroke="currentColor" stroke-width="2" x1="12" x2="12" y1="8" y2="12"></line>
                        <circle cx="12" cy="16" fill="currentColor" r="1"></circle>
                    </svg>
                </button><button class="btn-download" title="Download">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M12 5v14m0 0l-6-6m6 6l6-6" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button></div>
            <p style="text-align: right;"><i>Last modified: April 25, 2025</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="large-scale-machine-learning">Large Scale Machine Learning</h2>
            <p>Training machine learning models on large datasets poses significant challenges due to the computational intensity involved. To effectively handle this, various techniques such as stochastic gradient descent and online learning are employed. Let's delve into these methods and understand how they facilitate large-scale learning.</p>
            <h3 id="learning-with-large-datasets">Learning with Large Datasets</h3>
            <p>To achieve high performance, one effective approach is using a low bias algorithm on a massive dataset. For example:</p>
            <ul>
                <li><strong>Scenario</strong>: Consider a dataset with <code>m = 100,000,000</code> examples.</li>
                <li><strong>Challenge</strong>: Training a model like logistic regression on this scale requires significant computation for each gradient descent step.</li>
            </ul>
            <p>Logistic Regression Update Rule:</p>
            <p>$$
                \theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m(h_{\theta}(x^{(i)}) - y^{(i)}) x_j^{(i)}
                $$</p>
            <ul>
                <li><strong>Approach</strong>: Experiment with a smaller sample (say, 1000 examples) to test performance.</li>
            </ul>
            <p><img alt="Learning Curve Example" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/learning_curve.png" /></p>
            <p>Diagnosing Bias vs. Variance: </p>
            <ul>
                <li>A large gap between training and cross-validation errors indicates high variance, suggesting more data might help.</li>
                <li>A small gap implies high bias, where additional data may not improve performance.</li>
            </ul>
            <h3 id="stochastic-gradient-descent-sgd-">Stochastic Gradient Descent (SGD)</h3>
            <p>SGD optimizes the learning process for large datasets by updating parameters more frequently:</p>
            <ul>
                <li>
                    <p><strong>Batch Gradient Descent</strong>: Traditional method that sums gradient terms over all examples before updating parameters. It's inefficient for large datasets.</p>
                </li>
                <li>
                    <p><strong>SGD Approach</strong>: Randomly shuffle the training examples. Update $\theta_j$ for each example:</p>
                </li>
            </ul>
            <p>$$
                \theta_j := \theta_j - \alpha (h_{\theta}(x^{(i)}) - y^{(i)}) x_j^{(i)}
                $$</p>
            <ul>
                <li>This results in parameter updates for every training example, rather than at the end of a full pass over the data.</li>
            </ul>
            <p><img alt="Stochastic Gradient Descent Illustration" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/stochastic.png" /></p>
            <ul>
                <li><strong>Convergence Observation</strong>: Observe the cost function versus the number of iterations.</li>
            </ul>
            <p><img alt="SGD Convergence" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/stochastic_convergence.png" /></p>
            <h3 id="online-learning">Online Learning</h3>
            <ul>
                <li><strong>Concept</strong>: Online learning is a dynamic form of learning where the model updates its parameters as new data arrives.</li>
                <li><strong>Use Case</strong>: Ideal for continuously evolving datasets or when data comes in streams.</li>
                <li><strong>Advantage</strong>: Allows the model to adapt to new trends or changes in the data over time.</li>
            </ul>
            <h3 id="example-product-search-on-a-cellphone-website">Example: Product Search on a Cellphone Website</h3>
            <p>Imagine a cellphone-selling website scenario:</p>
            <ul>
                <li><strong>User Queries</strong>: Users enter queries like "Android phone 1080p camera."</li>
                <li><strong>Ranking Phones</strong>: The objective is to present the user with a list of ten phones, ranked according to their relevance or appeal.</li>
                <li><strong>Feature Vectors</strong>: Generate a feature vector (x) for each phone, tailored to the userâ€™s specific query.</li>
                <li><strong>Click Prediction (CTR)</strong>: Learn the probability $p(y = 1 | x; \theta)$, where $y = 1$ if a user clicks on a phone link, and $y = 0$ otherwise. This probability represents the predicted click-through rate (CTR).</li>
                <li><strong>Utilizing CTR</strong>: Rank and display phones based on their estimated CTR, showing the most likely to be clicked options first.</li>
            </ul>
            <h3 id="online-learning">Online Learning</h3>
            <ul>
                <li><strong>Concept</strong>: Online learning is a dynamic form of learning where the model updates its parameters as new data arrives.</li>
                <li><strong>Use Case</strong>: Ideal for continuously evolving datasets or when data comes in streams.</li>
                <li><strong>Advantage</strong>: Allows the model to adapt to new trends or changes in the data over time.</li>
            </ul>
            <h3 id="example-product-search-on-a-cellphone-website">Example: Product Search on a Cellphone Website</h3>
            <p>Imagine a cellphone-selling website scenario:</p>
            <ul>
                <li><strong>User Queries</strong>: Users enter queries like "Android phone 1080p camera."</li>
                <li><strong>Ranking Phones</strong>: The objective is to present the user with a list of ten phones, ranked according to their relevance or appeal.</li>
                <li><strong>Feature Vectors</strong>: Generate a feature vector (x) for each phone, tailored to the userâ€™s specific query.</li>
                <li><strong>Click Prediction (CTR)</strong>: Learn the probability $p(y = 1 | x; \theta)$, where $y = 1$ if a user clicks on a phone link, and $y = 0$ otherwise. This probability represents the predicted click-through rate (CTR).</li>
                <li><strong>Utilizing CTR</strong>: Rank and display phones based on their estimated CTR, showing the most likely to be clicked options first.</li>
            </ul>
            <p>Here's a mock implementation of a Online Learning using Python:</p>
            <h4 id="step-1-data-collection">Step 1: Data Collection</h4>
            <p>Collect data on user queries and clicks on phone links. For simplicity, let's assume the data looks like this:</p>
            <p>
            <table>
                <tr>
                    <td>User Query</td>
                    <td>Phone ID</td>
                    <td>Features (x)</td>
                    <td>Click (y)</td>
                </tr>
                <tr>
                    <td>"Android phone 1080p camera"</td>
                    <td>1</td>
                    <td>[0.8, 0.2, 0.9, 0.4]</td>
                    <td>1</td>
                </tr>
                <tr>
                    <td>"Android phone 1080p camera"</td>
                    <td>2</td>
                    <td>[0.6, 0.4, 0.8, 0.5]</td>
                    <td>0</td>
                </tr>
                <tr>
                    <td>"Cheap iPhone"</td>
                    <td>3</td>
                    <td>[0.3, 0.9, 0.5, 0.7]</td>
                    <td>1</td>
                </tr>
                <tr>
                    <td>...</td>
                    <td>...</td>
                    <td>...</td>
                    <td>...</td>
                </tr>
            </table>
            </p>
            <h4 id="step-2-feature-extraction">Step 2: Feature Extraction</h4>
            <p>Extract feature vectors for each phone based on user queries. These features can include attributes like price, camera quality, battery life, etc.</p>
            <h4 id="step-3-model-initialization">Step 3: Model Initialization</h4>
            <p>Initialize a simple logistic regression model for CTR prediction:</p>
            <p>
            <div>
                <pre><code class="language-python">from sklearn.linear_model import SGDClassifier
import numpy as np

# Initialize the model
model = SGDClassifier(loss='log', learning_rate='constant', eta0=0.01)

# Assume initial training data (for illustration purposes)
X_initial = np.array([[0.8, 0.2, 0.9, 0.4], [0.6, 0.4, 0.8, 0.5], [0.3, 0.9, 0.5, 0.7]])
y_initial = np.array([1, 0, 1])

# Initial training
model.partial_fit(X_initial, y_initial, classes=np.array([0, 1]))</code></pre>
            </div>
            </p>
            <h4 id="step-4-online-learning-process">Step 4: Online Learning Process</h4>
            <p>Update the model with new data as it arrives:</p>
            <p>
            <div>
                <pre><code class="language-python">def update_model(new_data):
    X_new = np.array([data['features'] for data in new_data])
    y_new = np.array([data['click'] for data in new_data])
    model.partial_fit(X_new, y_new)

# Example of new data arriving
new_data = [
    {'features': [0.7, 0.3, 0.85, 0.45], 'click': 1},
    {'features': [0.5, 0.5, 0.75, 0.55], 'click': 0},
]

# Update the model with new data
update_model(new_data)</code></pre>
            </div>
            </p>
            <h4 id="step-5-ranking-phones">Step 5: Ranking Phones</h4>
            <p>Predict the CTR for new user queries and rank the phones accordingly:</p>
            <p>
            <div>
                <pre><code class="language-python">def rank_phones(user_query, phones):
    # Extract features for each phone based on the user query
    feature_vectors = [phone['features'] for phone in phones]
    # Predict CTR for each phone
    predicted_ctr = model.predict_proba(feature_vectors)[:, 1]
    # Rank phones by predicted CTR
    ranked_phones = sorted(zip(phones, predicted_ctr), key=lambda x: x[1], reverse=True)
    return ranked_phones[:10]

# Example phones data
phones = [
    {'id': 1, 'features': [0.8, 0.2, 0.9, 0.4]},
    {'id': 2, 'features': [0.6, 0.4, 0.8, 0.5]},
    {'id': 3, 'features': [0.3, 0.9, 0.5, 0.7]},
    # More phones...
]

# Rank phones for a given user query
top_phones = rank_phones("Android phone 1080p camera", phones)
print(top_phones)</code></pre>
            </div>
            </p>
            <h3 id="map-reduce-in-large-scale-machine-learning">Map Reduce in Large Scale Machine Learning</h3>
            <p>Map Reduce is a programming model designed for processing large datasets across distributed clusters. It simplifies parallel computation on massive scales, making it a cornerstone for big data analytics and machine learning where traditional single-node processing falls short.</p>
            <p><img alt="Map Reduce Illustration" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/map_reduce.png" /></p>
            <h3 id="understanding-map-reduce">Understanding Map Reduce</h3>
            <p>Map Reduce works by breaking down the processing into two main phases: Map phase and Reduce phase.</p>
            <p>I. Map Phase:</p>
            <ul>
                <li><strong>Operation</strong>: This phase involves taking a large input and dividing it into smaller sub-problems. Each sub-problem is processed independently.</li>
                <li><strong>Function</strong>: The map function applies a specific operation to each sub-problem. For example, it might involve filtering data or sorting it.</li>
                <li><strong>Output</strong>: The result of the map function is a set of key-value pairs.</li>
            </ul>
            <p>II. Reduce Phase:</p>
            <ul>
                <li><strong>Operation</strong>: In this phase, the output from the map phase is combined or reduced into a smaller set of tuples.</li>
                <li><strong>Function</strong>: The reduce function aggregates the results to form a consolidated output.</li>
                <li><strong>Examples</strong>: Summation, counting, or averaging over large datasets.</li>
            </ul>
            <h3 id="implementation-of-map-reduce">Implementation of Map Reduce</h3>
            <p>One popular framework for implementing Map Reduce is Apache Hadoop, which handles large scale data processing across distributed clusters.</p>
            <p>Hadoop Ecosystem:</p>
            <ul>
                <li><strong>Hadoop Distributed File System (HDFS)</strong>: Splits data into blocks and distributes them across the cluster.</li>
                <li><strong>MapReduce Engine</strong>: Manages the processing by distributing tasks to nodes in the cluster, handling task failures, and managing communications.</li>
            </ul>
            <p>Process Flow:</p>
            <ol>
                <li><strong>Data Distribution</strong>: Data is split and distributed across the HDFS.</li>
                <li><strong>Job Submission</strong>: A MapReduce job is defined and submitted to the cluster.</li>
                <li><strong>Mapping</strong>: The map tasks process the data in parallel.</li>
                <li><strong>Shuffling</strong>: Data is shuffled and sorted between the map and reduce phases.</li>
                <li><strong>Reducing</strong>: Reduce tasks aggregate the results.</li>
                <li><strong>Output Collection</strong>: The final output is assembled and stored back in HDFS.</li>
            </ol>
            <p>Here's a mock implementation of a MapReduce job using Python and Hadoop Streaming:</p>
            <h4 id="step-1-data-distribution">Step 1: Data Distribution</h4>
            <p>Assume we have a text file <code>input.txt</code> containing the following data:</p>
            <p>
            <div>
                <pre><code class="language-shell">Hello world
Hello Hadoop
Hadoop is great</code></pre>
            </div>
            </p>
            <p>This data is stored in HDFS, which will handle data distribution.</p>
            <h4 id="step-2-job-submission">Step 2: Job Submission</h4>
            <p>We will create two Python scripts: one for the mapper and one for the reducer. These scripts will be used in the Hadoop Streaming job submission.</p>
            <h4 id="step-3-mapper-script">Step 3: Mapper Script</h4>
            <p><code>mapper.py</code>:</p>
            <p>
            <div>
                <pre><code class="language-python">#!/usr/bin/env python

import sys

# Read input line by line
for line in sys.stdin:
    # Remove leading and trailing whitespace
    line = line.strip()
    # Split the line into words
    words = line.split()
    # Output each word with a count of 1
    for word in words:
        print(f'{word}\t1')</code></pre>
            </div>
            </p>
            <p>Make sure the script is executable:</p>
            <p>
            <div>
                <pre><code class="language-bash">chmod +x mapper.py</code></pre>
            </div>
            </p>
            <h4 id="step-4-shuffling">Step 4: Shuffling</h4>
            <p>The Hadoop framework handles the shuffling and sorting of data between the map and reduce phases. No additional code is needed for this step.</p>
            <h4 id="step-5-reducer-script">Step 5: Reducer Script</h4>
            <p><code>reducer.py</code>:</p>
            <p>
            <div>
                <pre><code class="language-python">#!/usr/bin/env python

import sys

current_word = None
current_count = 0
word = None

# Read input line by line
for line in sys.stdin:
    # Remove leading and trailing whitespace
    line = line.strip()
    # Parse the input we got from mapper.py
    word, count = line.split('\t', 1)
    # Convert count (currently a string) to int
    try:
        count = int(count)
    except ValueError:
        # Count was not a number, so silently ignore/discard this line
        continue
    # This IF-switch only works because Hadoop sorts map output
    # by key (here: word) before it is passed to the reducer
    if current_word == word:
        current_count += count
    else:
        if current_word:
            # Write result to stdout
            print(f'{current_word}\t{current_count}')
        current_word = word
        current_count = count

# Do not forget to output the last word if needed
if current_word == word:
    print(f'{current_word}\t{current_count}')</code></pre>
            </div>
            </p>
            <p>Make sure the script is executable:</p>
            <p>
            <div>
                <pre><code class="language-bash">chmod +x reducer.py</code></pre>
            </div>
            </p>
            <h4 id="step-6-job-submission-and-execution">Step 6: Job Submission and Execution</h4>
            <p>Submit the MapReduce job to the Hadoop cluster using the Hadoop Streaming utility:</p>
            <p>
            <div>
                <pre><code class="language-bash">hadoop jar /path/to/hadoop-streaming.jar \
  -input /path/to/input/files \
  -output /path/to/output/dir \
  -mapper /path/to/mapper.py \
  -reducer /path/to/reducer.py \
  -file /path/to/mapper.py \
  -file /path/to/reducer.py</code></pre>
            </div>
            </p>
            <h4 id="example-job-submission">Example Job Submission</h4>
            <p>Assuming the input file is stored in HDFS at <code>/user/hadoop/input/input.txt</code> and the output directory is <code>/user/hadoop/output/</code>, the job submission would look like this:</p>
            <p>
            <div>
                <pre><code class="language-bash">hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
  -input /user/hadoop/input/input.txt \
  -output /user/hadoop/output/ \
  -mapper mapper.py \
  -reducer reducer.py \
  -file mapper.py \
  -file reducer.py</code></pre>
            </div>
            </p>
            <h3 id="advantages-of-map-reduce">Advantages of Map Reduce</h3>
            <ul>
                <li><strong>Scalability</strong>: Can handle petabytes of data by distributing tasks across numerous machines.</li>
                <li><strong>Fault Tolerance</strong>: Automatically handles failures. If a node fails, tasks are rerouted to other nodes.</li>
                <li><strong>Flexibility</strong>: Capable of processing structured, semi-structured, and unstructured data.</li>
            </ul>
            <h3 id="applications-in-machine-learning">Applications in Machine Learning</h3>
            <ul>
                <li><strong>Large-scale Data Processing</strong>: For training models on datasets that are too large for a single machine, especially for tasks like clustering, classification, and pattern recognition.</li>
                <li><strong>Parallel Training</strong>: Training multiple models or model parameters in parallel, reducing the overall training time.</li>
                <li><strong>Data Preprocessing</strong>: Large-scale data cleaning, transformation, and feature extraction.</li>
            </ul>
            <h3 id="forms-of-parallelization">Forms of Parallelization</h3>
            <ul>
                <li><strong>Multiple Machines</strong>: Distribute the data and computations across several machines.</li>
                <li><strong>Multiple CPUs</strong>: Utilize several CPUs within a machine.</li>
                <li><strong>Multiple Cores</strong>: Take advantage of multi-core processors for parallel processing.</li>
            </ul>
            <h3 id="local-and-distributed-parallelization">Local and Distributed Parallelization</h3>
            <ul>
                <li><strong>Numerical Linear Algebra Libraries</strong>: Some libraries can automatically parallelize computations across multiple cores.</li>
                <li><strong>Vectorization</strong>: With an efficient vectorization implementation, local libraries might handle much of the optimization, reducing the need for manual parallelization.</li>
                <li><strong>Distributed Systems</strong>: For data that's too large for a single system, distributed computing frameworks like Hadoop are used. These frameworks apply the Map Reduce paradigm to process data across multiple machines.</li>
            </ul>
            <h2 id="reference">Reference</h2>
            <p>These notes are based on the free video lectures offered by Stanford University, led by Professor Andrew Ng. These lectures are part of the renowned Machine Learning course available on Coursera. For more information and to access the full course, visit the <a href="https://www.coursera.org/learn/machine-learning">Coursera course page</a>.</p>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#large-scale-machine-learning">Large Scale Machine Learning</a>
                    <ol>
                        <li><a href="#learning-with-large-datasets">Learning with Large Datasets</a></li>
                        <li><a href="#stochastic-gradient-descent-sgd-">Stochastic Gradient Descent (SGD)</a></li>
                        <li><a href="#online-learning">Online Learning</a></li>
                        <li><a href="#example-product-search-on-a-cellphone-website">Example: Product Search on a Cellphone Website</a></li>
                        <li><a href="#online-learning">Online Learning</a></li>
                        <li><a href="#example-product-search-on-a-cellphone-website">Example: Product Search on a Cellphone Website</a>
                            <ol>
                                <li><a href="#step-1-data-collection">Step 1: Data Collection</a></li>
                                <li><a href="#step-2-feature-extraction">Step 2: Feature Extraction</a></li>
                                <li><a href="#step-3-model-initialization">Step 3: Model Initialization</a></li>
                                <li><a href="#step-4-online-learning-process">Step 4: Online Learning Process</a></li>
                                <li><a href="#step-5-ranking-phones">Step 5: Ranking Phones</a></li>
                            </ol>
                        </li>
                        <li><a href="#map-reduce-in-large-scale-machine-learning">Map Reduce in Large Scale Machine Learning</a></li>
                        <li><a href="#understanding-map-reduce">Understanding Map Reduce</a></li>
                        <li><a href="#implementation-of-map-reduce">Implementation of Map Reduce</a>
                            <ol>
                                <li><a href="#step-1-data-distribution">Step 1: Data Distribution</a></li>
                                <li><a href="#step-2-job-submission">Step 2: Job Submission</a></li>
                                <li><a href="#step-3-mapper-script">Step 3: Mapper Script</a></li>
                                <li><a href="#step-4-shuffling">Step 4: Shuffling</a></li>
                                <li><a href="#step-5-reducer-script">Step 5: Reducer Script</a></li>
                                <li><a href="#step-6-job-submission-and-execution">Step 6: Job Submission and Execution</a></li>
                                <li><a href="#example-job-submission">Example Job Submission</a></li>
                            </ol>
                        </li>
                        <li><a href="#advantages-of-map-reduce">Advantages of Map Reduce</a></li>
                        <li><a href="#applications-in-machine-learning">Applications in Machine Learning</a></li>
                        <li><a href="#forms-of-parallelization">Forms of Parallelization</a></li>
                        <li><a href="#local-and-distributed-parallelization">Local and Distributed Parallelization</a></li>
                    </ol>
                </li>
                <li><a href="#reference">Reference</a></li>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/01_introduction_to_machine_learning.html">Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/02_linear_regression.html">Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/03_review_of_linear_algebra.html">Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/04_linear_regression_multiple_variables.html">Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/06_logistic_regression.html">Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/07_regularization.html">Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/08_neural_networks_representation.html">Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/09_neural_networks_learning.html">Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/10_applying_machine_learning_advice.html">Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/11_machine_learning_system_design.html">Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/12_support_vector_machines.html">Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/13_clustering.html">Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/14_dimensionality_reduction.html">Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/15_anomaly_detection.html">Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/16_recommendation_systems.html">Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/17_large_scale_machine_learning.html">Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/18_photo_ocr.html">Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
    <div id="pdf-spinner-overlay">
        <div class="spinner"></div>
    </div>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>