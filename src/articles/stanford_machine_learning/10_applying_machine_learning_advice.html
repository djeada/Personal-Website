<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Advice for Applying Machine Learning Techniques and Debugging Learning Algorithms</title>
    <meta content="When facing high error rates with a machine learning model, especially when tested on new data, various strategies can be employed to diagnose and address the problem." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <div class="article-action-buttons"><button class="btn-suggest-edit" title="Suggest Edit">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M4 21h4l11-11-4-4L4 17v4z" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button><button class="btn-create-issue" title="Create Issue">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"></circle>
                        <line stroke="currentColor" stroke-width="2" x1="12" x2="12" y1="8" y2="12"></line>
                        <circle cx="12" cy="16" fill="currentColor" r="1"></circle>
                    </svg>
                </button><button class="btn-download" title="Download">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M12 5v14m0 0l-6-6m6 6l6-6" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button></div>
            <p style="text-align: right;"><i>Last modified: April 25, 2025</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="advice-for-applying-machine-learning-techniques-and-debugging-learning-algorithms">Advice for Applying Machine Learning Techniques and Debugging Learning Algorithms</h2>
            <h3 id="troubleshooting-high-error-rates">Troubleshooting High Error Rates</h3>
            <p>When facing high error rates with a machine learning model, especially when tested on new data, various strategies can be employed to diagnose and address the problem.</p>
            <h4 id="steps-for-improvement">Steps for Improvement</h4>
            <ol>
                <li><strong>Add More Training Data:</strong> Sometimes, the model has not seen enough examples to generalize well.</li>
                <li><strong>Add or Remove Features:</strong> The model might be missing key features that are important for predictions or might be overwhelmed by irrelevant or noisy features.</li>
                <li><strong>Adjust Regularization Parameter ($\lambda$):</strong> If the model is overfitting, increasing the regularization parameter can help. If underfitting, decreasing it might be beneficial.</li>
                <li><strong>Data Splitting:</strong> Divide your dataset into a training set and a test set to assess model performance on unseen data.</li>
                <li><strong>Model Selection with Validation Set:</strong> Create a training, validation, and test set to tune and select the best model.</li>
                <li><strong>Polynomial Degree Analysis:</strong> Plot errors for training and validation sets against polynomial degrees to diagnose high bias (underfitting) or high variance (overfitting).</li>
                <li><strong>Advanced Optimization Algorithms:</strong> Employ sophisticated optimization techniques to minimize the cost function more effectively.</li>
            </ol>
            <h3 id="debugging-a-learning-algorithm">Debugging a Learning Algorithm</h3>
            <p>Consider a scenario where you have used regularized linear regression to predict home prices, but the model yields high errors on new data.</p>
            <h4 id="cost-function">Cost Function</h4>
            <p>Use the cost function for the regularized linear regression:</p>
            <p>$$
                J(\theta) = \frac{1}{2m} \left[ \sum_{i=1}^{m}(h_{\theta}(x^{(i)} - y^{(i)})^2 + \lambda \sum_{j=1}^{m} \theta_j^2 \right]
                $$</p>
            <h4 id="next-steps-for-improvement">Next Steps for Improvement</h4>
            <ol>
                <li><strong>Obtain More Training Data:</strong> Helps the algorithm to better generalize.</li>
                <li><strong>Reduce Feature Set:</strong> Try using fewer features to avoid overfitting.</li>
                <li><strong>Incorporate More Features:</strong> Add new features that might be relevant.</li>
                <li><strong>Add Polynomial Features:</strong> Enables the model to capture more complex relationships.</li>
                <li><strong>Modify $\lambda$:</strong> Adjust the regularization parameter to find the right balance between bias and variance.</li>
            </ol>
            <h3 id="evaluating-the-hypothesis">Evaluating the Hypothesis</h3>
            <p>Evaluating the modelâ€™s performance involves splitting the data and computing errors:</p>
            <h4 id="data-split">Data Split</h4>
            <ul>
                <li><strong>Training Set:</strong> Used to learn the parameters, $\theta$.</li>
                <li><strong>Test Set:</strong> Used to compute the test error.</li>
            </ul>
            <p>Here is a simple implementation to split the data into a training set and a test set, train a logistic regression model on the training set, and evaluate it on the test set:</p>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

# Mock data
X = np.array([[0.5, 1.5],
              [1.5, 0.5],
              [3, 3.5],
              [2, 2.5],
              [1, 1],
              [3.5, 4],
              [2.5, 3],
              [1, 0.5]])
y = np.array([0, 0, 1, 1, 2, 2, 1, 0])  # 0: Triangle, 1: Cross, 2: Square

# Binary classification for simplicity (convert multi-class to binary problem)
y_binary = (y == 1).astype(int)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)

# Initialize the logistic regression model
model = LogisticRegression()

# Train the model on the training set
model.fit(X_train, y_train)

# Make predictions on the test set
y_test_pred = model.predict(X_test)

# Evaluate the model
test_accuracy = accuracy_score(y_test, y_test_pred)

print("Test Accuracy:", test_accuracy)
print("Test Predictions:", y_test_pred)
print("Actual Labels:", y_test)</code></pre>
            </div>
            </p>
            <h4 id="test-error-calculation">Test Error Calculation</h4>
            <p>Compute the test error using the learned parameters:</p>
            <p>$$J_{test}(\theta) = \frac{1}{2m_{test}} \sum_{i=1}^{m_{test}}(h_{\theta}(x^{(i)}<em>{test} - y^{(i)}</em>{test})^2$$</p>
            <p>This step helps to evaluate how well the model generalizes to new, unseen data.</p>
            <p>To compute the test error for a logistic regression model using the mean squared error (MSE) formula provided, you can follow these steps:</p>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Hypothesis function
def hypothesis(theta, X):
    return sigmoid(np.dot(X, theta))

# Test error function
def compute_test_error(theta, X_test, y_test):
    m_test = len(y_test)
    h = hypothesis(theta, X_test)
    error = (1 / (2 * m_test)) * np.sum((h - y_test) ** 2)
    return error

# Example usage
if __name__ == "__main__":
    # Sample data
    X = np.array([[0.5, 1.5],
                  [1.5, 0.5],
                  [3, 3.5],
                  [2, 2.5],
                  [1, 1],
                  [3.5, 4],
                  [2.5, 3],
                  [1, 0.5]])
    y = np.array([0, 0, 1, 1, 2, 2, 1, 0])  # 0: Triangle, 1: Cross, 2: Square

    # Binary classification for simplicity (convert multi-class to binary problem)
    y_binary = (y == 1).astype(int)

    # Split data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)

    # Initialize the logistic regression model
    model = LogisticRegression()

    # Train the model on the training set
    model.fit(X_train, y_train)

    # Get the learned parameters
    theta = np.hstack([model.intercept_, model.coef_[0]])

    # Add intercept term to test set
    X_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])

    # Compute the test error
    test_error = compute_test_error(theta, X_test, y_test)

    print("Test Error:", test_error)</code></pre>
            </div>
            </p>
            <h3 id="model-selection-process">Model Selection Process</h3>
            <p>When applying machine learning algorithms, selecting the right model and parameters is crucial. This includes choosing the regularization parameter or the polynomial degree for regression models. The challenge is to balance between underfitting (high bias) and overfitting (high variance).</p>
            <ol>
                <li><strong>Determine Polynomial Degree (<code>d</code>):</strong> You may want to choose between different degrees of polynomial models, ranging from linear ($d=1$) to higher-degree polynomials.</li>
            </ol>
            <p>For example, models can range from $h_{\theta}(x) = \theta_0 + \theta_1x$ to $h_{\theta}(x) = \theta_0 + ... + \theta_{10}x^{10}$.</p>
            <ol>
                <li>
                    <p><strong>Train Models:</strong> Train each model on the training dataset to obtain the parameter vector $\theta^d$ for each degree $d$.</p>
                </li>
                <li>
                    <p><strong>Compute Test Set Error:</strong> Use $J_{test}(\theta^d)$ to evaluate the error on the test set for each model.</p>
                </li>
                <li>
                    <p><strong>Cross-Validation:</strong> Test the models on a separate cross-validation set and compute the cross-validation error for each.</p>
                </li>
                <li>
                    <p><strong>Model Selection:</strong> Select the model with the lowest cross-validation error.</p>
                </li>
            </ol>
            <h3 id="error-metrics">Error Metrics</h3>
            <p>I. Training Error:</p>
            <p>$$
                J_{train}(\theta) = \frac{1}{2m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)} - y^{(i)})^2
                $$</p>
            <p>II. Cross-Validation Error:</p>
            <p>$$
                J_{cv}(\theta) = \frac{1}{2m_{cv}} \sum_{i=1}^{m_{cv}}(h_{\theta}(x^{(i)}<em>{cv} - y^{(i)}</em>{cv})^2
                $$</p>
            <p>III. Test Error:</p>
            <p>$$
                J_{test}(\theta) = \frac{1}{2m_{test}} \sum_{i=1}^{m_{test}}(h_{\theta}(x^{(i)}<em>{test} - y^{(i)}</em>{test})^2
                $$</p>
            <p>Let's create a mock example in Python to demonstrate these error metrics. We'll use synthetic data for simplicity:</p>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Split the data into training, cross-validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_cv, X_test, y_cv, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Define a simple linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict on training, cross-validation, and test sets
y_train_pred = model.predict(X_train)
y_cv_pred = model.predict(X_cv)
y_test_pred = model.predict(X_test)

# Calculate the error metrics
def calculate_error(y_true, y_pred):
    m = len(y_true)
    return (1/(2*m)) * np.sum((y_pred - y_true)**2)

J_train = calculate_error(y_train, y_train_pred)
J_cv = calculate_error(y_cv, y_cv_pred)
J_test = calculate_error(y_test, y_test_pred)

print(f'Training Error: {J_train}')
print(f'Cross-Validation Error: {J_cv}')
print(f'Test Error: {J_test}')</code></pre>
            </div>
            </p>
            <p>This example demonstrates how to compute and compare the training, cross-validation, and test errors for a machine learning model.</p>
            <h3 id="diagnosing-bias-vs-variance">Diagnosing Bias vs. Variance</h3>
            <p>Diagnosing the nature of the error (high bias or high variance) can guide you in improving your model.</p>
            <h4 id="visual-representation">Visual Representation</h4>
            <p><img alt="Bias vs Variance Diagnosis" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/diagnosis.png" /></p>
            <h4 id="plotting-error-vs-polynomial-degree">Plotting Error vs. Polynomial Degree</h4>
            <p>By plotting the training and cross-validation errors against the degree of the polynomial, you can visually assess the nature of the problem:</p>
            <p><img alt="Error vs Polynomial Degree" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/error_vs_d.png" /></p>
            <ul>
                <li>
                    <p><strong>High Bias (Underfitting):</strong> Both training and cross-validation errors are high. The model is too simple and does not capture the underlying trend in the data well.</p>
                </li>
                <li>
                    <p><strong>High Variance (Overfitting):</strong> Low training error but high cross-validation error. The model is too complex and captures noise in the training data, failing to generalize well.</p>
                </li>
            </ul>
            <h3 id="regularized-linear-regression-model">Regularized Linear Regression Model</h3>
            <p>Consider a high-order polynomial linear regression model:</p>
            <p>$$h_{\theta}(x) = \theta_0 + \theta_1x + \theta_2x^2 + \theta_3x^3 + \theta_4x^4$$</p>
            <p>The regularized cost function for this model is:</p>
            <p>$$J(\theta) = \frac{1}{2m} \left[ \sum_{i=1}^{m}(h_{\theta}(x^{(i)} - y^{(i)})^2 + \lambda \sum_{j=1}^{n} \theta_j^2 \right]$$</p>
            <ul>
                <li><strong>$\lambda$:</strong> Regularization parameter controlling the degree of regularization.</li>
                <li><strong>$\theta_j$:</strong> Coefficients of the polynomial terms.</li>
                <li><strong>$m$:</strong> Number of training examples.</li>
                <li><strong>$n$:</strong> Number of features (polynomial terms in this case).</li>
            </ul>
            <h3 id="impact-of-the-regularization-parameter-lambda-">Impact of the Regularization Parameter ($\lambda$)</h3>
            <p>The choice of $\lambda$ has a significant impact on the model's performance:</p>
            <ul>
                <li><strong>High $\lambda$ (High Bias):</strong> Leads to underfitting as it overly penalizes the coefficients, resulting in a too simple model.</li>
                <li><strong>Intermediate $\lambda$:</strong> Balances between bias and variance, often yielding a well-performing model.</li>
                <li><strong>Small $\lambda$ (High Variance):</strong> Leads to overfitting as it does not sufficiently penalize the coefficients, allowing the model to fit too closely to the training data.</li>
            </ul>
            <p><img alt="Effect of Lambda on Regularization" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/lambda.png" /></p>
            <p>Try a range of $\lambda$ values, for each value minimize $J(\theta)$ to find $\theta^{(i)}$, and then compute the average squared error on the cross-validation set. Choose the $\lambda$ that results in the lowest error.</p>
            <h3 id="learning-curves">Learning Curves</h3>
            <p>Learning curves plot the training error and cross-validation error against the number of training examples. They help diagnose bias and variance issues.</p>
            <p>Plotting Learning Curves:</p>
            <ul>
                <li><strong>$J_{train}$ (Training Error):</strong> Average squared error on the training set.</li>
                <li><strong>$J_{cv}$ (Cross-Validation Error):</strong> Average squared error on the cross-validation set.</li>
                <li><strong>$m$:</strong> Number of training examples.</li>
            </ul>
            <p><img alt="Learning Curve Visualization" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/learning_curve.png" /></p>
            <ul>
                <li><strong>Small Training Sets:</strong> $J_{train}$ tends to be lower because the model can fit small datasets well.</li>
                <li><strong>As $m$ Increases:</strong> The model generalizes better, so $J_{cv}$ decreases.</li>
            </ul>
            <h4 id="diagnosing-bias-vs-variance-from-learning-curves">Diagnosing Bias vs. Variance from Learning Curves</h4>
            <ul>
                <li><strong>High Bias (Underfitting):</strong> Both $J_{train}$ and $J_{cv}$ are high, and adding more training data does not significantly improve performance.</li>
                <li><strong>High Variance (Overfitting):</strong> $J_{train}$ is low, but $J_{cv}$ is much higher. Increasing the training set size can help the model generalize better.</li>
            </ul>
            <p>We'll use linear regression for simplicity, but the process can be adapted to other models, including logistic regression.</p>
            <p>Here's the implementation:</p>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import log_loss
from sklearn.model_selection import train_test_split

# Function to compute training and cross-validation errors
def learning_curves(X, y, X_val, y_val, model):
    m = len(y)
    train_errors = []
    val_errors = []
    
    for i in range(1, m + 1):
        # Train the model on the first i training examples
        model.fit(X[:i], y[:i])
        
        # Compute training error
        y_train_predict = model.predict_proba(X[:i])[:, 1]
        train_error = log_loss(y[:i], y_train_predict)
        train_errors.append(train_error)
        
        # Compute cross-validation error
        y_val_predict = model.predict_proba(X_val)[:, 1]
        val_error = log_loss(y_val, y_val_predict)
        val_errors.append(val_error)
    
    return train_errors, val_errors

# Plotting function
def plot_learning_curves(train_errors, val_errors):
    plt.plot(np.arange(1, len(train_errors) + 1), train_errors, label='Training Error')
    plt.plot(np.arange(1, len(train_errors) + 1), val_errors, label='Cross-Validation Error')
    plt.xlabel('Number of Training Examples')
    plt.ylabel('Error')
    plt.title('Learning Curves')
    plt.legend()
    plt.grid(True)
    plt.show()

# Example usage
if __name__ == "__main__":
    # Sample data (features and labels)
    X = np.array([[0.5, 1.5],
                  [1.5, 0.5],
                  [3, 3.5],
                  [2, 2.5],
                  [1, 1],
                  [3.5, 4],
                  [2.5, 3],
                  [1, 0.5]])
    y = np.array([0, 0, 1, 1, 2, 2, 1, 0])  # 0: Triangle, 1: Cross, 2: Square
    
    # Binary classification for simplicity (convert multi-class to binary problem)
    y_binary = (y == 1).astype(int)
    
    # Split data into training and cross-validation sets
    X_train, X_val, y_train, y_val = train_test_split(X, y_binary, test_size=0.3, random_state=42)
    
    # Initialize the logistic regression model
    model = LogisticRegression()
    
    # Compute learning curves
    train_errors, val_errors = learning_curves(X_train, y_train, X_val, y_val, model)
    
    # Plot learning curves
    plot_learning_curves(train_errors, val_errors)</code></pre>
            </div>
            </p>
            <h2 id="reference">Reference</h2>
            <p>These notes are based on the free video lectures offered by Stanford University, led by Professor Andrew Ng. These lectures are part of the renowned Machine Learning course available on Coursera. For more information and to access the full course, visit the <a href="https://www.coursera.org/learn/machine-learning">Coursera course page</a>.</p>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#advice-for-applying-machine-learning-techniques-and-debugging-learning-algorithms">Advice for Applying Machine Learning Techniques and Debugging Learning Algorithms</a>
                    <ol>
                        <li><a href="#troubleshooting-high-error-rates">Troubleshooting High Error Rates</a>
                            <ol>
                                <li><a href="#steps-for-improvement">Steps for Improvement</a></li>
                            </ol>
                        </li>
                        <li><a href="#debugging-a-learning-algorithm">Debugging a Learning Algorithm</a>
                            <ol>
                                <li><a href="#cost-function">Cost Function</a></li>
                                <li><a href="#next-steps-for-improvement">Next Steps for Improvement</a></li>
                            </ol>
                        </li>
                        <li><a href="#evaluating-the-hypothesis">Evaluating the Hypothesis</a>
                            <ol>
                                <li><a href="#data-split">Data Split</a></li>
                                <li><a href="#test-error-calculation">Test Error Calculation</a></li>
                            </ol>
                        </li>
                        <li><a href="#model-selection-process">Model Selection Process</a></li>
                        <li><a href="#error-metrics">Error Metrics</a></li>
                        <li><a href="#diagnosing-bias-vs-variance">Diagnosing Bias vs. Variance</a>
                            <ol>
                                <li><a href="#visual-representation">Visual Representation</a></li>
                                <li><a href="#plotting-error-vs-polynomial-degree">Plotting Error vs. Polynomial Degree</a></li>
                            </ol>
                        </li>
                        <li><a href="#regularized-linear-regression-model">Regularized Linear Regression Model</a></li>
                        <li><a href="#impact-of-the-regularization-parameter-lambda-">Impact of the Regularization Parameter ($\lambda$)</a></li>
                        <li><a href="#learning-curves">Learning Curves</a>
                            <ol>
                                <li><a href="#diagnosing-bias-vs-variance-from-learning-curves">Diagnosing Bias vs. Variance from Learning Curves</a></li>
                            </ol>
                        </li>
                    </ol>
                </li>
                <li><a href="#reference">Reference</a></li>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/01_introduction_to_machine_learning.html">Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/02_linear_regression.html">Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/03_review_of_linear_algebra.html">Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/04_linear_regression_multiple_variables.html">Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/06_logistic_regression.html">Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/07_regularization.html">Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/08_neural_networks_representation.html">Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/09_neural_networks_learning.html">Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/10_applying_machine_learning_advice.html">Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/11_machine_learning_system_design.html">Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/12_support_vector_machines.html">Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/13_clustering.html">Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/14_dimensionality_reduction.html">Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/15_anomaly_detection.html">Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/16_recommendation_systems.html">Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/17_large_scale_machine_learning.html">Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/18_photo_ocr.html">Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
    <div id="pdf-spinner-overlay">
        <div class="spinner"></div>
    </div>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>