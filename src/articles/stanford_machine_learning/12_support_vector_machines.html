<!DOCTYPE html>

<html lang="en">
<head>
<script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
<meta charset="utf-8"/>
<title>Support Vector Machines: Mathematical Insights</title>
<meta content="Support Vector Machines (SVMs) are powerful tools in machine learning, and their formulation can be derived from logistic regression cost functions." name="description"/>
<meta content="Adam Djellouli" name="author"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet"/>
<link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon"/>
<link href="../../resources/style.css" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>
<body><nav aria-label="Main navigation">
<a class="logo" href="https://adamdjellouli.com">
<img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG"/>
</a>
<input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox"/>
<ul aria-labelledby="navbar-toggle" role="menu">
<li role="menuitem">
<a href="../../index.html" title="Go to Home Page"> Home </a>
</li>
<li role="menuitem">
<a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
</li>
<li role="menuitem">
<a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
</li>
<li role="menuitem">
<a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
</li>
<li role="menuitem">
<a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
</li>
<li>
<script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
<div class="gcse-search"></div>
</li>
<li>
<button aria-label="Toggle dark mode" id="dark-mode-button"></button>
</li>
</ul>
</nav>
<div id="article-wrapper"><article-section id="article-body">
<p style="text-align: right;"><i>Last modified: December 25, 2025</i></p>
<p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
<h2 id="support-vector-machines-mathematical-insights">Support Vector Machines: Mathematical Insights</h2>
<p>Support Vector Machines (SVMs) are powerful tools in machine learning, and their formulation can be derived from logistic regression cost functions. This article delves into the mathematical underpinnings of SVMs, starting with logistic regression and transitioning to the SVM framework.</p>
<h3 id="understanding-logistic-regression-cost-function">Understanding Logistic Regression Cost Function</h3>
<p>Logistic regression employs a hypothesis function defined as:</p>
<p>$$h_{\theta}(x) = \frac{1}{1 + e^{-\theta^T x}}$$</p>
<p>This function outputs the probability of $y = 1$ given $x$ and parameter $\theta$. </p>
<p><img alt="sigmoid" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/sigmoid2.png"/></p>
<p>For a given example where $y = 1$, ideally, $h_{\theta}(x)$ should be close to 1. The cost function for logistic regression is:</p>
<p>$$\text{Cost}(h_{\theta}(x), y) = -y \log(h_{\theta}(x)) - (1 - y) \log(1 - h_{\theta}(x))$$</p>
<p><img alt="log_function" src="https://user-images.githubusercontent.com/37275728/201519577-c93854b4-1270-4082-9d9b-da0d543b0375.png"/></p>
<p>When integrated over all samples, this becomes:</p>
<p>$$J(\theta) = -\frac{1}{m} \sum_{i=1}^{m}[y^{(i)}\log(h_{\theta}(x^{(i)})) + (1 - y^{(i)})\log(1 - h_{\theta}(x^{(i)}))]$$</p>
<p>This cost function penalizes incorrect predictions, with the penalty increasing exponentially as the prediction diverges from the true value.</p>
<h3 id="transitioning-to-svm-cost-functions">Transitioning to SVM Cost Functions</h3>
<p>SVMs modify this logistic cost function to a piecewise linear form. The idea is to use two functions, $cost_1(\theta^T x)$ for $y=1$ and $cost_0(\theta^T x)$ for $y=0$. These functions are linear approximations of the logistic function's cost. The SVM cost function becomes:</p>
<p>$$J(\theta) = C \sum_{i=1}^{m}[y^{(i)} cost_1(\theta^T x^{(i)}) + (1 - y^{(i)}) cost_0(\theta^T x^{(i)})] + \frac{1}{2} \sum_{j=1}^{m} \theta_j^2$$</p>
<p>The parameter $C$ plays a crucial role in SVMs. It balances the trade-off between the smoothness of the decision boundary and classifying the training points correctly.</p>
<ul>
<li>A large $C$ applies a higher penalty to misclassified examples, leading to a decision boundary that might have lower bias but higher variance, potentially causing overfitting.</li>
<li>A small $C$ makes the decision boundary smoother, possibly with higher bias but lower variance, risking underfitting.</li>
</ul>
<p><img alt="svm_cost" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/svm_cost.png"/></p>
<h2 id="large-margin-intuition-in-svms">Large Margin Intuition in SVMs</h2>
<p>Support Vector Machines (SVMs) are designed to find a decision boundary not just to separate the classes, but to do so with the largest possible margin. This large margin intuition is key to understanding SVMs.</p>
<p><img alt="large_dist" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/large_dist.png"/></p>
<h3 id="the-concept-of-margin-in-svms">The Concept of Margin in SVMs</h3>
<p>When we talk about SVMs, we often refer to minimizing a function of the form <code>CA + B</code>. Let's break this down:</p>
<ul>
<li><strong>Minimizing A</strong>: This part relates to minimizing the misclassification errors. If we set the parameter <code>C</code> to be very large, we prioritize minimizing these errors. In other words, if <code>C</code> is large, we aim to make <code>A</code> equal to zero. For this to happen:</li>
<li>If <code>y = 1</code>, we need to find a $\theta$ such that $\theta^Tx \geq 1<code>.</code></li>
<li>If <code>y = 0</code>, we need to find a $\theta$ such that $\theta^Tx \leq -1.</li>
<li><strong>Minimizing B</strong>: This part relates to maximizing the margin. The margin is the distance between the decision boundary and the nearest data point from either class. The optimization problem can be expressed as:</li>
</ul>
<p>$$
\begin{aligned}
\text{minimize}\ \frac{1}{2} \sum_{j=1}^{m} \theta_j^2 \\
\text{subject to}\ \theta^Tx^{(i)} \geq 1\ \text{if}\ y^{(i)}=1 \\
\theta^Tx^{(i)} \leq -1\ \text{if}\ y^{(i)}=0
\end{aligned}
$$</p>
<h3 id="visualization-of-large-margin-classification">Visualization of Large Margin Classification</h3>
<p>In the visualization, the green and magenta lines are potential decision boundaries that might be chosen by a method like logistic regression. However, these lines might not generalize well to new data. The black line, chosen by the SVM, represents a stronger separator. It is chosen because it maximizes the margin, the minimum distance to any of the training samples.</p>
<h3 id="understanding-the-decision-boundary-in-svms">Understanding the Decision Boundary in SVMs</h3>
<p>Assuming we have only two features and $\theta_0 = 0$, we can rewrite the minimization of <code>B</code> as:</p>
<p>$$\frac{1}{2}(\theta_1^2 + \theta_2^2) = \frac{1}{2}(\sqrt{\theta_1^2 + \theta_2^2})^2 = \frac{1}{2}||\theta||^2$$</p>
<p>This is essentially minimizing the norm of the vector $\theta$. Now, consider what $\theta^Tx$ represents in this context. If we have a positive training example and we plot $\theta$ on the same axis, we are interested in the inner product of these two vectors, denoted as <code>p</code>.</p>
<ul>
<li><code>p</code> is actually <code>p^i</code>, representing the projection of the training example <code>i</code> on the vector $\theta$.</li>
<li>The conditions for the classification become:</li>
</ul>
<p>$$
\begin{aligned}
p^{(i)} \cdot ||\theta|| &amp;\geq 1\ \text{if}\ y^{(i)}=1 \\
p^{(i)} \cdot ||\theta|| &amp;\leq -1\ \text{if}\ y^{(i)}=0
\end{aligned}
$$</p>
<p><img alt="svm_vectors" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/svm_vectors.png"/></p>
<h2 id="adapting-svm-to-non-linear-classifiers">Adapting SVM to Non-linear Classifiers</h2>
<p>Support Vector Machines (SVMs) can be adapted to find non-linear boundaries, which is crucial for handling datasets where the classes are not linearly separable. </p>
<h3 id="non-linear-classification">Non-linear Classification</h3>
<p>Consider a training set where a linear decision boundary is not sufficient. The goal is to find a non-linear boundary that can effectively separate the classes.</p>
<p><img alt="Example of a Non-linear Boundary" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/non_linear_boundary.png"/></p>
<h3 id="introducing-landmarks-and-features">Introducing Landmarks and Features</h3>
<ol>
<li>
<p><strong>Defining Landmarks</strong>: We start by defining landmarks in our feature space. Landmarks ($l^1$, $l^2$, and $l^3$) are specific points in the feature space, chosen either manually or by some heuristic.</p>
<p><img alt="Landmarks in Feature Space" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/landmarks.png"/></p>
</li>
<li>
<p><strong>Kernel Functions</strong>: A kernel is a function that computes the similarity between each feature $x$ and the landmarks. For example, the Gaussian kernel for a landmark $l^1$ is defined as:</p>
</li>
</ol>
<p>$$f_1 = k(x, l^1) = \exp\left(- \frac{||x - l^{(1)}||^2}{2\sigma^2}\right)$$</p>
<pre><div><pre><code class="language-shell">- A large $\sigma^2$ leads to smoother feature variation (higher bias, lower variance).
- A small $\sigma^2$ results in abrupt feature changes (low bias, high variance).</code></pre></div></pre>
<ol>
<li>
<p><strong>Prediction Example</strong>: Consider predicting the class of a new point (e.g., the magenta dot in the figure). Using our kernel functions and a specific set of $\theta$ values, we can compute the classification.</p>
<p><img alt="Evaluating a New Point" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/landmarks_magneta.png"/></p>
<p>For a point close to $l^1$, $f_1$ will be close to 1, and others will be closer to 0. Hence, for $\theta_0 = -0.5, \theta_1 = 1, \theta_2 = 1, \theta_3 = 0$, the prediction will be 1.</p>
</li>
</ol>
<h3 id="choosing-landmarks">Choosing Landmarks</h3>
<ul>
<li>Landmarks are often chosen to be the same as the training examples, resulting in <code>m</code> landmarks for <code>m</code> training examples.</li>
<li>Each example's feature set is evaluated based on its proximity to each landmark using the chosen kernel function.</li>
</ul>
<h3 id="different-kernels">Different Kernels</h3>
<ul>
<li><strong>Linear Kernel</strong>: Equivalent to no kernel, predicts $y = 1$ if $(\theta^T x) \geq 0$.</li>
<li><strong>Gaussian Kernel</strong>: Useful for creating complex non-linear boundaries.</li>
<li>Other types of kernels include Polynomial, String, Chi-squared, and Histogram Intersection kernels.</li>
</ul>
<h3 id="logistic-regression-vs-svm-in-non-linear-classification">Logistic Regression vs. SVM in Non-linear Classification</h3>
<ul>
<li><strong>High Feature Count (n) Compared to Examples (m)</strong>: Use logistic regression or SVM with a linear kernel.</li>
<li><strong>Small n, Intermediate m</strong>: Gaussian kernel with SVM is suitable.</li>
<li><strong>Small n, Large m</strong>: SVM with Gaussian kernel may be slow; logistic regression or SVM with a linear kernel is preferred.</li>
<li><strong>SVM's Power</strong>: The ability to use different kernels makes SVMs versatile for learning complex non-linear functions.</li>
<li><strong>Convex Optimization</strong>: SVMs provide a global minimum, ensuring consistency in the solution.</li>
</ul>
<h2 id="reference">Reference</h2>
<p>These notes are based on the free video lectures offered by Stanford University, led by Professor Andrew Ng. These lectures are part of the renowned Machine Learning course available on Coursera. For more information and to access the full course, visit the <a href="https://www.coursera.org/learn/machine-learning">Coursera course page</a>.</p>
</article-section><div id="table-of-contents"><h2>Table of Contents</h2><ol><li><a href="#support-vector-machines-mathematical-insights">Support Vector Machines: Mathematical Insights</a><ol><li><a href="#understanding-logistic-regression-cost-function">Understanding Logistic Regression Cost Function</a></li><li><a href="#transitioning-to-svm-cost-functions">Transitioning to SVM Cost Functions</a></li></ol></li><li><a href="#large-margin-intuition-in-svms">Large Margin Intuition in SVMs</a><ol><li><a href="#the-concept-of-margin-in-svms">The Concept of Margin in SVMs</a></li><li><a href="#visualization-of-large-margin-classification">Visualization of Large Margin Classification</a></li><li><a href="#understanding-the-decision-boundary-in-svms">Understanding the Decision Boundary in SVMs</a></li></ol></li><li><a href="#adapting-svm-to-non-linear-classifiers">Adapting SVM to Non-linear Classifiers</a><ol><li><a href="#non-linear-classification">Non-linear Classification</a></li><li><a href="#introducing-landmarks-and-features">Introducing Landmarks and Features</a></li><li><a href="#choosing-landmarks">Choosing Landmarks</a></li><li><a href="#different-kernels">Different Kernels</a></li><li><a href="#logistic-regression-vs-svm-in-non-linear-classification">Logistic Regression vs. SVM in Non-linear Classification</a></li></ol></li><li><a href="#reference">Reference</a></li></ol><div id="related-articles"><h2>Related Articles</h2><ol><li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/01_introduction_to_machine_learning.html">Introduction to Machine Learning</a></li><li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/02_linear_regression.html">Linear Regression</a></li><li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/03_review_of_linear_algebra.html">Review of Linear Algebra</a></li><li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/04_linear_regression_multiple_variables.html">Linear Regression Multiple Variables</a></li><li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/06_logistic_regression.html">Logistic Regression</a></li><li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/07_regularization.html">Regularization</a></li><li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/08_neural_networks_representation.html">Neural Networks Representation</a></li><li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/09_neural_networks_learning.html">Neural Networks Learning</a></li><li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/10_applying_machine_learning_advice.html">Applying Machine Learning Advice</a></li><li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/11_machine_learning_system_design.html">Machine Learning System Design</a></li><li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/12_support_vector_machines.html">Support Vector Machines</a></li><li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/13_clustering.html">Clustering</a></li><li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/14_dimensionality_reduction.html">Dimensionality Reduction</a></li><li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/15_anomaly_detection.html">Anomaly Detection</a></li><li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/16_recommendation_systems.html">Recommendation Systems</a></li><li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/17_large_scale_machine_learning.html">Large Scale Machine Learning</a></li><li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/18_photo_ocr.html">Photo Ocr</a></li></ol></div></div></div><footer>
<div class="footer-columns">
<div class="footer-column">
<img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png"/>
</div>
<div class="footer-column">
<h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
<p>Thank you for visiting my personal website. All content here is free to use, but please remember to be respectful and avoid any misuse of the site. If youâ€™d like to get in touch, feel free to reach out via my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or connect with me on <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have technical questions or ideas to share. Wishing you all the best and a fantastic life ahead!</p>
</div>
<div class="footer-column">
<h2>Follow me</h2>
<ul class="social-media">
<li>
<a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
</a>YouTube
                </li>
<li>
<a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
</a>LinkedIn
                </li>
<li>
<a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
</a>Instagram
                </li>
<li>
<a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
</a>Github
                </li>
</ul>
</div>
</div>
<div>
<p id="copyright">
            Â© Adam Djellouli. All rights reserved.
        </p>
</div>
<script>
        document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
    </script>
<script src="../../app.js"></script>
</footer></body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script></html>