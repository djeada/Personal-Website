<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Neural Networks: Learning and Classification</title>
    <meta content="Neural networks, a core algorithm in machine learning, draw inspiration from the human brain's structure and function." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>Last modified: June 15, 2024</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="neural-networks-learning-and-classification">Neural Networks: Learning and Classification</h2>
            <p>Neural networks, a core algorithm in machine learning, draw inspiration from the human brain's structure and function. They consist of layers containing interconnected nodes (neurons), each designed to perform specific computational tasks. Neural networks can tackle various classification problems, such as binary and multi-class classifications. The efficacy of these networks is enhanced through the adjustment of their parameters, guided by a cost function that quantifies the deviation between predictions and actual values.</p>
            <h3 id="classification-problems-in-neural-networks">Classification Problems in Neural Networks</h3>
            <ul>
                <li><strong>Training Set Representation:</strong> Typically represented as ${(x^1, y^1), (x^2, y^2), ..., (x^n, y^n)}$, where $x^i$ is the $i^{th}$ input and $y^i$ is the corresponding target output.</li>
                <li>$L$: Total number of layers in the network.</li>
                <li>$s_l$: Number of units (excluding the bias unit) in layer $l$.</li>
            </ul>
            <h4 id="example-network-architecture">Example Network Architecture</h4>
            <p><img alt="Example of Multi-Layer Neural Network" src="https://user-images.githubusercontent.com/37275728/201518449-ec13fac4-0716-4131-8e5e-f0405ce075a5.png" /></p>
            <p>In the illustrated network:</p>
            <ul>
                <li>There are 4 layers ($L=4$).</li>
                <li>The first layer ($s_1$) has 3 units.</li>
                <li>The second and third layers ($s_2$, $s_3$) each have 5 units.</li>
                <li>The fourth layer ($s_4$) has 4 units.</li>
            </ul>
            <h3 id="binary-classification">Binary Classification</h3>
            <p>In binary classification, the neural network predicts a single output which can take one of two possible values (often represented as 0 or 1). Characteristics of binary classification in neural networks include:</p>
            <ul>
                <li><strong>Single Output Node:</strong> The final layer produces a real number value, interpreted as a probability or classification decision.</li>
                <li><strong>Output Representation:</strong> $k = 1$ (one output class) and $s_L = 1$ (one unit in the output layer).</li>
            </ul>
            <h3 id="multi-class-classification">Multi-class Classification</h3>
            <p>Multi-class classification extends the network's capability to differentiate between multiple classes. Here, the target output $y$ is a vector representing the probability of each class. Characteristics include:</p>
            <ul>
                <li><strong>Multiple Output Classes:</strong> $k$ distinct classifications are represented.</li>
                <li><strong>Output Vector:</strong> The output $y$ is a $k$-dimensional vector of real numbers, where each element corresponds to a class.</li>
                <li><strong>Layer Configuration:</strong> The output layer has $s_L = k$ units, each corresponding to one of the $k$ classes.</li>
            </ul>
            <h3 id="learning-process">Learning Process</h3>
            <p>The learning process in neural networks involves:</p>
            <ol>
                <li><strong>Cost Function:</strong> A generalization of the logistic regression cost function, which sums the error over all output units and classes. This function quantifies the difference between predicted and actual values.</li>
                <li><strong>Gradient Computation:</strong> Employing the chain rule to calculate the partial derivatives of the cost function with respect to each network parameter.</li>
                <li><strong>Backpropagation Algorithm:</strong> Used for efficiently computing these partial derivatives. The algorithm propagates the error signal backwards through the network, adjusting the parameters (weights and biases) to minimize the cost function.</li>
            </ol>
            <h3 id="neural-network-cost-function">Neural Network Cost Function</h3>
            <p>The cost function for neural networks is an extension of the logistic regression cost function, adapted for multiple outputs. This function evaluates the performance of a neural network by measuring the difference between the predicted outputs and the actual values across all output units.</p>
            <p>The cost function for a neural network with multiple outputs (where $K$ is the number of output units) is defined as:</p>
            <p>$$J(\Theta) = -\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K}[ y_k^{(i)} \log{(h_{\Theta}(x^{(i)}))}<em>k + (1- y_k^{(i)}) \log(1 - {(h</em>{\Theta} (x^{(i)}))}<em>k)] + \frac{\lambda}{2m} \sum</em>{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} (\Theta^{(l)}_{ji})^2$$</p>
            <ul>
                <li><strong>$m$:</strong> Number of training examples.</li>
                <li><strong>$K$:</strong> Number of output units.</li>
                <li><strong>$L$:</strong> Total number of layers in the network.</li>
                <li><strong>$s_l$:</strong> Number of units in layer $l$.</li>
                <li><strong>$y_k^{(i)}$:</strong> Actual value of the $k^{th}$ output unit for the $i^{th}$ training example.</li>
                <li><strong>$h_{\Theta}(x^{(i)})$:</strong> Hypothesis function applied to the $i^{th}$ training example.</li>
                <li><strong>$\lambda$:</strong> Regularization parameter to avoid overfitting.</li>
                <li><strong>$\Theta^{(l)}_{ji}$:</strong> Weight from unit $i$ in layer $l$ to unit $j$ in layer $l+1$.</li>
            </ul>
            <p>This function incorporates a sum over the $K$ output units for each training example, and also includes a regularization term to penalize large weights and prevent overfitting.</p>
            <h3 id="partial-derivative-terms">Partial Derivative Terms</h3>
            <p>In order to optimize the cost function, we need to understand how changes in the weights $\Theta$ affect the cost. This is where the partial derivative terms come into play.</p>
            <ul>
                <li><strong>$\Theta^{(1)}_{ji}$:</strong> Represents the weight from the $i^{th}$ unit in layer 1 to the $j^{th}$ unit in layer 2.</li>
                <li><strong>$\Theta^{(1)}<em>{10}$ , $\Theta^{(1)}</em>{11}$ , $\Theta^{(1)}_{21}$:</strong> Specific weights that map from the input layer to the first and second units in the second layer, including the bias unit.</li>
            </ul>
            <h3 id="gradient-computation">Gradient Computation</h3>
            <p>Gradient computation in neural networks is achieved through a process known as backpropagation, which calculates the gradient of the cost function with respect to each weight in the network.</p>
            <h4 id="forward-propagation-example">Forward Propagation Example</h4>
            <p>Consider a neural network with one training example $(x,y)$:</p>
            <ul>
                <li><strong>Layer 1 (Input Layer):</strong> $a^{(1)} = x$ and $z^{(2)} = \Theta^{(1)}a^{(1)}$.</li>
                <li><strong>Layer 2 (Hidden Layer):</strong> $a^{(2)} = g(z^{(2)})$, append $a^{(2)}_0$, and then $z^{(3)} = \Theta^{(2)}a^{(2)}$.</li>
                <li><strong>Layer 3 (Another Hidden Layer):</strong> $a^{(3)} = g(z^{(3)})$, append $a^{(3)}_0$, and then $z^{(4)} = \Theta^{(3)}a^{(3)}$.</li>
                <li><strong>Output Layer:</strong> $a^{(4)} = h_{\Theta}(x) = g(z^{(4)})$.</li>
            </ul>
            <p><img alt="Gradient Computation in Neural Networks" src="https://user-images.githubusercontent.com/37275728/201518441-7740e76d-9a6b-426f-98ad-85a5ff207a89.png" /></p>
            <p>Each layer's output ($a^{(l)}$) becomes the input for the next layer, with the activation function $g$ (typically a sigmoid or ReLU function) applied to the weighted sums. The final output $a^{(4)}$ is the hypothesis of the network for the given input $x$. The backpropagation algorithm then computes the gradient by propagating the error backwards from the output layer to the input layer, adjusting the weights $\Theta$ to minimize the cost function $J(\Theta)$.</p>
            <h3 id="backpropagation-algorithm">Backpropagation Algorithm</h3>
            <p>Backpropagation is a key algorithm in training neural networks, used for calculating the gradient of the cost function with respect to each parameter in the network. It involves propagating errors backward through the network, from the output layer to the input layer.</p>
            <h4 id="calculating-error-terms-deltas-">Calculating Error Terms (Deltas)</h4>
            <ul>
                <li><strong>$\delta_j$ Vector:</strong> Represents the error for each node $j$ in layer $l$. It is an $L \times 1$ vector, where $L$ is the total number of layers.</li>
                <li><strong>Last Layer Error ($\delta^{(L)}$):</strong> For the last layer, the error is calculated as the difference between the network's output ($a^{(L)}$) and the actual value ($y$):</li>
            </ul>
            <p>$$\delta^{(L)}_j = a^{(L)}_j - y_j$$</p>
            <ul>
                <li><strong>Error for Other Layers:</strong> The error terms for the other layers are computed recursively using the errors from the subsequent layer:</li>
            </ul>
            <p>$$\delta^{(l)}_j = (\Theta^{(l)}_j)^T \delta^{(l+1)} \cdot g'(z^{(l)}_j)$$</p>
            <h4 id="accumulating-gradient-delta-">Accumulating Gradient ($\Delta$)</h4>
            <ul>
                <li><strong>Partial Derivative Accumulation:</strong> $\Delta^{(l)}<em>{ij}$ accumulates the partial derivatives of the cost function with respect to the weights $\Theta^{(l)}</em>{ij}$:</li>
            </ul>
            <p>$$\Delta^{(l)}<em>{ij} := \Delta^{(l)}</em>{ij} + a^{(l)}_j \delta^{(l+1)}_i$$</p>
            <ul>
                <li><strong>Layer Notation:</strong> $l$ denotes the layer, $j$ the node in layer $l$, and $i$ the error of the affected node in the subsequent layer $l+1$.</li>
            </ul>
            <h4 id="calculating-the-gradient">Calculating the Gradient</h4>
            <p>The gradient of the cost function $J(\Theta)$ with respect to the weights is given by:</p>
            <p>$$
                \frac{\partial}{\partial \Theta^{(l)}<em>{ij}}J(\Theta) = \begin{cases}
                    \frac{1}{m} \Delta^{(l)}</em>{ij} + \lambda \Theta ^{(l)}<em>{ij} \quad &amp;\text{if} \, j \neq 0 \\
                    \frac{1}{m} \Delta^{(l)}</em>{ij} \quad &amp;\text{if} \, j=0 \\
                \end{cases}
                $$</p>
            <h3 id="gradient-checking">Gradient Checking</h3>
            <p>Due to the complexity of backpropagation, implementing it correctly is crucial. Gradient checking is a technique to validate the correctness of the computed gradients.</p>
            <p>Steps in Gradient Checking:</p>
            <ol>
                <li><strong>Function $J(\Theta)$:</strong> Start with a function representing the cost.</li>
                <li><strong>Compute Perturbed Weights:</strong> Calculate $J(\Theta + \epsilon)$ and $J(\Theta - \epsilon)$, where $\epsilon$ is a small value.</li>
                <li><strong>Slope Estimation:</strong> The derivative is approximated by the slope of the straight line joining these two points.</li>
            </ol>
            <p><img alt="Gradient Checking Visualization" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/gradient_checking.png" /></p>
            <p>Gradient checking serves as a diagnostic tool to ensure that the backpropagation implementation is correct. However, it's computationally expensive and typically used only for debugging and not during regular training.</p>
            <h2 id="reference">Reference</h2>
            <p>These notes are derived from the free video lectures offered by Stanford University, led by Professor Andrew Ng. These lectures are part of the renowned Machine Learning course available on Coursera. For more information and to access the full course, visit the <a href="https://www.coursera.org/learn/machine-learning">Coursera course page</a>.</p>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#neural-networks-learning-and-classification">Neural Networks: Learning and Classification</a>
                    <ol>
                        <li><a href="#classification-problems-in-neural-networks">Classification Problems in Neural Networks</a>
                            <ol>
                                <li><a href="#example-network-architecture">Example Network Architecture</a></li>
                            </ol>
                        </li>
                        <li><a href="#binary-classification">Binary Classification</a></li>
                        <li><a href="#multi-class-classification">Multi-class Classification</a></li>
                        <li><a href="#learning-process">Learning Process</a></li>
                        <li><a href="#neural-network-cost-function">Neural Network Cost Function</a></li>
                        <li><a href="#partial-derivative-terms">Partial Derivative Terms</a></li>
                        <li><a href="#gradient-computation">Gradient Computation</a>
                            <ol>
                                <li><a href="#forward-propagation-example">Forward Propagation Example</a></li>
                            </ol>
                        </li>
                        <li><a href="#backpropagation-algorithm">Backpropagation Algorithm</a>
                            <ol>
                                <li><a href="#calculating-error-terms-deltas-">Calculating Error Terms (Deltas)</a></li>
                                <li><a href="#accumulating-gradient-delta-">Accumulating Gradient ($\Delta$)</a></li>
                                <li><a href="#calculating-the-gradient">Calculating the Gradient</a></li>
                            </ol>
                        </li>
                        <li><a href="#gradient-checking">Gradient Checking</a></li>
                    </ol>
                </li>
                <li><a href="#reference">Reference</a></li>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/01_introduction_to_machine_learning.html">Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/02_linear_regression.html">Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/03_review_of_linear_algebra.html">Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/04_linear_regression_multiple_variables.html">Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/06_logistic_regression.html">Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/07_regularization.html">Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/08_neural_networks_representation.html">Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/09_neural_networks_learning.html">Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/10_applying_machine_learning_advice.html">Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/11_machine_learning_system_design.html">Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/12_support_vector_machines.html">Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/13_clustering.html">Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/14_dimensionality_reduction.html">Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/15_anomaly_detection.html">Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/16_recommendation_systems.html">Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/17_large_scale_machine_learning.html">Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/18_photo_ocr.html">Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>