<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Recommendation Systems</title>
    <meta content="Recommendation systems are a fundamental component in the interface between users and large-scale content providers like Amazon, eBay, and iTunes." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <div class="article-action-buttons"><button class="btn-suggest-edit" title="Suggest Edit">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M4 21h4l11-11-4-4L4 17v4z" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button><button class="btn-create-issue" title="Create Issue">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"></circle>
                        <line stroke="currentColor" stroke-width="2" x1="12" x2="12" y1="8" y2="12"></line>
                        <circle cx="12" cy="16" fill="currentColor" r="1"></circle>
                    </svg>
                </button><button class="btn-download" title="Download">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M12 5v14m0 0l-6-6m6 6l6-6" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button></div>
            <p style="text-align: right;"><i>Last modified: October 27, 2022</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="recommendation-systems">Recommendation Systems</h2>
            <p>Recommendation systems are a fundamental component in the interface between users and large-scale content providers like Amazon, eBay, and iTunes. These systems personalize user experiences by suggesting products, movies, or content based on past interactions and preferences.</p>
            <h3 id="concept-of-recommender-systems">Concept of Recommender Systems</h3>
            <ul>
                <li><strong>Purpose</strong>: To recommend new content or products to users based on their historical interactions or preferences.</li>
                <li><strong>Nature</strong>: Not a single algorithm, but a category of methods tailored to predicting user preferences.</li>
            </ul>
            <h3 id="example-scenario-movie-rating-prediction">Example Scenario: Movie Rating Prediction</h3>
            <p>Imagine a company that sells movies and allows users to rate them. Consider the following setup:</p>
            <ul>
                <li><strong>Movies</strong>: A catalog of five movies.</li>
                <li><strong>Users</strong>: A database of four users.</li>
                <li><strong>User Ratings</strong>: Users rate movies on a scale of 1 to 5.</li>
            </ul>
            <p><img alt="Movie Recommendation Example" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/movie_recommendation.png" /></p>
            <p>Notations:</p>
            <ul>
                <li>$n_u$: Number of users.</li>
                <li>$n_m$: Number of movies.</li>
                <li>$r(i, j)$: Indicator (1 or 0) if user $j$ has rated movie $i$.</li>
                <li>$y^{(i, j)}$: Rating given by user $j$ to movie $i$.</li>
            </ul>
            <h3 id="feature-vectors-and-parameter-vectors">Feature Vectors and Parameter Vectors</h3>
            <ul>
                <li><strong>Movie Features</strong>: Each movie can have a feature vector representing various attributes or genres.</li>
                <li><strong>Additional Feature (x0 = 1)</strong>: For computational convenience, an additional feature is added to each movie's feature vector.</li>
            </ul>
            <p>Example for "Cute Puppies of Love":</p>
            <p>$$
                x^{(3)} = \begin{bmatrix}
                1 \\
                0.99 \\
                0
                \end{bmatrix}
                $$</p>
            <ul>
                <li><strong>User Parameters</strong>: Each user has a parameter vector representing their preferences.</li>
            </ul>
            <p>Example for user 1 (Alice) and her preference for "Cute Puppies of Love":</p>
            <p>$$
                \theta^{(1)} = \begin{bmatrix}
                0 \\
                5 \\
                0
                \end{bmatrix}
                $$</p>
            <h3 id="example-scenario-movie-rating-prediction">Example Scenario: Movie Rating Prediction</h3>
            <p>Imagine a company that sells movies and allows users to rate them. Consider the following setup:</p>
            <ul>
                <li><strong>Movies</strong>: A catalog of five movies.</li>
                <li><strong>Users</strong>: A database of four users.</li>
                <li><strong>User Ratings</strong>: Users rate movies on a scale of 1 to 5.</li>
            </ul>
            <p><img alt="Movie Recommendation Example" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/movie_recommendation.png" /></p>
            <p>Notations:</p>
            <ul>
                <li>$n_u$: Number of users.</li>
                <li>$n_m$: Number of movies.</li>
                <li>$r(i, j)$: Indicator (1 or 0) if user $j$ has rated movie $i$.</li>
                <li>$y^{(i, j)}$: Rating given by user $j$ to movie $i$.</li>
            </ul>
            <h3 id="feature-vectors-and-parameter-vectors">Feature Vectors and Parameter Vectors</h3>
            <ul>
                <li><strong>Movie Features</strong>: Each movie can have a feature vector representing various attributes or genres.</li>
                <li><strong>Additional Feature ($x_0 = 1$)</strong>: For computational convenience, an additional feature is added to each movie's feature vector.</li>
            </ul>
            <p>Example for "Cute Puppies of Love":</p>
            <p>$$
                x^{(3)} = \begin{bmatrix}
                1 \\
                0.99 \\
                0
                \end{bmatrix}
                $$</p>
            <ul>
                <li><strong>User Parameters</strong>: Each user has a parameter vector representing their preferences.</li>
            </ul>
            <p>Example for user 1 (Alice) and her preference for "Cute Puppies of Love":</p>
            <p>$$
                \theta^{(1)} = \begin{bmatrix}
                0 \\
                5 \\
                0
                \end{bmatrix}
                $$</p>
            <h3 id="mock-implementation-in-python">Mock Implementation in Python</h3>
            <p>This implementation demonstrates how to set up a basic collaborative filtering model for movie rating prediction using Python.</p>
            <h4 id="step-1-data-setup">Step 1: Data Setup</h4>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np

# Number of users and movies
n_u = 4
n_m = 5

# User ratings matrix (0 if not rated)
ratings = np.array([
    [5, 4, 0, 0, 3],
    [0, 0, 4, 0, 4],
    [0, 0, 5, 3, 3],
    [3, 4, 0, 0, 5]
])

# Indicator matrix r(i, j) = 1 if rated, 0 otherwise
R = (ratings != 0).astype(int)</code></pre>
            </div>
            </p>
            <h4 id="step-2-feature-and-parameter-initialization">Step 2: Feature and Parameter Initialization</h4>
            <p>
            <div>
                <pre><code class="language-python"># Initialize random movie feature vectors (3 features per movie)
X = np.random.rand(n_m, 3)

# Initialize random user parameter vectors (3 parameters per user)
Theta = np.random.rand(n_u, 3)</code></pre>
            </div>
            </p>
            <h4 id="step-3-collaborative-filtering-cost-function">Step 3: Collaborative Filtering Cost Function</h4>
            <p>
            <div>
                <pre><code class="language-python">def collaborative_filtering_cost(X, Theta, ratings, R, lambda_ = 0.1):
    predictions = X.dot(Theta.T)
    error = (predictions - ratings) * R
    cost = 0.5 * np.sum(error**2)
    
    # Regularization
    cost += (lambda_ / 2) * (np.sum(Theta**2) + np.sum(X**2))
    
    return cost

cost = collaborative_filtering_cost(X, Theta, ratings, R)
print(f'Initial cost: {cost}')</code></pre>
            </div>
            </p>
            <h4 id="step-4-gradient-descent-for-optimization">Step 4: Gradient Descent for Optimization</h4>
            <p>
            <div>
                <pre><code class="language-python">def collaborative_filtering_gradient(X, Theta, ratings, R, alpha=0.01, lambda_ = 0.1, iterations=1000):
    X_grad = np.zeros(X.shape)
    Theta_grad = np.zeros(Theta.shape)
    
    for _ in range(iterations):
        predictions = X.dot(Theta.T)
        error = (predictions - ratings) * R
        
        X_grad = error.dot(Theta) + lambda_ * X
        Theta_grad = error.T.dot(X) + lambda_ * Theta
        
        X -= alpha * X_grad
        Theta -= alpha * Theta_grad
        
        cost = collaborative_filtering_cost(X, Theta, ratings, R, lambda_)
        if _ % 100 == 0:
            print(f'Iteration {_}: cost = {cost}')
    
    return X, Theta

X, Theta = collaborative_filtering_gradient(X, Theta, ratings, R)</code></pre>
            </div>
            </p>
            <h3 id="making-predictions">Making Predictions</h3>
            <p>To predict how much Alice might like "Cute Puppies of Love", we compute:</p>
            <p>$$
                (\theta^{(1)})^T x^{(3)} = (0 \cdot 1) + (5 \cdot 0.99) + (0 \cdot 0) = 4.95
                $$</p>
            <p>This predicts a high rating of 4.95 out of 5 for Alice for this movie, based on her preference parameters and the movie's features.</p>
            <p>Here is an example implementation in Python: </p>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np

# Feature vector for the movie "Cute Puppies of Love"
x_cute_puppies = np.array([1, 0.99, 0])

# Parameter vector for Alice
theta_alice = np.array([0, 5, 0])

# Compute the dot product
predicted_rating = np.dot(theta_alice, x_cute_puppies)

# Print the predicted rating
print(f"Predicted rating for Alice for 'Cute Puppies of Love': {predicted_rating}")</code></pre>
            </div>
            </p>
            <p>When you run this code, it will output:</p>
            <p>
            <div>
                <pre><code class="language-shell">Predicted rating for Alice for 'Cute Puppies of Love': 4.95</code></pre>
            </div>
            </p>
            <h3 id="collaborative-filtering">Collaborative Filtering</h3>
            <ul>
                <li><strong>Method</strong>: Collaborative filtering is often used in recommender systems. It involves learning either user preferences or item features, depending on the data available.</li>
                <li><strong>Learning</strong>: This can be achieved using algorithms like gradient descent.</li>
                <li><strong>Advantage</strong>: The system can learn to make recommendations on its own without explicit programming of the features or preferences.</li>
            </ul>
            <h2 id="learning-user-preferences-in-recommendation-systems">Learning User Preferences in Recommendation Systems</h2>
            <p>In recommendation systems, learning user preferences $(\theta^j)$ is a key component. This process is akin to linear regression, but with a unique approach tailored for recommendation contexts.</p>
            <h3 id="minimizing-cost-function-for-user-preferences">Minimizing Cost Function for User Preferences</h3>
            <ul>
                <li>The objective is to minimize the following cost function for each user $j$:</li>
            </ul>
            <p>$$
                \min_{\theta^j} = \frac{1}{2m^{(j)}} \sum_{i:r(i,j)=1} \left((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)}\right)^2 + \frac{\lambda}{2m^{(j)}} \sum_{k=1}^{n} (\theta_k^{(j)})^2
                $$</p>
            <ul>
                <li>Here, $m^{(j)}$ is the number of movies rated by user $j$, $\lambda$ is the regularization parameter, and $r(i, j)$ indicates whether user $j$ has rated movie $i$.</li>
            </ul>
            <h3 id="gradient-descent-for-optimization">Gradient Descent for Optimization</h3>
            <p>Update Rule for $\theta_k^{(j)}$:</p>
            <ul>
                <li>For $k = 0$ (bias term):</li>
            </ul>
            <p>$$
                \theta_k^{(j)} := \theta_k^{(j)} - \alpha \sum_{i:r(i,j)=1} \left((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)}\right) x_k^{(i)}
                $$</p>
            <ul>
                <li>For $k \neq 0$:</li>
            </ul>
            <p>$$
                \theta_k^{(j)} := \theta_k^{(j)} - \alpha \left(\sum_{i:r(i,j)=1} \left((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)}\right) x_k^{(i)} + \lambda \theta_k^{(j)}\right)
                $$</p>
            <h3 id="collaborative-filtering-algorithm">Collaborative Filtering Algorithm</h3>
            <p>Collaborative filtering leverages the interplay between user preferences and item (movie) features:</p>
            <ul>
                <li><strong>Learning Features from Preferences</strong>: Given user preferences $(\theta^{(1)}, ..., \theta^{(n_u)})$, the algorithm can learn movie features $(x^{(1)}, ..., x^{(n_m)})$.</li>
                <li><strong>Cost Function for Movies</strong>:</li>
            </ul>
            <p>$$
                \min_{x^{(1)}, ..., x^{(n_m)}} \frac{1}{2} \sum_{i=1}^{n_m} \sum_{i:r(i,j)=1} \left((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)}\right)^2 + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2
                $$</p>
            <ul>
                <li><strong>Iterative Process</strong>: The algorithm typically involves an iterative process, alternating between optimizing for movie features and user preferences.</li>
            </ul>
            <h3 id="collaborative-filtering-implementation">Collaborative Filtering Implementation</h3>
            <p>Here, we'll implement a basic collaborative filtering model using gradient descent to predict user ratings for movies.</p>
            <h4 id="step-1-data-setup">Step 1: Data Setup</h4>
            <p>First, we need to set up our data, including user ratings and an indicator matrix showing which movies have been rated by which users.</p>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np

# Number of users and movies
n_u = 4
n_m = 5

# User ratings matrix (0 if not rated)
ratings = np.array([
    [5, 4, 0, 0, 3],
    [0, 0, 4, 0, 4],
    [0, 0, 5, 3, 3],
    [3, 4, 0, 0, 5]
])

# Indicator matrix r(i, j) = 1 if rated, 0 otherwise
R = (ratings != 0).astype(int)</code></pre>
            </div>
            </p>
            <h4 id="step-2-feature-and-parameter-initialization">Step 2: Feature and Parameter Initialization</h4>
            <p>We initialize random feature vectors for movies and parameter vectors for users.</p>
            <p>
            <div>
                <pre><code class="language-python"># Initialize random movie feature vectors (3 features per movie)
X = np.random.rand(n_m, 3)

# Initialize random user parameter vectors (3 parameters per user)
Theta = np.random.rand(n_u, 3)</code></pre>
            </div>
            </p>
            <h4 id="step-3-collaborative-filtering-cost-function">Step 3: Collaborative Filtering Cost Function</h4>
            <p>We define the cost function for collaborative filtering, including regularization.</p>
            <p>
            <div>
                <pre><code class="language-python">def collaborative_filtering_cost(X, Theta, ratings, R, lambda_=0.1):
    predictions = X.dot(Theta.T)
    error = (predictions - ratings) * R
    cost = 0.5 * np.sum(error ** 2)
    
    # Regularization
    cost += (lambda_ / 2) * (np.sum(Theta ** 2) + np.sum(X ** 2))
    
    return cost

cost = collaborative_filtering_cost(X, Theta, ratings, R)
print(f'Initial cost: {cost}')</code></pre>
            </div>
            </p>
            <h4 id="step-4-gradient-descent-for-optimization">Step 4: Gradient Descent for Optimization</h4>
            <p>We implement gradient descent to optimize the feature and parameter vectors.</p>
            <p>
            <div>
                <pre><code class="language-python">def collaborative_filtering_gradient(X, Theta, ratings, R, alpha=0.01, lambda_=0.1, iterations=1000):
    for _ in range(iterations):
        predictions = X.dot(Theta.T)
        error = (predictions - ratings) * R
        
        X_grad = error.dot(Theta) + lambda_ * X
        Theta_grad = error.T.dot(X) + lambda_ * Theta
        
        X -= alpha * X_grad
        Theta -= alpha * Theta_grad
        
        if _ % 100 == 0:
            cost = collaborative_filtering_cost(X, Theta, ratings, R, lambda_)
            print(f'Iteration {_}: cost = {cost}')
    
    return X, Theta

X, Theta = collaborative_filtering_gradient(X, Theta, ratings, R)</code></pre>
            </div>
            </p>
            <h4 id="step-5-prediction">Step 5: Prediction</h4>
            <p>Finally, we use the optimized feature and parameter vectors to predict ratings for all users and movies.</p>
            <p>
            <div>
                <pre><code class="language-python"># Predict ratings for all users and movies
predictions = X.dot(Theta.T)

# Print predictions
print("Predicted ratings:")
print(predictions)</code></pre>
            </div>
            </p>
            <h3 id="vectorization-low-rank-matrix-factorization">Vectorization: Low Rank Matrix Factorization</h3>
            <ul>
                <li><strong>Matrix Y</strong>: Organize all user ratings into a matrix Y, which represents the interactions between users and movies.</li>
            </ul>
            <p>Example $[5 \times 4]$ Matrix Y for 5 movies and 4 users:</p>
            <p>$$
                Y = \begin{pmatrix}
                5 &amp; 5 &amp; 0 &amp; 0 \\
                5 &amp; ? &amp; ? &amp; 0 \\
                ? &amp; 4 &amp; 0 &amp; ? \\
                0 &amp; 0 &amp; 5 &amp; 4 \\
                0 &amp; 0 &amp; 5 &amp; 0
                \end{pmatrix}
                $$</p>
            <ul>
                <li><strong>Predicted Ratings Matrix</strong>: The predicted ratings can be expressed as the product of matrices $X$ and $\Theta^T$.</li>
            </ul>
            <p>$$
                X \Theta^T = \begin{pmatrix}
                (\theta^{(1)})^T(x^{(1)}) &amp; \dots &amp; (\theta^{(n_u)})^T(x^{(1)}) \\
                \vdots &amp; \ddots &amp; \vdots \\
                (\theta^{(1)})^T(x^{(n_m)}) &amp; \dots &amp; (\theta^{(n_u)})^T(x^{(n_m)})
                \end{pmatrix}
                $$</p>
            <ul>
                <li><strong>Matrix X</strong>: Contains the features for each movie, stacked in rows.</li>
                <li><strong>Matrix $\Theta$</strong>: Contains the user preferences, also stacked in rows.</li>
            </ul>
            <h3 id="low-rank-matrix-factorization-example-implementation">Low Rank Matrix Factorization Example Implementation</h3>
            <p>To implement the low-rank matrix factorization for collaborative filtering, we need to set up the necessary matrices and perform the matrix multiplication to predict ratings. Here's how to do it in Python.</p>
            <h4 id="step-1-define-the-matrix-y">Step 1: Define the Matrix Y</h4>
            <p>Organize all user ratings into a matrix $Y$, which represents the interactions between users and movies.</p>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np

# Example [5 x 4] Matrix Y for 5 movies and 4 users
Y = np.array([
    [5, 5, 0, 0],
    [5, 0, 0, 0],
    [0, 4, 0, 0],
    [0, 0, 5, 4],
    [0, 0, 5, 0]
])

print("Matrix Y:")
print(Y)</code></pre>
            </div>
            </p>
            <h4 id="step-2-initialize-matrices-x-and-Î¸">Step 2: Initialize Matrices X and Î˜</h4>
            <p>Matrix $X$ contains the features for each movie, and matrix $\Theta$ contains the user preferences.</p>
            <p>
            <div>
                <pre><code class="language-python"># Number of movies (n_m) and users (n_u)
n_m, n_u = Y.shape

# Number of features
n_features = 3

# Initialize movie feature matrix X and user preference matrix Theta with random values
X = np.random.rand(n_m, n_features)
Theta = np.random.rand(n_u, n_features)

print("\nInitial Matrix X (Movie Features):")
print(X)

print("\nInitial Matrix Î˜ (User Preferences):")
print(Theta)</code></pre>
            </div>
            </p>
            <h4 id="step-3-compute-the-predicted-ratings-matrix">Step 3: Compute the Predicted Ratings Matrix</h4>
            <p>The predicted ratings matrix can be expressed as the product of matrices $X$ and $\Theta^T$.</p>
            <p>
            <div>
                <pre><code class="language-python"># Compute the predicted ratings
predicted_ratings = X.dot(Theta.T)

print("\nPredicted Ratings Matrix XÎ˜^T:")
print(predicted_ratings)</code></pre>
            </div>
            </p>
            <p>This Python code demonstrates the setup of matrices $Y$, $X$, and $\Theta$, and the calculation of the predicted ratings matrix using matrix multiplication. The matrix $X$ contains the features for each movie, and $\Theta$ contains the user preferences. The predicted ratings matrix is obtained by multiplying $X$ with the transpose of $\Theta$.</p>
            <h3 id="scenario-a-user-with-no-ratings">Scenario: A User with No Ratings</h3>
            <p>Imagine a user in a movie recommendation system who hasn't rated any movies. This scenario presents a challenge for typical collaborative filtering algorithms.</p>
            <p><img alt="User with No Ratings" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/no_ratings.png" /></p>
            <ul>
                <li>For such a user, there are no movies for which $r(i, j) = 1$.</li>
                <li>As a result, the algorithm ends up minimizing only the regularization term for this user, which doesn't provide meaningful insight for recommendations.</li>
            </ul>
            <h3 id="mean-normalization-approach">Mean Normalization Approach</h3>
            <ol>
                <li><strong>Compute the Mean Rating</strong>: Calculate the average rating for each movie and store these in a vector $\mu$.</li>
            </ol>
            <p>Example $\mu$ vector for a system with 5 movies:</p>
            <p>$$
                \mu = \begin{bmatrix}
                2.5 \\
                2.5 \\
                2 \\
                2.25 \\
                1.25
                \end{bmatrix}
                $$</p>
            <ol>
                <li><strong>Normalize the Ratings Matrix</strong>: Subtract the mean rating for each movie from all its ratings in the matrix $Y$.</li>
            </ol>
            <p>Normalized Ratings Matrix $Y$:</p>
            <p>$$
                Y = \begin{pmatrix}
                2.5 &amp; 2.5 &amp; -2.5 &amp; -2.5 &amp; ? \\
                2.5 &amp; ? &amp; ? &amp; -2.5 &amp; ? \\
                ? &amp; 2 &amp; -2 &amp; ? &amp; ? \\
                -2.25 &amp; -2.25 &amp; 2.75 &amp; 1.75 &amp; ? \\
                -1.25 &amp; -1.25 &amp; 3.75 &amp; -1.25 &amp; ?
                \end{pmatrix}
                $$</p>
            <ol>
                <li><strong>Adjust for Users with No Ratings</strong>: For a new user with no ratings, their predicted rating for each movie can be initialized to the mean rating of that movie. This provides a baseline from which personalized recommendations can evolve as the user starts rating movies.</li>
            </ol>
            <h3 id="scenario-a-user-with-no-ratings">Scenario: A User with No Ratings</h3>
            <p>Imagine a user in a movie recommendation system who hasn't rated any movies. This scenario presents a challenge for typical collaborative filtering algorithms.</p>
            <p>For such a user, there are no movies for which $r(i, j) = 1$. As a result, the algorithm ends up minimizing only the regularization term for this user, which doesn't provide meaningful insight for recommendations.</p>
            <h3 id="mean-normalization-approach">Mean Normalization Approach</h3>
            <ol>
                <li><strong>Compute the Mean Rating</strong>: Calculate the average rating for each movie and store these in a vector $\mu$.</li>
            </ol>
            <p>Example $\mu$ vector for a system with 5 movies:</p>
            <p>$$
                \mu = \begin{bmatrix}
                2.5 \\
                2.5 \\
                2 \\
                2.25 \\
                1.25
                \end{bmatrix}
                $$</p>
            <ol>
                <li><strong>Normalize the Ratings Matrix</strong>: Subtract the mean rating for each movie from all its ratings in the matrix $Y$.</li>
            </ol>
            <p>Normalized Ratings Matrix $Y$:</p>
            <p>$$
                Y = \begin{pmatrix}
                2.5 &amp; 2.5 &amp; -2.5 &amp; -2.5 &amp; ? \\
                2.5 &amp; ? &amp; ? &amp; -2.5 &amp; ? \\
                ? &amp; 2 &amp; -2 &amp; ? &amp; ? \\
                -2.25 &amp; -2.25 &amp; 2.75 &amp; 1.75 &amp; ? \\
                -1.25 &amp; -1.25 &amp; 3.75 &amp; -1.25 &amp; ?
                \end{pmatrix}
                $$</p>
            <ol>
                <li><strong>Adjust for Users with No Ratings</strong>: For a new user with no ratings, their predicted rating for each movie can be initialized to the mean rating of that movie. This provides a baseline from which personalized recommendations can evolve as the user starts rating movies.</li>
            </ol>
            <h3 id="implementation-in-python">Implementation in Python</h3>
            <h4 id="step-1-define-the-matrix-y-and-compute-mean-ratings">Step 1: Define the Matrix Y and Compute Mean Ratings</h4>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np

# Example [5 x 4] Matrix Y for 5 movies and 4 users
Y = np.array([
    [5, 5, 0, 0],
    [5, 0, 0, 0],
    [0, 4, 0, 0],
    [0, 0, 5, 4],
    [0, 0, 5, 0]
])

# Compute the mean rating for each movie, ignoring zeros
mean_ratings = np.true_divide(Y.sum(1), (Y != 0).sum(1))

print("Mean Ratings Vector Î¼:")
print(mean_ratings)</code></pre>
            </div>
            </p>
            <h4 id="step-2-normalize-the-ratings-matrix">Step 2: Normalize the Ratings Matrix</h4>
            <p>
            <div>
                <pre><code class="language-python"># Subtract the mean rating for each movie from all its ratings
Y_normalized = Y - mean_ratings[:, np.newaxis]
# Ensure we do not subtract from places where rating was 0
Y_normalized[Y == 0] = 0

print("\nNormalized Ratings Matrix Y:")
print(Y_normalized)</code></pre>
            </div>
            </p>
            <h4 id="step-3-adjust-for-users-with-no-ratings">Step 3: Adjust for Users with No Ratings</h4>
            <p>For a new user with no ratings, their predicted rating for each movie can be initialized to the mean rating of that movie.</p>
            <p>
            <div>
                <pre><code class="language-python"># Predicted ratings for a new user with no ratings
new_user_ratings = mean_ratings.copy()

print("\nPredicted Ratings for New User with No Ratings:")
print(new_user_ratings)</code></pre>
            </div>
            </p>
            <h3 id="benefits-of-mean-normalization">Benefits of Mean Normalization</h3>
            <ul>
                <li><strong>Handling New Users</strong>: Provides a starting point for recommendations for users who have not yet provided any ratings.</li>
                <li><strong>Rating Sparsity</strong>: Mitigates issues arising from sparse data, which is common in many real-world recommendation systems.</li>
                <li><strong>Balancing the Dataset</strong>: Normalization helps in balancing the dataset, especially when there are variations in the number of ratings per movie.</li>
            </ul>
            <h2 id="reference">Reference</h2>
            <p>These notes are based on the free video lectures offered by Stanford University, led by Professor Andrew Ng. These lectures are part of the renowned Machine Learning course available on Coursera. For more information and to access the full course, visit the <a href="https://www.coursera.org/learn/machine-learning">Coursera course page</a>.</p>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#recommendation-systems">Recommendation Systems</a>
                    <ol>
                        <li><a href="#concept-of-recommender-systems">Concept of Recommender Systems</a></li>
                        <li><a href="#example-scenario-movie-rating-prediction">Example Scenario: Movie Rating Prediction</a></li>
                        <li><a href="#feature-vectors-and-parameter-vectors">Feature Vectors and Parameter Vectors</a></li>
                        <li><a href="#example-scenario-movie-rating-prediction">Example Scenario: Movie Rating Prediction</a></li>
                        <li><a href="#feature-vectors-and-parameter-vectors">Feature Vectors and Parameter Vectors</a></li>
                        <li><a href="#mock-implementation-in-python">Mock Implementation in Python</a>
                            <ol>
                                <li><a href="#step-1-data-setup">Step 1: Data Setup</a></li>
                                <li><a href="#step-2-feature-and-parameter-initialization">Step 2: Feature and Parameter Initialization</a></li>
                                <li><a href="#step-3-collaborative-filtering-cost-function">Step 3: Collaborative Filtering Cost Function</a></li>
                                <li><a href="#step-4-gradient-descent-for-optimization">Step 4: Gradient Descent for Optimization</a></li>
                            </ol>
                        </li>
                        <li><a href="#making-predictions">Making Predictions</a></li>
                        <li><a href="#collaborative-filtering">Collaborative Filtering</a></li>
                    </ol>
                </li>
                <li><a href="#learning-user-preferences-in-recommendation-systems">Learning User Preferences in Recommendation Systems</a>
                    <ol>
                        <li><a href="#minimizing-cost-function-for-user-preferences">Minimizing Cost Function for User Preferences</a></li>
                        <li><a href="#gradient-descent-for-optimization">Gradient Descent for Optimization</a></li>
                        <li><a href="#collaborative-filtering-algorithm">Collaborative Filtering Algorithm</a></li>
                        <li><a href="#collaborative-filtering-implementation">Collaborative Filtering Implementation</a>
                            <ol>
                                <li><a href="#step-1-data-setup">Step 1: Data Setup</a></li>
                                <li><a href="#step-2-feature-and-parameter-initialization">Step 2: Feature and Parameter Initialization</a></li>
                                <li><a href="#step-3-collaborative-filtering-cost-function">Step 3: Collaborative Filtering Cost Function</a></li>
                                <li><a href="#step-4-gradient-descent-for-optimization">Step 4: Gradient Descent for Optimization</a></li>
                                <li><a href="#step-5-prediction">Step 5: Prediction</a></li>
                            </ol>
                        </li>
                        <li><a href="#vectorization-low-rank-matrix-factorization">Vectorization: Low Rank Matrix Factorization</a></li>
                        <li><a href="#low-rank-matrix-factorization-example-implementation">Low Rank Matrix Factorization Example Implementation</a>
                            <ol>
                                <li><a href="#step-1-define-the-matrix-y">Step 1: Define the Matrix Y</a></li>
                                <li><a href="#step-2-initialize-matrices-x-and-Î¸">Step 2: Initialize Matrices X and Î˜</a></li>
                                <li><a href="#step-3-compute-the-predicted-ratings-matrix">Step 3: Compute the Predicted Ratings Matrix</a></li>
                            </ol>
                        </li>
                        <li><a href="#scenario-a-user-with-no-ratings">Scenario: A User with No Ratings</a></li>
                        <li><a href="#mean-normalization-approach">Mean Normalization Approach</a></li>
                        <li><a href="#scenario-a-user-with-no-ratings">Scenario: A User with No Ratings</a></li>
                        <li><a href="#mean-normalization-approach">Mean Normalization Approach</a></li>
                        <li><a href="#implementation-in-python">Implementation in Python</a>
                            <ol>
                                <li><a href="#step-1-define-the-matrix-y-and-compute-mean-ratings">Step 1: Define the Matrix Y and Compute Mean Ratings</a></li>
                                <li><a href="#step-2-normalize-the-ratings-matrix">Step 2: Normalize the Ratings Matrix</a></li>
                                <li><a href="#step-3-adjust-for-users-with-no-ratings">Step 3: Adjust for Users with No Ratings</a></li>
                            </ol>
                        </li>
                        <li><a href="#benefits-of-mean-normalization">Benefits of Mean Normalization</a></li>
                    </ol>
                </li>
                <li><a href="#reference">Reference</a></li>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/01_introduction_to_machine_learning.html">Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/02_linear_regression.html">Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/03_review_of_linear_algebra.html">Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/04_linear_regression_multiple_variables.html">Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/06_logistic_regression.html">Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/07_regularization.html">Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/08_neural_networks_representation.html">Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/09_neural_networks_learning.html">Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/10_applying_machine_learning_advice.html">Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/11_machine_learning_system_design.html">Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/12_support_vector_machines.html">Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/13_clustering.html">Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/14_dimensionality_reduction.html">Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/15_anomaly_detection.html">Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/16_recommendation_systems.html">Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/17_large_scale_machine_learning.html">Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/18_photo_ocr.html">Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
    <div id="pdf-spinner-overlay">
        <div class="spinner"></div>
    </div>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>