<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <title>Linear Regression</title>
    <meta charset="utf-8" />
    <meta content="Linear regression is a type of supervised learning that is used to predict a continuous output variable given a set of input variables or features." name="description" />
    <meta content="Adam Djellouli" name="keywords" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" type="text/css" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <link href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport">
    <meta content="ie-edge" http-equiv="X-UA-Compatible">
    </meta>
    </meta>
</head>

<body>
    <nav>
        <a class="logo" href="../index.html" title="Adam Djellouli - Home">
            <img alt="Adam Djellouli Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul>
            <li> <a href="../../index.html" title="Home"> Home </a> </li>
            <li> <a class="active" href="../../core/blog.html" title="Adam Djellouli's Blog - Programming, technology and more"> Blog </a> </li>
            <li> <a href="../../core/tools.html" title="Useful Tools by Adam Djellouli"> Tools </a> </li>
            <li> <a href="../../core/projects.html" title="Projects by Adam Djellouli"> Projects </a> </li>
            <li> <a href="../../core/resume.html" title="Adam Djellouli's Resume"> Resume </a> </li>
            <li> <a href="../../core/about.html" title="About Adam Djellouli"> About </a> </li>
            <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
        </ul>
    </nav>
    <section id="article-body"></section>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>This article is written in: üá∫üá∏</i></p>
            <h2 id="linear-regression">Linear Regression</h2>
            <p>Linear regression is a type of supervised learning that is used to predict a continuous output variable given a set of input variables or features. It is a useful tool for understanding the relationship between different variables and can be used to make predictions about future values of the output variable. In linear regression, the output variable is modeled as a linear function of the input variables, using coefficients or parameters called theta. To fit the best straight line to the data, we can use a cost function that expresses how frustrated we are with the model's current prediction of the output variable given the input variables. The optimization objective is to find the values of theta that minimize the cost function. One way to do this is to use the gradient descent algorithm, which involves iteratively updating the values of theta to minimize the cost function. To understand the behavior of the cost function, we can create surface or contour plots that show how the cost function changes with different values of theta. By understanding the cost function and using optimization techniques like gradient descent, we can improve the performance of our linear regression model.</p>
            <h3 id="notation-used-throughout-the-course-">Notation (used throughout the course)</h3>
            <ul>
                <li>m = number of training examples.</li>
                <li>$x$ = input variables / features.</li>
                <li>$y$ = output variable ‚Äùtarget‚Äù variables.</li>
                <li>$(x, y)$ - single training example.</li>
                <li>$(x^i, y^i)$ - specific example (ith training example).</li>
            </ul>
            <p><img alt="house price table" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/house_price_table.png" /></p>
            <h3 id="with-our-training-set-defined-how-do-we-use-it-">With our training set defined - how do we use it?</h3>
            <ul>
                <li>Take training set.</li>
                <li>Pass into a learning algorithm.</li>
                <li>Algorithm outputs a function (h = hypothesis).</li>
                <li>This function takes an input (e.g. size of new house).</li>
                <li>Tries to output the estimated value of Y.</li>
            </ul>
            <p>$$h_{\theta}(x) = \theta_0 + \theta_1x$$</p>
            <ul>
                <li>Y is a linear function of x!</li>
                <li>$\theta_0$ is zero condition.</li>
                <li>$\theta_1$ is gradient.</li>
            </ul>
            <h3 id="cost-function">Cost function</h3>
            <ul>
                <li>A cost function expresses how dissatisfied we are with the model's present coefficients in the prediction of output y from input x. </li>
                <li>We may use the cost function to determine how to fit the best straight line to our data.</li>
                <li>We want to want to solve a minimization problem. Minimize $(h_{\theta}(x) - y)^2$.</li>
                <li>Sum this over the training set.</li>
            </ul>
            <p>$$J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{m}^{i=1}(h_{\theta}(x^{(i)}) - y^{(i)})^2$$</p>
            <p>For $\theta_0 = 0$ we have:</p>
            <p>$$h_{\theta}(x) = \theta_1x\quad and \quad J(\theta_1) = \frac{1}{2m} \sum_{m}^{i=1}(h_{\theta}(x^{(i)}) - y^{(i)})^2$$</p>
            <p>Data:
                * $\theta_1 = 1$ and $J(\theta_1)= 0$.
                * $\theta_1 = 0.5$ and $J(\theta_1)= 0.58$.
                * $\theta_1 = 0$ and $J(\theta_1)= 2.3$.</p>
            <p><img alt="cost_function" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/cost_function.png" /></p>
            <p>The optimization objective for the learning algorithm is find the value of Œ∏1
                which minimizes J(Œ∏1 ). So, here Œ∏1 = 1 is the best value for Œ∏1 .</p>
            <h3 id="a-deeper-insight-into-the-cost-function-simplified-cost-function">A deeper insight into the cost function - simplified cost function</h3>
            <p>The real cost function takes two variables as parameters! $J(\theta_0, \theta_1)$.
                We can now generates a 3D surface plot where axis are:</p>
            <ul>
                <li>$X = \theta_1$.</li>
                <li>$Z = \theta_0$.</li>
                <li>$Y = J(\theta_0,\theta_1)$.</li>
            </ul>
            <p><img alt="surface_cost_function" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/surface_cost_function.png" /></p>
            <p>The best hypothesis is at the bottom of the bowl.
                Instead of a surface plot we can use a contour figures/plots.
                * Set of ellipses in different colors.
                * Each colour is the same value of $J(\theta_0,\theta_1)$, but obviously plot to different
                locations because Œ∏1 and Œ∏0 will vary.
                * Imagine a bowl shape function coming out of the screen so the middle is
                the concentric circles.</p>
            <p><img alt="contour_cost_function" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/contour_cost_function.png" /></p>
            <p>The best hypothesis is located in the center of the contour plot.</p>
            <h2 id="gradient-descent-algorithm">Gradient descent algorithm</h2>
            <pre><div><pre><code class="language-shell">\theta = [0, 0]
while not converged:
  for j in [0, 1]:
      \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)</code></pre>
    </div>
    </pre>
    <ul>
        <li>Begin with initial guesses, could be (0,0) or any other value.</li>
        <li>Repeatedly change values of $\theta_0$ and $\theta_1$ to try to reduce $J(\theta_0, \theta_1)$.</li>
        <li>Continue until you reach a local minimum.</li>
        <li>Reached minimum is determined by the starting point.</li>
    </ul>
    <p><img alt="gradient_descent" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/gradient_descent.png" /></p>
    <p><img alt="gradient_descent" src="https://user-images.githubusercontent.com/37275728/201476896-555ad8c4-8422-428b-937f-12cdf70d75bd.png" /></p>
    <p>Two key terms in the algorithm:
        * $\alpha$ term.
        * Derivative term.</p>
    <h3 id="partial-derivative-vs-derivative">Partial derivative vs. derivative</h3>
    <ul>
        <li>Use partial derivative when we have multiple variables but only derive with respect to one.</li>
        <li>Use derivative when we are deriving with respect to all the variables.</li>
    </ul>
    <p>Derivative says:
        * Let‚Äôs look at the slope of the line by taking the tangent at the point.
        * As a result, going towards the minimum (down) will result in a negative derivative; nevertheless, alpha is always positive, thus $J(\theta_1)$ will be updated to a lower value.
        * Similarly, as we progress up a slope, we increase the value of $J(\theta_1)$.</p>
    <p>$\alpha$ term
        * If it‚Äôs too small, it takes too long to converge.
        * If it is too large, it may exceed the minimum and fail to converge.</p>
    <p>When you get to a local minimum
        * Gradient of tangent/derivative is 0.
        * So derivative term = 0.
        * $\alpha \cdot 0 = 0$.
        * So $\theta_1 = \theta_1 - 0$.
        * So $\theta_1$ remains the same.</p>
    <h2 id="linear-regression-with-gradient-descent">Linear regression with gradient descent</h2>
    <p>Apply gradient descent to minimize the squared error cost function $J(\theta_0, \theta_1)$.</p>
    <p>$$\frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1) = \frac{\partial}{\partial \theta_j} \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^2$$</p>
    <p>$$= \frac{\partial}{\partial \theta_j} \frac{1}{2m} \sum_{i=1}^{m} (\theta_0 + \theta_1x^{(i)} - y^{(i)})^2$$</p>
    <p>For each case, we must determine the partial derivative:</p>
    <p>$$j=0:\frac{\partial}{\partial \theta_0} J(\theta_0, \theta_1)=\frac{\partial}{\partial \theta_0} \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})$$</p>
    <p>$$j=1:\frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1)=\frac{\partial}{\partial \theta_1} \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}$$</p>
    <h2 id="normal-equations-method">Normal equations method</h2>
    <ul>
        <li>To solve the minimization problem we can solve it $[ min J(\theta_0, \theta_1) ]$ exactly using a numerical method which avoids the iterative approach used by gradient descent.</li>
        <li>Can be much faster for some problems, but it is much more complicated (will be covered in detail later).</li>
    </ul>
    <h2 id="extension-of-the-current-model">Extension of the current model</h2>
    <p>We could learn with a larger number of features.
        * e.g. with houses: Size, Age, Number bedrooms, Number floors...
        * The disadvantage is that we can't plot in more than three dimensions.
        * Best way to get around with this is the notation of linear algebra (matrices and vectors).</p>
    </section>
    <div id="table-of-contents">
        <h2>Table of Contents</h2>
        <ol>
            <li><a href="#linear-regression">Linear Regression</a></li>
            <li><a href="#notation-used-throughout-the-course-">Notation (used throughout the course)</a></li>
            <li><a href="#with-our-training-set-defined-how-do-we-use-it-">With our training set defined - how do we use it?</a></li>
            <li><a href="#cost-function">Cost function</a></li>
            <li><a href="#a-deeper-insight-into-the-cost-function-simplified-cost-function">A deeper insight into the cost function - simplified cost function</a></li>
            <li><a href="#gradient-descent-algorithm">Gradient descent algorithm</a></li>
            <li><a href="#partial-derivative-vs-derivative">Partial derivative vs. derivative</a></li>
            <li><a href="#linear-regression-with-gradient-descent">Linear regression with gradient descent</a></li>
            <li><a href="#normal-equations-method">Normal equations method</a></li>
            <li><a href="#extension-of-the-current-model">Extension of the current model</a></li>
        </ol>
    </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/addjellouli/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                ¬© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../app.js"></script>
    </footer>
</body>

</html>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>