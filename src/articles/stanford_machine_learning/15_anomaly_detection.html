<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Anomaly Detection in Machine Learning</title>
    <meta content="Anomaly detection involves identifying data points that significantly differ from the majority of the data, often signaling unusual or suspicious activities." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper"><article-section id="article-body">
            <div class="article-action-buttons"><button class="btn-suggest-edit" title="Suggest Edit">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M4 21h4l11-11-4-4L4 17v4z" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button><button class="btn-create-issue" title="Create Issue">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"></circle>
                        <line stroke="currentColor" stroke-width="2" x1="12" x2="12" y1="8" y2="12"></line>
                        <circle cx="12" cy="16" fill="currentColor" r="1"></circle>
                    </svg>
                </button><button class="btn-download" title="Download">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M12 5v14m0 0l-6-6m6 6l6-6" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button></div>
            <p style="text-align: right;"><i>Last modified: April 15, 2019</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="anomaly-detection-in-machine-learning">Anomaly Detection in Machine Learning</h2>
            <p>Anomaly detection involves identifying data points that significantly differ from the majority of the data, often signaling unusual or suspicious activities. This technique is widely used across various domains, such as fraud detection, manufacturing, and system monitoring.</p>
            <h3 id="concept-of-anomaly-detection">Concept of Anomaly Detection</h3>
            <ul>
                <li><strong>Baseline Establishment</strong>: The first step in anomaly detection is to establish a normal pattern or baseline from the dataset.</li>
                <li><strong>Probability Threshold (Epsilon)</strong>: Data points are flagged as anomalies if their probability, according to the established model, falls below a threshold $\epsilon$.</li>
                <li>If $p(x_{\text{test}}) &lt; \epsilon$, the data point is considered an anomaly.</li>
                <li>If $p(x_{\text{test}}) \geq \epsilon$, the data point is considered normal.</li>
                <li><strong>Threshold Selection</strong>: The value of $\epsilon$ is critical and is chosen based on the desired confidence level and the specific context of the application.</li>
            </ul>
            <p><img alt="Illustration of Anomaly Detection" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/anomaly.png" /></p>
            <h3 id="applications-of-anomaly-detection">Applications of Anomaly Detection</h3>
            <p>I. Fraud Detection:</p>
            <ul>
                <li>User behavior metrics (online time, login location, spending patterns) are analyzed.</li>
                <li>A model of typical user behavior is created, and deviations are flagged as potential fraud.</li>
            </ul>
            <p>II. Manufacturing: In scenarios like aircraft engine production, anomalies can indicate defects or potential failures.</p>
            <p>III. Data Center Monitoring: Monitoring metrics (memory usage, disk accesses, CPU load) to identify machines that are likely to fail.</p>
            <h3 id="utilizing-the-gaussian-distribution">Utilizing the Gaussian Distribution</h3>
            <ul>
                <li><strong>Mean ($\mu$) and Variance ($\sigma^2$)</strong>: The Gaussian distribution is characterized by these parameters.</li>
                <li><strong>Probability Calculation</strong>:</li>
            </ul>
            <p>$$
                p(x; \mu; \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
                $$</p>
            <p><img alt="Gaussian Distribution" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/gaussian.png" /></p>
            <ul>
                <li><strong>Data Fitting</strong>: Estimating the Gaussian distribution from the dataset to represent the normal behavior.</li>
            </ul>
            <p><img alt="Data Fitting with Gaussian" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/data_fit.png" /></p>
            <h3 id="evaluating-anomaly-detection-systems">Evaluating Anomaly Detection Systems</h3>
            <ul>
                <li><strong>Data Split</strong>: Divide the data into training, cross-validation, and test sets.</li>
                <li><strong>Performance Metrics</strong>: Precision and recall are often used to evaluate the effectiveness of the anomaly detection system.</li>
                <li><strong>Fine-Tuning</strong>: Adjust the model and $\epsilon$ based on the performance metrics to improve accuracy.</li>
            </ul>
            <h3 id="algorithm-steps">Algorithm Steps</h3>
            <ol>
                <li>
                    <p><strong>Feature Selection</strong>: Choose features $x_i$ that might be indicators of anomalous behavior.</p>
                </li>
                <li>
                    <p><strong>Parameter Fitting</strong>: Calculate the mean ($\mu_j$) and variance ($\sigma_j^2$) for each feature</p>
                </li>
            </ol>
            <p>$$
                \mu_j = \frac{1}{m} \sum_{i=1}^m x_j^{(i)}
                $$</p>
            <p>$$
                \sigma_j^2 = \frac{1}{m} \sum_{i=1}^m (x_j^{(i)} - \mu_j)^2
                $$</p>
            <ol>
                <li><strong>Probability Computation for New Example</strong>: For a new example $x$, compute the probability $p(x)$:</li>
            </ol>
            <p>$$
                p(x) = \prod_{j=1}^n \frac{1}{\sqrt{2\pi\sigma_j^2}} \exp\left(-\frac{(x_j - \mu_j)^2}{2\sigma_j^2}\right)
                $$</p>
            <p>Below is the Python implementation of the Gaussian distribution, data fitting, and evaluation of anomaly detection systems based on the algorithm steps.</p>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np

# Function to calculate Gaussian Probability
def gaussian_probability(x, mean, variance):
    coefficient = 1 / np.sqrt(2 * np.pi * variance)
    exponent = np.exp(-((x - mean) ** 2) / (2 * variance))
    return coefficient * exponent

# Function to fit Gaussian parameters (mean and variance) for each feature
def fit_gaussian_parameters(X):
    mean = np.mean(X, axis=0)
    variance = np.var(X, axis=0)
    return mean, variance

# Function to calculate the probability of a new example using the Gaussian distribution
def compute_probability(x, mean, variance):
    probabilities = gaussian_probability(x, mean, variance)
    return np.prod(probabilities)

# Example dataset
X_train = np.array([[1.1, 2.2], [1.3, 2.1], [1.2, 2.3], [1.1, 2.4]])
X_cross_val = np.array([[1.0, 2.0], [1.4, 2.5]])

# Fitting the Gaussian parameters
mean, variance = fit_gaussian_parameters(X_train)

# Calculate the probability of a new example
x_new = np.array([1.2, 2.2])
probability = compute_probability(x_new, mean, variance)

print("Mean:", mean)
print("Variance:", variance)
print("Probability of new example:", probability)

# Performance Metrics (Precision and Recall)
# Note: This requires true labels and predicted labels
from sklearn.metrics import precision_score, recall_score

# Example true labels and predicted labels
y_true = np.array([0, 0, 1, 1])
y_pred = np.array([0, 0, 1, 0])

precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)

print("Precision:", precision)
print("Recall:", recall)</code></pre>
            </div>
            </p>
            <p>Here are the results:</p>
            <p>I. Gaussian Parameters:</p>
            <ul>
                <li>Mean: Feature 1: 1.175, Feature 2: 2.25</li>
                <li>Variance: Feature 1: 0.006875, Feature 2: 0.0125</li>
            </ul>
            <p>II. Probability of New Example:</p>
            <ul>
                <li>For the new example $[1.2, 2.2]$, the calculated probability using the Gaussian distribution is approximately 14.844.</li>
            </ul>
            <p>III. Performance Metrics:</p>
            <ul>
                <li>Precision: 1.0 (All instances predicted as anomalies are actually anomalies, with no false positives)</li>
                <li>Recall: 0.5 (Only 50% of the actual anomalies were correctly identified, indicating some false negatives)</li>
            </ul>
            <h3 id="developing-and-evaluating-an-anomaly-detection-system">Developing and Evaluating an Anomaly Detection System</h3>
            <p>I. <strong>Labeled Data</strong>: Have a dataset where $y=0$ indicates normal (non-anomalous) examples, and $y=1$ represents anomalous examples.</p>
            <p>II. <strong>Data Division</strong>: Separate the dataset into a training set (normal examples), a cross-validation (CV) set, and a test set, with both the CV and test sets including some anomalous examples.</p>
            <p>III. <strong>Example Case</strong>:</p>
            <ul>
                <li>Imagine a dataset with 10,000 normal (good) engines and 50 flawed (anomalous) engines.</li>
                <li>Training set: 6,000 good engines.</li>
                <li>CV set: 2,000 good engines, 10 anomalous.</li>
                <li>Test set: 2,000 good engines, 10 anomalous.</li>
            </ul>
            <p>IV. <strong>Evaluation Metrics</strong>:</p>
            <ul>
                <li>True positives (TP), false positives (FP), false negatives (FN), and true negatives (TN).</li>
                <li>Precision (the proportion of true positives among all positives identified by the model).</li>
                <li>Recall (the proportion of true positives identified out of all actual positives).</li>
                <li>F1-score (a harmonic mean of precision and recall, providing a balance between them).</li>
            </ul>
            <p>Below is the complete Python code for the implementation:</p>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score

# Simulate dataset
np.random.seed(0)

# Normal examples
normal_examples = np.random.normal(0, 1, (10000, 2))

# Anomalous examples
anomalous_examples = np.random.normal(5, 1, (50, 2))

# Labels
y_normal = np.zeros(10000)
y_anomalous = np.ones(50)

# Combine the data
X = np.vstack((normal_examples, anomalous_examples))
y = np.concatenate((y_normal, y_anomalous))

# Shuffle the dataset
indices = np.arange(X.shape[0])
np.random.shuffle(indices)
X = X[indices]
y = y[indices]

# Data division
X_train = X[:6000]
y_train = y[:6000]

X_cv = X[6000:8000]
y_cv = y[6000:8000]

X_test = X[8000:]
y_test = y[8000:]

# Fit Gaussian parameters on the training set
mean, variance = fit_gaussian_parameters(X_train)

# Probability threshold
epsilon = 0.01  # This threshold can be tuned

# Compute probabilities for CV and Test sets
p_cv = np.array([compute_probability(x, mean, variance) for x in X_cv])
p_test = np.array([compute_probability(x, mean, variance) for x in X_test])

# Predict anomalies
y_pred_cv = (p_cv &lt; epsilon).astype(int)
y_pred_test = (p_test &lt; epsilon).astype(int)

# Calculate evaluation metrics for the CV set
tp_cv = np.sum((y_cv == 1) &amp; (y_pred_cv == 1))
fp_cv = np.sum((y_cv == 0) &amp; (y_pred_cv == 1))
fn_cv = np.sum((y_cv == 1) &amp; (y_pred_cv == 0))
tn_cv = np.sum((y_cv == 0) &amp; (y_pred_cv == 0))

precision_cv = precision_score(y_cv, y_pred_cv)
recall_cv = recall_score(y_cv, y_pred_cv)
f1_cv = f1_score(y_cv, y_pred_cv)

# Calculate evaluation metrics for the Test set
tp_test = np.sum((y_test == 1) &amp; (y_pred_test == 1))
fp_test = np.sum((y_test == 0) &amp; (y_pred_test == 1))
fn_test = np.sum((y_test == 1) &amp; (y_pred_test == 0))
tn_test = np.sum((y_test == 0) &amp; (y_pred_test == 0))

precision_test = precision_score(y_test, y_pred_test)
recall_test = recall_score(y_test, y_pred_test)
f1_test = f1_score(y_test, y_pred_test)

# Display results
results = {
    "CV Set": {
        "TP": tp_cv,
        "FP": fp_cv,
        "FN": fn_cv,
        "TN": tn_cv,
        "Precision": precision_cv,
        "Recall": recall_cv,
        "F1 Score": f1_cv
    },
    "Test Set": {
        "TP": tp_test,
        "FP": fp_test,
        "FN": fn_test,
        "TN": tn_test,
        "Precision": precision_test,
        "Recall": recall_test,
        "F1 Score": f1_test
    }
}

import ace_tools as tools; tools.display_dataframe_to_user(name="Anomaly Detection Results", dataframe=results)</code></pre>
            </div>
            </p>
            <p>Below are the results for the cross-validation (CV) set and the test set.</p>
            <p>I. Cross-Validation (CV) Set:</p>
            <ul>
                <li><strong>True Positives (TP)</strong>: 6</li>
                <li><strong>False Positives (FP)</strong>: 18</li>
                <li><strong>False Negatives (FN)</strong>: 4</li>
                <li><strong>True Negatives (TN)</strong>: 1982</li>
                <li><strong>Precision</strong>: 0.25</li>
                <li><strong>Recall</strong>: 0.6</li>
                <li><strong>F1 Score</strong>: 0.35294117647058826</li>
            </ul>
            <p>II. Test Set:</p>
            <ul>
                <li><strong>True Positives (TP)</strong>: 3</li>
                <li><strong>False Positives (FP)</strong>: 20</li>
                <li><strong>False Negatives (FN)</strong>: 7</li>
                <li><strong>True Negatives (TN)</strong>: 1980</li>
                <li><strong>Precision</strong>: 0.13043478260869565</li>
                <li><strong>Recall</strong>: 0.3</li>
                <li><strong>F1 Score</strong>: 0.1818181818181818</li>
            </ul>
            <p>III. Discussion:</p>
            <ol>
                <li>The precision is quite low in both the CV and test sets, indicating a high number of false positives. This means the model is predicting too many normal examples as anomalies.</li>
                <li>The recall is moderate in the CV set (0.6) but lower in the test set (0.3). This means the model is missing some anomalies, especially in the test set.</li>
                <li>The F1 score, which balances precision and recall, is also low, indicating the overall performance of the model needs improvement.</li>
            </ol>
            <p>IV. Next Steps:</p>
            <ol>
                <li>Adjusting the threshold for anomaly detection could help balance precision and recall. A lower threshold might reduce false positives, while a higher threshold could reduce false negatives.</li>
                <li>Additional or better features could improve the model's ability to distinguish between normal and anomalous examples.</li>
                <li>Consider more sophisticated anomaly detection algorithms such as Isolation Forest, One-Class SVM, or neural networks designed for anomaly detection.</li>
                <li>Increasing the number of anomalous examples in the training set could help the model learn to identify anomalies more accurately.</li>
            </ol>
            <h3 id="selecting-a-good-evaluation-metric">Selecting a Good Evaluation Metric</h3>
            <ul>
                <li><strong>F1-Score</strong>: This is particularly useful in the context of anomaly detection where the class distribution is highly imbalanced (many more normal than anomalous examples).</li>
                <li><strong>Precision and Recall</strong>: These metrics provide a more nuanced understanding of the systemâ€™s performance, especially when the cost of false positives and false negatives varies.</li>
            </ul>
            <h3 id="concept-of-multivariate-gaussian-distribution">Concept of Multivariate Gaussian Distribution</h3>
            <ul>
                <li><strong>Scenario</strong>: Imagine fitting a Gaussian distribution to two features, such as CPU load and memory usage.</li>
                <li><strong>Example</strong>: In a test set, consider an example with a high memory use (x1 = 1.5) but low CPU load (x2 = 0.4). Individually, these values might fall within normal ranges, but their combination could be anomalous.</li>
            </ul>
            <p><img alt="Example of Multivariate Gaussian Distribution" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/mult_gauss.png" /></p>
            <h3 id="parameters-of-the-multivariate-gaussian-model">Parameters of the Multivariate Gaussian Model</h3>
            <ul>
                <li><strong>Mean Vector ($\mu$)</strong>: An n-dimensional vector representing the mean of each feature.</li>
                <li><strong>Covariance Matrix ($\Sigma$)</strong>: An $[n \times n]$ matrix, capturing how each pair of features varies together.</li>
                <li><strong>Probability Density Function</strong>:</li>
            </ul>
            <p>$$
                p(x; \mu; \Sigma) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu)^T \Sigma^{-1}(x - \mu)\right)
                $$</p>
            <p><img alt="Covariance Matrix" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/cov_matrix_sigma.png" /></p>
            <p>Hereâ€™s the complete Python code for the implementation:</p>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np

# Function to calculate the Multivariate Gaussian Probability
def multivariate_gaussian_probability(x, mean, covariance):
    n = len(mean)
    diff = x - mean
    exponent = -0.5 * np.dot(np.dot(diff.T, np.linalg.inv(covariance)), diff)
    coefficient = 1 / ((2 * np.pi) ** (n / 2) * np.linalg.det(covariance) ** 0.5)
    return coefficient * np.exp(exponent)

# Example dataset: CPU load and memory usage
data = np.array([[0.5, 1.2], [0.6, 1.4], [0.8, 1.3], [0.7, 1.5], [0.9, 1.7], [0.6, 1.3]])

# Calculate the mean vector and covariance matrix
mean_vector = np.mean(data, axis=0)
covariance_matrix = np.cov(data, rowvar=False)

# Example of a new point
x_new = np.array([1.5, 0.4])

# Calculate the probability using the multivariate Gaussian distribution
probability = multivariate_gaussian_probability(x_new, mean_vector, covariance_matrix)

mean_vector, covariance_matrix, probability</code></pre>
            </div>
            </p>
            <p>Here are the results:</p>
            <p>I. Parameters of the Multivariate Gaussian Model:</p>
            <p><strong>Mean Vector ($\mu$)</strong>:</p>
            <ul>
                <li>Feature 1 (CPU Load): 0.683</li>
                <li>Feature 2 (Memory Usage): 1.4</li>
            </ul>
            <p><strong>Covariance Matrix ($\Sigma$)</strong>:</p>
            <p>
            <div>
                <pre><code class="language-shell">[[0.02166667, 0.02      ],
[0.02      , 0.032     ]]</code></pre>
            </div>
            </p>
            <p>II. Probability of New Example: For the new example with a high memory use ($x1 = 1.5$) and low CPU load ($x2 = 0.4$), the calculated probability using the multivariate Gaussian distribution is approximately <strong>8.86e-56</strong>.</p>
            <p>III. Interpretation:</p>
            <ol>
                <li>The mean vector represents the average CPU load and memory usage from the dataset. This gives the center of the multivariate Gaussian distribution.</li>
                <li>The covariance matrix captures how CPU load and memory usage vary together. A positive covariance between these features suggests that as one increases, the other tends to increase as well.</li>
                <li>The extremely low probability (8.86e-56) indicates that the new example $[1.5, 0.4]$ is highly unlikely under the learned multivariate Gaussian distribution. This combination of high memory use and low CPU load is unusual compared to the typical patterns in the training data, suggesting it could be an anomaly.</li>
            </ol>
            <h3 id="gaussian-model-vs-multivariate-gaussian-model">Gaussian Model vs. Multivariate Gaussian Model</h3>
            <p>
            <table>
                <tr>
                    <td><strong>Aspect</strong></td>
                    <td><strong>Gaussian Model</strong></td>
                    <td><strong>Multivariate Gaussian Model</strong></td>
                </tr>
                <tr>
                    <td><strong>Usage</strong></td>
                    <td>More commonly used in anomaly detection.</td>
                    <td>Used less frequently.</td>
                </tr>
                <tr>
                    <td><strong>Feature Creation</strong></td>
                    <td>Requires manual creation of features to capture unusual combinations in values.</td>
                    <td>Directly captures correlations between features without needing extra feature engineering.</td>
                </tr>
                <tr>
                    <td><strong>Computational Efficiency</strong></td>
                    <td>Generally more computationally efficient.</td>
                    <td>Less efficient computationally.</td>
                </tr>
                <tr>
                    <td><strong>Scalability</strong></td>
                    <td>Scales well to large feature vectors.</td>
                    <td>Requires more examples than the number of features (m &gt; n).</td>
                </tr>
                <tr>
                    <td><strong>Training Set Size</strong></td>
                    <td>Works effectively even with small training sets.</td>
                    <td>Requires a larger training set relative to the number of features.</td>
                </tr>
                <tr>
                    <td><strong>Advantage</strong></td>
                    <td>Simple to implement.</td>
                    <td>Can detect anomalies due to unusual combinations of normal-appearing individual features.</td>
                </tr>
            </table>
            </p>
            <h2 id="reference">Reference</h2>
            <p>These notes are based on the free video lectures offered by Stanford University, led by Professor Andrew Ng. These lectures are part of the renowned Machine Learning course available on Coursera. For more information and to access the full course, visit the <a href="https://www.coursera.org/learn/machine-learning">Coursera course page</a>.</p>
        </article-section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#anomaly-detection-in-machine-learning">Anomaly Detection in Machine Learning</a>
                    <ol>
                        <li><a href="#concept-of-anomaly-detection">Concept of Anomaly Detection</a></li>
                        <li><a href="#applications-of-anomaly-detection">Applications of Anomaly Detection</a></li>
                        <li><a href="#utilizing-the-gaussian-distribution">Utilizing the Gaussian Distribution</a></li>
                        <li><a href="#evaluating-anomaly-detection-systems">Evaluating Anomaly Detection Systems</a></li>
                        <li><a href="#algorithm-steps">Algorithm Steps</a></li>
                        <li><a href="#developing-and-evaluating-an-anomaly-detection-system">Developing and Evaluating an Anomaly Detection System</a></li>
                        <li><a href="#selecting-a-good-evaluation-metric">Selecting a Good Evaluation Metric</a></li>
                        <li><a href="#concept-of-multivariate-gaussian-distribution">Concept of Multivariate Gaussian Distribution</a></li>
                        <li><a href="#parameters-of-the-multivariate-gaussian-model">Parameters of the Multivariate Gaussian Model</a></li>
                        <li><a href="#gaussian-model-vs-multivariate-gaussian-model">Gaussian Model vs. Multivariate Gaussian Model</a></li>
                    </ol>
                </li>
                <li><a href="#reference">Reference</a></li>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/01_introduction_to_machine_learning.html">Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/02_linear_regression.html">Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/03_review_of_linear_algebra.html">Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/04_linear_regression_multiple_variables.html">Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/06_logistic_regression.html">Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/07_regularization.html">Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/08_neural_networks_representation.html">Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/09_neural_networks_learning.html">Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/10_applying_machine_learning_advice.html">Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/11_machine_learning_system_design.html">Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/12_support_vector_machines.html">Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/13_clustering.html">Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/14_dimensionality_reduction.html">Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/15_anomaly_detection.html">Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/16_recommendation_systems.html">Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/17_large_scale_machine_learning.html">Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/18_photo_ocr.html">Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>Thank you for visiting my personal website. All content here is free to use, but please remember to be respectful and avoid any misuse of the site. If youâ€™d like to get in touch, feel free to reach out via my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or connect with me on <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have technical questions or ideas to share. Wishing you all the best and a fantastic life ahead!</p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
    <div id="pdf-spinner-overlay">
        <div class="spinner"></div>
    </div>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>