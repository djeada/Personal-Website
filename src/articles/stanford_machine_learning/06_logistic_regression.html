<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Logistic Regression</title>
    <meta content="Logistic regression is a statistical method used for classification in machine learning." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <div class="article-action-buttons"><button class="btn-suggest-edit" title="Suggest Edit">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M4 21h4l11-11-4-4L4 17v4z" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button><button class="btn-create-issue" title="Create Issue">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"></circle>
                        <line stroke="currentColor" stroke-width="2" x1="12" x2="12" y1="8" y2="12"></line>
                        <circle cx="12" cy="16" fill="currentColor" r="1"></circle>
                    </svg>
                </button><button class="btn-download" title="Download">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M12 5v14m0 0l-6-6m6 6l6-6" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button></div>
            <p style="text-align: right;"><i>Last modified: April 14, 2019</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="logistic-regression">Logistic Regression</h2>
            <p>Logistic regression is a statistical method used for classification in machine learning. Unlike linear regression, which predicts continuous values, logistic regression predicts discrete outcomes, like classifying an email as spam or not spam.</p>
            <h3 id="classification">Classification</h3>
            <p>Yields discrete values (e.g., 0 or 1, representing classes).</p>
            <p>Examples:</p>
            <ul>
                <li>Email: Spam (1) or Not Spam (0).</li>
                <li>Online Transaction: Fraudulent (1) or Not Fraudulent (0).</li>
                <li>Tumor Diagnosis: Malignant (1) or Benign (0).</li>
            </ul>
            <h3 id="logistic-regression-vs-linear-regression">Logistic Regression vs Linear Regression</h3>
            <p>Applying linear regression to classification tasks, like cancer diagnosis, may not yield effective results, especially when the data doesn't fit well into a linear model.</p>
            <p><img alt="cancer_classification" src="https://user-images.githubusercontent.com/37275728/201496614-36ec47d4-437e-4d25-82bf-27289489a5a7.png" /></p>
            <h3 id="hypothesis-representation">Hypothesis Representation</h3>
            <ul>
                <li>Classifier output should be between 0 and 1 (probability).</li>
                <li>Hypothesis $h_{\theta}(x) = g(\theta^Tx)$.</li>
                <li>$g(z)$ is the sigmoid or logistic function:</li>
            </ul>
            <p>$$
                g(z) = \frac{1}{1 + e^{-z}}
                $$</p>
            <p>Hypothesis Equation:</p>
            <p>$$
                h_{\theta}(x) = \frac{1}{1+e^{-\theta^Tx}}
                $$</p>
            <ul>
                <li>The output of $h_{\theta}(x)$ is interpreted as the probability of the positive class given the input $x$.
                    $$ h_{\theta}(x) = P(y=1|x\ ;\ \theta) $$</li>
                <li><strong>Example</strong>: If $h_{\theta}(x) = 0.7$ for a tumor, it implies a 70% chance of the tumor being malignant.</li>
            </ul>
            <h3 id="sigmoid-function">Sigmoid Function</h3>
            <p>Visualizes how $h_{\theta}(x)$ translates a linear combination of inputs into a probability:</p>
            <p><img alt="sigmoid" src="https://user-images.githubusercontent.com/37275728/201496643-38a45685-61a5-4af4-bf24-2acaa22ef1ff.png" /></p>
            <h3 id="decision-boundary-in-logistic-regression">Decision Boundary in Logistic Regression</h3>
            <p>The decision boundary in logistic regression is critical for classification tasks. It separates the different classes based on the probability calculated using the sigmoid function.</p>
            <h4 id="linear-decision-boundary">Linear Decision Boundary</h4>
            <ul>
                <li><strong>Principle</strong>: Predict $y = 1$ if the probability is greater than 0.5, else predict $y = 0$.</li>
                <li><strong>Hypothesis</strong>: $h_{\theta}(x) = g(\theta^T x)$, where $g$ is the sigmoid function.</li>
                <li><strong>Predicting $y = 1$</strong>: Occurs when $\theta^T x \geq 0$.</li>
                <li><strong>Predicting $y = 0$</strong>: Occurs when $\theta^T x \leq 0$.</li>
            </ul>
            <h4 id="example-of-a-linear-decision-boundary">Example of a Linear Decision Boundary</h4>
            <p>Hypothesis: </p>
            <p>$$
                h_{\theta}(x) = g(\theta_0 + \theta_1x_1 + \theta_2x_2)
                $$</p>
            <p>Theta Vector:</p>
            <p>$$
                \theta = \begin{bmatrix}
                -3 \\
                1 \\
                1
                \end{bmatrix}
                $$</p>
            <p>Condition for $y = 1$:</p>
            <p>$$
                -3 + x_1 + x_2 \geq 0
                $$</p>
            <p>Hence, the decision boundary is a straight line: $x_2 = -x_1 + 3$.</p>
            <p><img alt="linear_decision_boundary" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/linear_decision_boundary.png" /></p>
            <p>Here's the Python implementation:</p>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Hypothesis function
def hypothesis(theta, X):
    return sigmoid(np.dot(X, theta))

# Predict function
def predict(theta, X):
    return hypothesis(theta, X) &gt;= 0.5

# Define the theta vector
theta = np.array([-3, 1, 1])

# Define the range for x1 and compute the corresponding x2 for the decision boundary
x1_vals = np.linspace(0, 5, 100)
x2_vals = -x1_vals + 3

# Plotting the decision boundary
plt.plot(x1_vals, x2_vals, label=r'$x_2 = -x_1 + 3$')
plt.xlim(0, 5)
plt.ylim(0, 5)
plt.xlabel(r'$x_1$')
plt.ylabel(r'$x_2$')
plt.title('Linear Decision Boundary')
plt.legend()
plt.grid(True)
plt.show()</code></pre>
            </div>
            </p>
            <h4 id="non-linear-decision-boundaries">Non-linear Decision Boundaries</h4>
            <ul>
                <li><strong>Purpose</strong>: To fit more complex, non-linear datasets.</li>
                <li><strong>Approach</strong>: Introduce polynomial terms in the hypothesis.</li>
            </ul>
            <h4 id="example-of-a-non-linear-decision-boundary">Example of a Non-linear Decision Boundary</h4>
            <p>Hypothesis: </p>
            <p>$$
                h_{\theta}(x) = g(\theta_0 + \theta_1x_1 + \theta_2x_1^2 + \theta_3x_2^2)
                $$</p>
            <p>Theta Vector:</p>
            <p>$$
                \theta = \begin{bmatrix}
                -1 \\
                0 \\
                1 \\
                1
                \end{bmatrix}
                $$</p>
            <p>Condition for $y = 1$:</p>
            <p>$$
                x_1^2 + x_2^2 \geq 1
                $$</p>
            <p>This forms a circular decision boundary with radius 1 around the origin: $x_1^2 + x_2^2 = 1$.</p>
            <p><img alt="non_linear_decision_boundary" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/non_linear_decision_boundary.png" /></p>
            <p>Here's the Python implementation:</p>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Hypothesis function for non-linear decision boundary
def hypothesis(theta, X):
    # Compute the polynomial terms
    poly_terms = np.dot(X, theta)
    return sigmoid(poly_terms)

# Predict function
def predict(theta, X):
    return hypothesis(theta, X) &gt;= 0.5

# Define the theta vector
theta = np.array([-1, 0, 1, 1])

# Generate a grid of values for x1 and x2
x1_vals = np.linspace(-2, 2, 400)
x2_vals = np.linspace(-2, 2, 400)
x1, x2 = np.meshgrid(x1_vals, x2_vals)

# Compute the decision boundary condition
decision_boundary = theta[0] + theta[1] * x1 + theta[2] * x1**2 + theta[3] * x2**2

# Plot the decision boundary
plt.contour(x1, x2, decision_boundary, levels=[0], linewidths=2, colors='red')
plt.xlim(-2, 2)
plt.ylim(-2, 2)
plt.xlabel(r'$x_1$')
plt.ylabel(r'$x_2$')
plt.title('Non-linear Decision Boundary')
plt.grid(True)
plt.gca().set_aspect('equal', adjustable='box')
plt.show()</code></pre>
            </div>
            </p>
            <h3 id="cost-function-for-logistic-regression">Cost Function for Logistic Regression</h3>
            <p>Logistic regression uses a different cost function compared to linear regression, tailored to the classification setting.</p>
            <h4 id="training-set-representation">Training Set Representation</h4>
            <p>Consider a training set of $m$ examples:</p>
            <p>$$ {(x^{(1)}, y^{(1)}), ..., (x^{(m)}, y^{(m)})} $$</p>
            <p>where</p>
            <p>$$<br />
                x = \begin{bmatrix}
                x_0 \\
                x_1 \\
                ... \\
                x_n
                \end{bmatrix}
                $$</p>
            <p>with $x_0 = 1$ and $y$ being either 0 or 1.</p>
            <h4 id="linear-regression-cost-function">Linear Regression Cost Function</h4>
            <p>In linear regression, the cost function $J(\theta)$ is defined as:</p>
            <p>$$ J(\theta) = \frac{1}{2m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2 $$</p>
            <h4 id="defining-cost-for-logistic-regression">Defining Cost for Logistic Regression</h4>
            <p>For logistic regression, we define a different "cost" function:</p>
            <p>$$ cost(h_{\theta}(x^{(i)}), y^{(i)}) = \frac{1}{2} (h_{\theta}(x^{(i)}) - y^{(i)})^2 $$</p>
            <p>Redefining $J(\theta)$:</p>
            <p>$$ J(\theta) = \frac{1}{m} \sum_{i=1}^{m}cost(h_{\theta}(x^{(i)}), y^{(i)}) $$</p>
            <p>This cost function for logistic regression is not convex, leading to potential issues with local optima.</p>
            <h4 id="logistic-regression-cost-function">Logistic Regression Cost Function</h4>
            <p>The logistic regression cost function is defined as:</p>
            <p>$$
                cost(h_{\theta}(x), y) = \begin{cases}
                -\log(h_{\theta}(x)) &amp; \text{if } y=1 \\
                -\log(1 - h_{\theta}(x)) &amp; \text{if } y=0
                \end{cases}
                $$</p>
            <p>Then, the overall cost function $J(\theta)$ becomes:</p>
            <p>$$J(\theta) = \frac{1}{m} \sum_{i=1}^{m}[-y^{(i)}\log(h_{\theta}(x^{(i)})) - (1-y^{(i)})\log(1 - h_{\theta}(x^{(i)}))]
                $$</p>
            <h4 id="gradient-of-the-cost-function">Gradient of the Cost Function</h4>
            <p>The gradient of $J(\theta)$ for logistic regression is:</p>
            <p>$$
                \frac{\partial}{\partial \theta_j} J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}
                $$</p>
            <p>Note: While this gradient looks identical to that of linear regression, the formulae differ due to the different definitions of $h_{\theta}(x)$ in linear and logistic regression.</p>
            <p>Here's the Python implementation:</p>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Hypothesis function
def hypothesis(theta, X):
    return sigmoid(np.dot(X, theta))

# Cost function for logistic regression
def compute_cost(theta, X, y):
    m = len(y)
    h = hypothesis(theta, X)
    cost = (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
    return cost

# Gradient of the cost function
def compute_gradient(theta, X, y):
    m = len(y)
    h = hypothesis(theta, X)
    gradient = (1 / m) * np.dot(X.T, (h - y))
    return gradient

# Example usage
if __name__ == "__main__":
    # Sample data (X should include the intercept term)
    X = np.array([[1, 0.5, 1.5],
                  [1, 1.5, 0.5],
                  [1, 3, 3.5],
                  [1, 2, 2.5]])
    y = np.array([0, 0, 1, 1])
    
    # Initial theta
    theta = np.array([0, 0, 0])
    
    # Compute cost and gradient
    cost = compute_cost(theta, X, y)
    gradient = compute_gradient(theta, X, y)
    
    print("Cost:", cost)
    print("Gradient:", gradient)</code></pre>
            </div>
            </p>
            <p>In the example usage, we define a small sample dataset with features $X$ and labels $y$, initialize the theta vector, and compute both the cost and the gradient. The computed cost and gradient are printed out for inspection.</p>
            <h3 id="multiclass-classification-problems">Multiclass Classification Problems</h3>
            <p>Logistic regression can be extended to handle multiclass classification problems through the "one-vs-all" (or "one-vs-rest") method.</p>
            <h4 id="one-vs-all-approach">One-vs-All Approach</h4>
            <p>The one-vs-all strategy involves training multiple binary classifiers, each focused on distinguishing one class from all other classes.</p>
            <h4 id="visualization-of-multiclass-classification">Visualization of Multiclass Classification</h4>
            <p>Consider a dataset with three classes: triangles, crosses, and squares.</p>
            <p><img alt="multiclass_classification" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/multiclass_classification.png" /></p>
            <h4 id="implementing-one-vs-all">Implementing One-vs-All</h4>
            <p>The process involves splitting the training set into separate binary classification problems:</p>
            <ol>
                <li><strong>Triangle vs Others</strong>: Train a classifier $h_{\theta}^{(1)}(x)$ to distinguish triangles (1) from crosses and squares (0).</li>
                <li><strong>Crosses vs Others</strong>: Train another classifier $h_{\theta}^{(2)}(x)$ to distinguish crosses (1) from triangles and squares (0).</li>
                <li><strong>Squares vs Others</strong>: Lastly, train a classifier $h_{\theta}^{(3)}(x)$ to distinguish squares (1) from crosses and triangles (0).</li>
            </ol>
            <p><img alt="one_vs_all" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/one_vs_all.png" /></p>
            <p>To implement the One-vs-All (OvA) approach for multi-class classification, we need to train separate binary classifiers for each class, treating each class as the positive class and all others as the negative class. Here is the step-by-step implementation:</p>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np
from scipy.optimize import minimize

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Hypothesis function
def hypothesis(theta, X):
    return sigmoid(np.dot(X, theta))

# Cost function for logistic regression
def compute_cost(theta, X, y):
    m = len(y)
    h = hypothesis(theta, X)
    cost = (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
    return cost

# Gradient of the cost function
def compute_gradient(theta, X, y):
    m = len(y)
    h = hypothesis(theta, X)
    gradient = (1 / m) * np.dot(X.T, (h - y))
    return gradient

# One-vs-All training function
def one_vs_all(X, y, num_labels, lambda_=0.1):
    m, n = X.shape
    all_theta = np.zeros((num_labels, n + 1))
    
    # Add intercept term to X
    X = np.hstack((np.ones((m, 1)), X))
    
    # Train each classifier
    for c in range(num_labels):
        initial_theta = np.zeros(n + 1)
        options = {'maxiter': 50}
        result = minimize(compute_cost, initial_theta, args=(X, (y == c).astype(int)), method='TNC', jac=compute_gradient, options=options)
        all_theta[c] = result.x
    
    return all_theta

# Prediction function for One-vs-All
def predict_one_vs_all(all_theta, X):
    m = X.shape[0]
    X = np.hstack((np.ones((m, 1)), X))
    predictions = hypothesis(all_theta.T, X)
    return np.argmax(predictions, axis=1)

# Example usage
if __name__ == "__main__":
    # Sample data (X should include the intercept term)
    X = np.array([[0.5, 1.5],
                  [1.5, 0.5],
                  [3, 3.5],
                  [2, 2.5],
                  [1, 1],
                  [3.5, 4],
                  [2.5, 3],
                  [1, 0.5]])
    y = np.array([0, 0, 1, 1, 2, 2, 1, 0])  # 0: Triangle, 1: Cross, 2: Square
    
    # Train One-vs-All classifiers
    num_labels = 3
    all_theta = one_vs_all(X, y, num_labels)
    
    # Make predictions
    predictions = predict_one_vs_all(all_theta, X)
    print("Predictions:", predictions)
    print("Actual labels:", y)</code></pre>
            </div>
            </p>
            <ol>
                <li>We define a small sample dataset with features $X$ and labels $y$.</li>
                <li>The <code>one_vs_all</code> function trains the classifiers.</li>
                <li>The <code>predict_one_vs_all</code> function makes predictions on the dataset.</li>
            </ol>
            <h4 id="classification-decision">Classification Decision</h4>
            <ul>
                <li>When classifying a new example, compute the probability that it belongs to each class using the respective classifiers.</li>
                <li>The class with the highest probability is chosen as the prediction.</li>
            </ul>
            <h2 id="reference">Reference</h2>
            <p>These notes are based on the free video lectures offered by Stanford University, led by Professor Andrew Ng. These lectures are part of the renowned Machine Learning course available on Coursera. For more information and to access the full course, visit the <a href="https://www.coursera.org/learn/machine-learning">Coursera course page</a>.</p>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#logistic-regression">Logistic Regression</a>
                    <ol>
                        <li><a href="#classification">Classification</a></li>
                        <li><a href="#logistic-regression-vs-linear-regression">Logistic Regression vs Linear Regression</a></li>
                        <li><a href="#hypothesis-representation">Hypothesis Representation</a></li>
                        <li><a href="#sigmoid-function">Sigmoid Function</a></li>
                        <li><a href="#decision-boundary-in-logistic-regression">Decision Boundary in Logistic Regression</a>
                            <ol>
                                <li><a href="#linear-decision-boundary">Linear Decision Boundary</a></li>
                                <li><a href="#example-of-a-linear-decision-boundary">Example of a Linear Decision Boundary</a></li>
                                <li><a href="#non-linear-decision-boundaries">Non-linear Decision Boundaries</a></li>
                                <li><a href="#example-of-a-non-linear-decision-boundary">Example of a Non-linear Decision Boundary</a></li>
                            </ol>
                        </li>
                        <li><a href="#cost-function-for-logistic-regression">Cost Function for Logistic Regression</a>
                            <ol>
                                <li><a href="#training-set-representation">Training Set Representation</a></li>
                                <li><a href="#linear-regression-cost-function">Linear Regression Cost Function</a></li>
                                <li><a href="#defining-cost-for-logistic-regression">Defining Cost for Logistic Regression</a></li>
                                <li><a href="#logistic-regression-cost-function">Logistic Regression Cost Function</a></li>
                                <li><a href="#gradient-of-the-cost-function">Gradient of the Cost Function</a></li>
                            </ol>
                        </li>
                        <li><a href="#multiclass-classification-problems">Multiclass Classification Problems</a>
                            <ol>
                                <li><a href="#one-vs-all-approach">One-vs-All Approach</a></li>
                                <li><a href="#visualization-of-multiclass-classification">Visualization of Multiclass Classification</a></li>
                                <li><a href="#implementing-one-vs-all">Implementing One-vs-All</a></li>
                                <li><a href="#classification-decision">Classification Decision</a></li>
                            </ol>
                        </li>
                    </ol>
                </li>
                <li><a href="#reference">Reference</a></li>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/01_introduction_to_machine_learning.html">Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/02_linear_regression.html">Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/03_review_of_linear_algebra.html">Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/04_linear_regression_multiple_variables.html">Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/06_logistic_regression.html">Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/07_regularization.html">Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/08_neural_networks_representation.html">Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/09_neural_networks_learning.html">Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/10_applying_machine_learning_advice.html">Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/11_machine_learning_system_design.html">Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/12_support_vector_machines.html">Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/13_clustering.html">Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/14_dimensionality_reduction.html">Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/15_anomaly_detection.html">Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/16_recommendation_systems.html">Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/17_large_scale_machine_learning.html">Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/18_photo_ocr.html">Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All content here is free to use,
                    but please remember to be respectful and avoid any misuse of the site.
                    If youâ€™d like to get in touch, feel free to reach out via my
                    <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a>
                    or connect with me on
                    <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a>
                    if you have technical questions or ideas to share.
                    Wishing you all the best and a fantastic life ahead!
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
    <div id="pdf-spinner-overlay">
        <div class="spinner"></div>
    </div>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>