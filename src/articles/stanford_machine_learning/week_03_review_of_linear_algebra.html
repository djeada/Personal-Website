<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <title>Linear Algebra Review</title>
    <meta charset="utf-8" />
    <meta content="Last modified: June 11, 2024" name="description" />
    <meta content="Adam Djellouli" name="keywords" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" type="text/css" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <link href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="ie-edge" http-equiv="X-UA-Compatible" />
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul role="menu">
            <li role="menuitem"> <a href="../../index.html" title="Go to Home Page"> Home </a> </li>
            <li role="menuitem"> <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a> </li>
            <li role="menuitem"> <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a> </li>
            <li role="menuitem"> <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a> </li>
            <li role="menuitem"> <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a> </li>
            <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>Last modified: June 11, 2024</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="linear-algebra-review">Linear Algebra Review</h2>
            <p>Linear Algebra forms the backbone of many machine learning algorithms, including linear regression. Understanding matrices and vectors is fundamental in this context.</p>
            <h3 id="matrices-overview">Matrices Overview</h3>
            <ul>
                <li><strong>Definition</strong>: Matrices are rectangular arrays of numbers enclosed in square brackets. They are the cornerstone of organizing and manipulating large sets of data.</li>
                <li><strong>Notation</strong>: Typically denoted by uppercase letters (e.g., A, B, X, Y).</li>
                <li><strong>Dimensions</strong>: Expressed as [Rows x Columns]. The entry in the $i^{th}$ row and $j^{th}$ column of a matrix A is denoted as $A_{(i,j)}$.</li>
            </ul>
            <p><img alt="matrix_element" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/matrix_element.png" /></p>
            <h3 id="vectors-overview">Vectors Overview</h3>
            <ul>
                <li><strong>Definition</strong>: A vector is a special type of matrix with only one column (n x 1 matrix).</li>
                <li><strong>Notation</strong>: Usually represented by lowercase letters. The $i^{th}$ element of vector $v$ is denoted as $v_i$.</li>
                <li><strong>Usage</strong>: Vectors are used to represent data points or features in machine learning.</li>
            </ul>
            <h4 id="vector-representation">Vector Representation</h4>
            <p>A vector can be represented as:</p>
            <p>$$
                y = \begin{bmatrix}
                x_{1} \\
                x_{2} \\
                \vdots \\
                x_{m}
                \end{bmatrix}
                $$</p>
            <p>where $x_{1}, x_{2}, ..., x_{m}$ are the elements of the vector.</p>
            <h3 id="matrix-manipulation">Matrix Manipulation</h3>
            <p>Understanding matrix manipulation is essential in linear algebra and machine learning for organizing and processing data. Here are some fundamental operations:</p>
            <h4 id="matrix-addition">Matrix Addition</h4>
            <p>Element-wise addition of two matrices of the same dimension.</p>
            <p><img alt="matrix_addition" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/matrix_addition.png" /></p>
            <p><strong>Important</strong>: The matrices must have the same number of rows and columns.</p>
            <h4 id="multiplication-by-scalar">Multiplication by Scalar</h4>
            <p>Multiplying every element of a matrix by a scalar value.</p>
            <p><img alt="matrix_mult_scalar" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/matrix_mult_scalar.png" /></p>
            <h4 id="multiplication-by-vector">Multiplication by Vector</h4>
            <p>Multiplying a matrix with a vector.</p>
            <p><img alt="matrix_mult_vector" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/matrix_mult_vector.png" /></p>
            <p><strong>Important</strong>: The number of columns in the matrix must equal the number of elements in the vector.</p>
            <h4 id="multiplication-by-another-matrix">Multiplication by Another Matrix</h4>
            <p>Combining two matrices.</p>
            <p>Procedure:</p>
            <ul>
                <li>Multiply matrix A with each column vector of matrix B.</li>
                <li>This results in a new matrix where each column is the result of multiplying A with a column of B.</li>
            </ul>
            <p><img alt="matrix_mult_matrix" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/matrix_mult_matrix.png" /></p>
            <p><img alt="matrix_mult_column" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/matrix_mult_column.png" /></p>
            <p><strong>Important</strong>: The number of columns in the first matrix must match the number of rows in the second matrix.</p>
            <h2 id="matrix-multiplication-properties">Matrix Multiplication Properties</h2>
            <p>Matrix multiplication, a crucial operation in linear algebra, has specific properties that distinguish it from scalar multiplication.</p>
            <h3 id="lack-of-commutativity">Lack of Commutativity</h3>
            <p>I. For real numbers, multiplication is commutative.</p>
            <p>$$3 \cdot 5 == 5 \cdot 3$$</p>
            <p>II. The commutative property does not hold for matrix multiplication.</p>
            <p>$$A \times B \neq B \times A$$</p>
            <h3 id="associativity">Associativity</h3>
            <p>I. Associative property holds for multiplication of real numbers.</p>
            <p>$$3 \cdot (5 \cdot 2) == (3 \cdot 5) \cdot 2$$</p>
            <p>II. Matrix multiplication is associative.</p>
            <p>$$A \times (B \times C) == (A \times B) \times C$$</p>
            <h2 id="identity-matrix">Identity Matrix</h2>
            <p>I. In scalar multiplication, 1 is the identity element.</p>
            <p>$$z \cdot 1 = z$$</p>
            <p>II. For matrices, the identity matrix $I$ serves as the identity element.</p>
            <p>$$
                I =
                \begin{bmatrix}
                1 &amp; 0 &amp; 0 \\
                0 &amp; 1 &amp; 0 \\
                0 &amp; 0 &amp; 1
                \end{bmatrix}
                $$</p>
            <p>When multiplied by any compatible matrix $A$, it results in $A$ itself.</p>
            <p>$$A \times I = A$$</p>
            <h2 id="matrix-inverse">Matrix Inverse</h2>
            <p>I. For non-zero real numbers, the multiplicative inverse holds.</p>
            <p>$$x \cdot \frac{1}{x} = 1$$</p>
            <p>II. Only square matrices can have an inverse. Not every square matrix has an inverse, analogous to how 0 has no multiplicative inverse in real numbers. Finding matrix inverses involves specific numerical methods.</p>
            <p><img alt="matrix_inverse" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/matrix_inverse.png" /></p>
            <h2 id="matrix-transpose">Matrix Transpose</h2>
            <p>The transpose of a matrix $A$ of size $m \times n$ is another matrix $B$ of size $n \times m$, where the elements are flipped over its diagonal.</p>
            <p>$B_{(j,i)} = A_{(i,j)}$.</p>
            <p><img alt="matrix_transpose" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/matrix_transpose.png" /></p>
            <h3 id="application-in-machine-learning">Application in Machine Learning</h3>
            <ul>
                <li><strong>Data Representation</strong>: Matrices and vectors are used to represent datasets, where each row can be a data point and columns are the features.</li>
                <li><strong>Model Representation</strong>: In linear regression, the hypothesis can be represented as a matrix-vector product, simplifying the computation and representation of multiple features.</li>
                <li><strong>Efficiency</strong>: Linear algebra operations, especially when implemented in optimized libraries, provide efficient computation for large datasets.</li>
            </ul>
            <h3 id="example-house-prices">Example: House Prices</h3>
            <p>Suppose there are multiple hypotheses to predict house prices based on different factors. With a dataset of house sizes, these hypotheses can be applied simultaneously using matrix operations.</p>
            <p>For four houses and three different hypotheses:</p>
            <ul>
                <li>Convert the data into a $4 \times 2$ matrix by adding an extra column of ones (to account for the bias term in linear regression).</li>
                <li>Multiply this matrix with a matrix containing the parameters of the hypotheses.</li>
            </ul>
            <p>This approach demonstrates how matrix multiplication can streamline the application of multiple hypotheses to a dataset, enhancing efficiency and scalability.</p>
            <p><img alt="matrix_mult_use" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/matrix_mult_use.png" /></p>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#linear-algebra-review">Linear Algebra Review</a>
                    <ol>
                        <li><a href="#matrices-overview">Matrices Overview</a></li>
                        <li><a href="#vectors-overview">Vectors Overview</a>
                            <ol>
                                <li><a href="#vector-representation">Vector Representation</a></li>
                            </ol>
                        </li>
                        <li><a href="#matrix-manipulation">Matrix Manipulation</a>
                            <ol>
                                <li><a href="#matrix-addition">Matrix Addition</a></li>
                                <li><a href="#multiplication-by-scalar">Multiplication by Scalar</a></li>
                                <li><a href="#multiplication-by-vector">Multiplication by Vector</a></li>
                                <li><a href="#multiplication-by-another-matrix">Multiplication by Another Matrix</a></li>
                            </ol>
                        </li>
                    </ol>
                </li>
                <li><a href="#matrix-multiplication-properties">Matrix Multiplication Properties</a>
                    <ol>
                        <li><a href="#lack-of-commutativity">Lack of Commutativity</a></li>
                        <li><a href="#associativity">Associativity</a></li>
                    </ol>
                </li>
                <li><a href="#identity-matrix">Identity Matrix</a></li>
                <li><a href="#matrix-inverse">Matrix Inverse</a></li>
                <li><a href="#matrix-transpose">Matrix Transpose</a>
                    <ol>
                        <li><a href="#application-in-machine-learning">Application in Machine Learning</a></li>
                        <li><a href="#example-house-prices">Example: House Prices</a></li>
                    </ol>
                </li>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_01_introduction_to_machine_learning.html">Week 01 Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_02_linear_regression.html">Week 02 Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_03_review_of_linear_algebra.html">Week 03 Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_04_linear_regression_multiple_variables.html">Week 04 Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_06_logistic_regression.html">Week 06 Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_07_regularization.html">Week 07 Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_08_neural_networks_representation.html">Week 08 Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_09_neural_networks_learning.html">Week 09 Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_10_applying_machine_learning_advice.html">Week 10 Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_11_machine_learning_system_design.html">Week 11 Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_12_support_vector_machines.html">Week 12 Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_13_clustering.html">Week 13 Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_14_dimensionality_reduction.html">Week 14 Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_15_anomaly_detection.html">Week 15 Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_16_recommendation_systems.html">Week 16 Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_17_large_scale_machine_learning.html">Week 17 Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_18_photo_ocr.html">Week 18 Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
</body>

</html>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>