<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Linear Regression with Multiple Variables</title>
    <meta content="Multiple linear regression extends the concept of simple linear regression to multiple independent variables." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper"><article-section id="article-body">
            <div class="article-action-buttons"><button class="btn-suggest-edit" title="Suggest Edit">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M4 21h4l11-11-4-4L4 17v4z" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button><button class="btn-create-issue" title="Create Issue">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"></circle>
                        <line stroke="currentColor" stroke-width="2" x1="12" x2="12" y1="8" y2="12"></line>
                        <circle cx="12" cy="16" fill="currentColor" r="1"></circle>
                    </svg>
                </button><button class="btn-download" title="Download">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M12 5v14m0 0l-6-6m6 6l6-6" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button></div>
            <p style="text-align: right;"><i>Last modified: March 11, 2020</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="linear-regression-with-multiple-variables">Linear Regression with Multiple Variables</h2>
            <p>Multiple linear regression extends the concept of simple linear regression to multiple independent variables. This technique models a dependent variable as a linear combination of several independent variables.</p>
            <ul>
                <li><strong>Variables</strong>: The model uses several variables (or features), such as house size, number of bedrooms, floors, and age.</li>
                <li>$n$: Number of features (e.g., 4 in our example).</li>
                <li>$m$: Number of examples (rows in data).</li>
                <li>$x^i$: Feature vector of the $i^{th}$ training example.</li>
                <li>$x_j^i$: Value of the $j^{th}$ feature in the $i^{th}$ training set.</li>
            </ul>
            <h3 id="hypothesis-function">Hypothesis Function</h3>
            <p>The hypothesis in multiple linear regression combines all the features:</p>
            <p>$$ h_{\theta}(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + \theta_4x_4 $$</p>
            <p>For simplification, introduce $x_0 = 1$ (bias term), making the feature vector $n + 1$-dimensional.</p>
            <p>$$ h_{\theta}(x) = \theta^T X $$</p>
            <h3 id="cost-function">Cost Function</h3>
            <p>The cost function measures the discrepancy between the model's predictions and actual values. It is defined as:</p>
            <p>$$ J(\theta_0, \theta_1, ..., \theta_n) = \frac{1}{2m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2 $$</p>
            <h3 id="gradient-descent-for-multiple-variables">Gradient Descent for Multiple Variables</h3>
            <p>Gradient descent is employed to find the optimal parameters that minimize the cost function.</p>
            <div>
                <pre><code class="language-plaintext">Î¸ = [0] * n
while not converged:
  for j in [0, ..., n]:
      Î¸_j := Î¸_j - Î± âˆ‚/âˆ‚Î¸_j J(Î¸_0, ..., Î¸_n)</code></pre>
            </div>
            <ul>
                <li><strong>Simultaneous Update</strong>: Adjust each $\theta_j$â€‹ (from 0 to nn) simultaneously.</li>
                <li><strong>Learning Rate ($\alpha$)</strong>: Determines the step size in each iteration.</li>
                <li><strong>Partial Derivative</strong>: Represents the direction and rate of change of the cost function with respect to each $\theta_j$â€‹.</li>
            </ul>
            <p>Here is a Python code that demonstrates the gradient descent algorithm for multiple variables with mock data:</p>
            <div>
                <pre><code class="language-python">import numpy as np

# Mock data
# Features (x0, x1, x2)
X = np.array([
    [1, 2, 3],
    [1, 3, 4],
    [1, 4, 5],
    [1, 5, 6]
])

# Target values
y = np.array([7, 10, 13, 16])

# Parameters
alpha = 0.01  # Learning rate
num_iterations = 1000  # Number of iterations for gradient descent

# Initialize theta (parameters) to zeros
theta = np.zeros(X.shape[1])

# Cost function
def compute_cost(X, y, theta):
    m = len(y)
    predictions = X.dot(theta)
    cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)
    return cost

# Gradient descent algorithm
def gradient_descent(X, y, theta, alpha, num_iterations):
    m = len(y)
    cost_history = np.zeros(num_iterations)
    
    for iteration in range(num_iterations):
        # Compute the prediction error
        error = X.dot(theta) - y
        
        # Update theta values simultaneously
        for j in range(len(theta)):
            partial_derivative = (1 / m) * np.sum(error * X[:, j])
            theta[j] = theta[j] - alpha * partial_derivative
        
        # Save the cost for the current iteration
        cost_history[iteration] = compute_cost(X, y, theta)
    
    return theta, cost_history

# Run gradient descent
theta, cost_history = gradient_descent(X, y, theta, alpha, num_iterations)

print("Optimized theta:", theta)
print("Final cost:", cost_history[-1])

# Plotting the cost function history
import matplotlib.pyplot as plt

plt.plot(range(num_iterations), cost_history, 'b')
plt.xlabel('Number of iterations')
plt.ylabel('Cost J')
plt.title('Cost function history')
plt.show()</code></pre>
            </div>
            <ul>
                <li>In the context of multiple variables, gradient descent adjusts each parameter theta simultaneously.</li>
                <li>The cost function typically measures the difference between predicted and actual values.</li>
                <li>A common cost function used in linear regression is the mean squared error.</li>
                <li>The learning rate, represented by alpha, determines the step size in each iteration of gradient descent.</li>
                <li>Choosing an appropriate learning rate is crucial, as too high a value can cause divergence, while too low a value can slow convergence.</li>
                <li>Gradient descent involves computing the partial derivative of the cost function with respect to each parameter theta.</li>
                <li>The partial derivative indicates the direction and rate of change of the cost function concerning a specific parameter.</li>
                <li>In each iteration, the parameters are updated by subtracting the product of the learning rate and the partial derivative.</li>
                <li>The process continues until the parameters converge, meaning changes in the cost function are minimal.</li>
                <li>To visualize the convergence, the cost function value is often plotted over iterations.</li>
                <li>Mock data can be used to illustrate the gradient descent process, involving features matrix X and target values vector y.</li>
                <li>Initialization of parameters to zeros is a common starting point in gradient descent.</li>
                <li>The prediction error is calculated as the difference between the predicted values and the actual target values.</li>
                <li>Updating theta values involves iterating through each parameter and adjusting it based on the computed partial derivatives.</li>
                <li>Tracking the cost function value at each iteration helps in understanding the convergence behavior.</li>
                <li>The optimized theta values are obtained after the specified number of iterations or when convergence criteria are met.</li>
            </ul>
            <p><img alt="Figure_1" src="https://github.com/djeada/Stanford-Machine-Learning/assets/37275728/8267dcbe-b7a0-43b5-932c-6b22c0562d1f" /></p>
            <h3 id="feature-scaling">Feature Scaling</h3>
            <p>When features have different scales, gradient descent may converge slowly.</p>
            <ul>
                <li><strong>Example</strong>: If $x_1$ = size (0 - 2000 feet) and $x_2$ = number of bedrooms (1-5), the cost function contours become skewed, leading to a longer path to convergence.</li>
                <li><strong>Solution</strong>: Scale the features to have comparable ranges.</li>
            </ul>
            <p><img alt="feature_scaling" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/feature_scaling.png" /></p>
            <p>Here is the Python code that focuses on feature scaling using mock data:</p>
            <div>
                <pre><code class="language-python">import numpy as np

# Mock data
# Features (x0, x1, x2)
X = np.array([
    [1, 2000, 3],
    [1, 1600, 4],
    [1, 2400, 2],
    [1, 3000, 5]
])

# Function to perform feature scaling
def feature_scaling(X):
    # Exclude the first column (intercept term)
    X_scaled = X.copy()
    for i in range(1, X.shape[1]):
        mean = np.mean(X[:, i])
        std = np.std(X[:, i])
        X_scaled[:, i] = (X[:, i] - mean) / std
    return X_scaled

# Apply feature scaling
X_scaled = feature_scaling(X)

print("Original Features:\n", X)
print("Scaled Features:\n", X_scaled)</code></pre>
            </div>
            <ul>
                <li>In the context of implementing feature scaling in Python, one would first compute the mean and standard deviation of each feature.</li>
                <li>The next step involves adjusting each feature value by subtracting the mean and dividing by the standard deviation.</li>
                <li>This transformation ensures that each feature contributes equally to the cost function, facilitating faster and more stable convergence of gradient descent.</li>
                <li>The feature scaling process is crucial for algorithms that rely on distance measurements, such as gradient descent, as it helps in achieving better performance and accuracy.</li>
                <li>By scaling features appropriately, one can prevent the gradient descent algorithm from being dominated by features with larger scales, ensuring a more balanced optimization process.</li>
            </ul>
            <h3 id="mean-normalization">Mean Normalization</h3>
            <p>Adjust each feature $x_i$ by subtracting the mean and dividing by the range (max - min).</p>
            <p><img alt="mean_normalization" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/mean_normalization.png" /></p>
            <p>Transforms the features to have approximately zero mean, aiding in faster convergence.</p>
            <p>Below is the Python code that demonstrates mean normalization using mock data:</p>
            <div>
                <pre><code class="language-python">import numpy as np

# Mock data
# Features (x0, x1, x2)
X = np.array([
    [1, 2000, 3],
    [1, 1600, 4],
    [1, 2400, 2],
    [1, 3000, 5]
])

# Function to perform mean normalization
def mean_normalization(X):
    # Exclude the first column (intercept term)
    X_normalized = X.copy()
    for i in range(1, X.shape[1]):
        mean = np.mean(X[:, i])
        min_val = np.min(X[:, i])
        max_val = np.max(X[:, i])
        X_normalized[:, i] = (X[:, i] - mean) / (max_val - min_val)
    return X_normalized

# Apply mean normalization
X_normalized = mean_normalization(X)

print("Original Features:\n", X)
print("Mean Normalized Features:\n", X_normalized)</code></pre>
            </div>
            <ul>
                <li>The purpose of mean normalization is to transform the features so that they have approximately zero mean, which aids in faster convergence of optimization algorithms like gradient descent.</li>
                <li>When features have different scales, optimization algorithms may converge slowly or get stuck in local minima. Mean normalization helps to bring all features to a similar scale.</li>
                <li>In the process of mean normalization, for each feature, the mean value is first calculated.</li>
                <li>Next, the range of the feature is determined by subtracting the minimum value from the maximum value.</li>
                <li>Each feature value is then adjusted by subtracting the mean and dividing by the range. This results in features with values centered around zero.</li>
                <li>Mean normalization is particularly useful for algorithms that are sensitive to the scale of input features, such as gradient descent.</li>
                <li>By ensuring that features have similar scales, mean normalization prevents features with larger scales from dominating the learning process.</li>
                <li>The transformed features, having approximately zero mean, contribute to a more balanced and efficient optimization process.</li>
                <li>In Python we are iterating over each feature, computing the mean, minimum, and maximum values, and applying the normalization formula.</li>
                <li>This preprocessing step is crucial for improving the performance of machine learning models, especially those relying on distance measurements or gradient-based optimization.</li>
            </ul>
            <h3 id="learning-rate-alpha-">Learning Rate $\alpha$</h3>
            <ul>
                <li><strong>Monitoring</strong>: Plot $\min J(\theta)$ against the number of iterations to observe how $J(\theta)$ decreases.</li>
                <li><strong>Signs of Proper Functioning</strong>: $J(\theta)$ should decrease with every iteration.</li>
                <li><strong>Iterative Adjustment</strong>: Avoid hard-coding iteration thresholds. Instead, use results to adjust future runs.</li>
            </ul>
            <p><img alt="min_cost_function" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/min_cost_function.png" /></p>
            <h3 id="automatic-convergence-tests">Automatic Convergence Tests</h3>
            <ul>
                <li><strong>Goal</strong>: Determine if $J(\theta)$ changes by a small enough threshold, indicating convergence.</li>
                <li><strong>Challenge</strong>: Choosing an appropriate threshold can be difficult.</li>
                <li><strong>Indicator of Incorrect $\alpha$</strong>: If $J(\theta)$ increases, it suggests the need for a smaller learning rate.</li>
            </ul>
            <p><img alt="alpha_big" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/alpha_big.png" /></p>
            <ul>
                <li><strong>Adjusting $\alpha$</strong>: If the learning rate is too high, you might overshoot the minimum. Conversely, if $\alpha$ is too small, convergence becomes inefficiently slow.</li>
            </ul>
            <p><img alt="alpha_small" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/alpha_small.png" /></p>
            <h3 id="features-and-polynomial-regression">Features and Polynomial Regression</h3>
            <ul>
                <li><strong>Polynomial Regression</strong>: An advanced form of linear regression where the relationship between the independent variable $x$ and the dependent variable $y$ is modeled as an $n^{th}$ degree polynomial.</li>
                <li><strong>Application</strong>: Can fit data better than simple linear regression by capturing non-linear relationships.</li>
                <li><strong>Consideration</strong>: Balancing between overfitting (too complex model) and underfitting (too simple model).</li>
            </ul>
            <p><img alt="polynomial_regression" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/polynomial_regression.png" /></p>
            <h3 id="normal-equation">Normal Equation</h3>
            <ul>
                <li><strong>Normal Equation</strong>: Provides an analytical solution to the linear regression problem.</li>
                <li><strong>Alternative to Gradient Descent</strong>: Unlike the iterative nature of gradient descent, the normal equation solves for $\theta$ directly.</li>
            </ul>
            <h4 id="procedure">Procedure</h4>
            <ul>
                <li>$\theta$ is an $n+1$ dimensional vector.</li>
                <li>The cost function $J(\theta)$ takes $\theta$ as an argument.</li>
                <li>Minimize $J(\theta)$ by setting its partial derivatives with respect to $\theta_j$ to zero and solving for each $\theta_j$.</li>
            </ul>
            <h4 id="example">Example</h4>
            <ul>
                <li>Given $m=4$ training examples and $n=4$ features.</li>
                <li>Add an extra column $x_0$ (bias term) to form an $[m \times n+1]$ matrix (design matrix X).</li>
                <li>Construct a $[m \times 1]$ column vector $y$.</li>
                <li>Calculate $\theta$ using:</li>
            </ul>
            <p>$$ \theta = (X^TX)^{-1}X^Ty $$</p>
            <p><img alt="normal_eq_table" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/normal_eq_table.png" />
                <img alt="normal_eq_matrix" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/normal_eq_matrix.png" />
            </p>
            <p>The computed $\theta$ values minimize the cost function for the given training data.</p>
            <p>Here is the Python code that uses the provided data to solve for ( \theta ) using the normal equation, and then plots the solution:</p>
            <div>
                <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# Given data
# Features (x0, x1, x2, x3, x4)
X = np.array([
    [1, 2104, 5, 1, 45],
    [1, 1416, 3, 2, 40],
    [1, 1534, 3, 2, 30],
    [1, 852, 2, 1, 36]
])

# Target values
y = np.array([460, 232, 315, 178])

# Normal equation: theta = (X^T * X)^-1 * X^T * y
def normal_equation(X, y):
    X_transpose = X.T
    theta = np.linalg.inv(X_transpose.dot(X)).dot(X_transpose).dot(y)
    return theta

# Calculate theta using the normal equation for multivariable regression
theta = normal_equation(X, y)
print("Calculated theta values for multivariable regression:", theta)

# Using the first feature (size) for plotting the regression line
sizes = X[:, 1]
predicted_prices = X.dot(theta)

# Plotting the regression line for multivariable regression
plt.scatter(sizes, y, color='red', label='Actual Prices')
plt.plot(sizes, predicted_prices, color='blue', label='Predicted Prices', linestyle='--')
plt.xlabel('Size (square feet)')
plt.ylabel('Price ($1000)')
plt.title('House Prices vs. Size (Multivariable Regression)')
plt.legend()
plt.show()

# Simple Linear Regression with size as the only feature
X_simple = X[:, [0, 1]]  # Only intercept term and size feature
theta_simple = normal_equation(X_simple, y)

# Predicting using the model with only size
predicted_prices_simple = X_simple.dot(theta_simple)

# Sorting the data by size for a proper line plot
sorted_indices = X[:, 1].argsort()
sizes_sorted = sizes[sorted_indices]
y_sorted = y[sorted_indices]
predicted_prices_sorted = predicted_prices_simple[sorted_indices]

# Plotting the regression line with size as the only feature
plt.scatter(sizes, y, color='red', label='Actual Prices')
plt.plot(sizes_sorted, predicted_prices_sorted, color='blue', label='Predicted Prices', linestyle='--')
plt.xlabel('Size (square feet)')
plt.ylabel('Price ($1000)')
plt.title('House Prices vs. Size (Simple Linear Regression)')
plt.legend()
plt.show()

print("Calculated theta values for simple linear regression:", theta_simple)</code></pre>
            </div>
            <ul>
                <li>To implement the normal equation, the transpose of the design matrix ( X ) is first computed. This step involves swapping the rows and columns of ( X ).</li>
                <li>Next, the product of the transpose of ( X ) and ( X ) is calculated, resulting in a square matrix.</li>
                <li>The inverse of this square matrix is then found. This step can be computationally expensive for large matrices but is feasible for smaller datasets.</li>
                <li>The result of the inversion is then multiplied by the transpose of ( X ) and the target vector ( y ) to obtain the parameter vector ( \theta ).</li>
                <li>Using the calculated ( \theta ) values, predictions can be made by multiplying the design matrix ( X ) by ( \theta ).</li>
                <li>To validate the model, the predicted values are compared with the actual target values. Plotting these values can help visualize the model's performance.</li>
                <li>In the given example, the design matrix ( X ) includes an intercept term and features such as size, number of bedrooms, number of floors, and age of the house. The target vector ( y ) represents house prices.</li>
                <li>The normal equation provides a straightforward solution for linear regression problems, especially when the number of features is relatively small. It eliminates the need for tuning hyperparameters like the learning rate and avoids the iterative nature of gradient descent.</li>
                <li>However, for datasets with a large number of features, the normal equation can become computationally impractical due to the matrix inversion step.</li>
            </ul>
            <p><img alt="output(5)" src="https://github.com/djeada/Stanford-Machine-Learning/assets/37275728/fad034dc-1e3f-4865-af0c-a7ec12be9e3b" /></p>
            <h3 id="gradient-descent-vs-normal-equation">Gradient Descent vs Normal Equation</h3>
            <p>Comparing these two methods helps understand their practical applications:</p>
            <p>
            <table>
                <tr>
                    <td>Aspect</td>
                    <td>Gradient Descent</td>
                    <td>Normal Equation</td>
                </tr>
                <tr>
                    <td>Learning Rate</td>
                    <td>Requires selecting a learning rate</td>
                    <td>No learning rate needed</td>
                </tr>
                <tr>
                    <td>Iterations</td>
                    <td>Numerous iterations needed</td>
                    <td>Direct computation without iterations</td>
                </tr>
                <tr>
                    <td>Efficiency</td>
                    <td>Works well for large $n$ (even millions)</td>
                    <td>Becomes slow for large $n$</td>
                </tr>
                <tr>
                    <td>Use Case</td>
                    <td>Preferred for very large feature sets</td>
                    <td>Ideal for smaller feature sets</td>
                </tr>
            </table>
            </p>
            <p>Understanding when to use polynomial regression, and choosing between gradient descent and the normal equation, is crucial in developing efficient and effective linear regression models.</p>
            <h2 id="reference">Reference</h2>
            <p>These notes are based on the free video lectures offered by Stanford University, led by Professor Andrew Ng. These lectures are part of the renowned Machine Learning course available on Coursera. For more information and to access the full course, visit the <a href="https://www.coursera.org/learn/machine-learning">Coursera course page</a>.</p>
        </article-section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#linear-regression-with-multiple-variables">Linear Regression with Multiple Variables</a>
                    <ol>
                        <li><a href="#hypothesis-function">Hypothesis Function</a></li>
                        <li><a href="#cost-function">Cost Function</a></li>
                        <li><a href="#gradient-descent-for-multiple-variables">Gradient Descent for Multiple Variables</a></li>
                        <li><a href="#feature-scaling">Feature Scaling</a></li>
                        <li><a href="#mean-normalization">Mean Normalization</a></li>
                        <li><a href="#learning-rate-alpha-">Learning Rate $\alpha$</a></li>
                        <li><a href="#automatic-convergence-tests">Automatic Convergence Tests</a></li>
                        <li><a href="#features-and-polynomial-regression">Features and Polynomial Regression</a></li>
                        <li><a href="#normal-equation">Normal Equation</a>
                            <ol>
                                <li><a href="#procedure">Procedure</a></li>
                                <li><a href="#example">Example</a></li>
                            </ol>
                        </li>
                        <li><a href="#gradient-descent-vs-normal-equation">Gradient Descent vs Normal Equation</a></li>
                    </ol>
                </li>
                <li><a href="#reference">Reference</a></li>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/01_introduction_to_machine_learning.html">Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/02_linear_regression.html">Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/03_review_of_linear_algebra.html">Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/04_linear_regression_multiple_variables.html">Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/06_logistic_regression.html">Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/07_regularization.html">Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/08_neural_networks_representation.html">Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/09_neural_networks_learning.html">Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/10_applying_machine_learning_advice.html">Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/11_machine_learning_system_design.html">Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/12_support_vector_machines.html">Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/13_clustering.html">Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/14_dimensionality_reduction.html">Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/15_anomaly_detection.html">Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/16_recommendation_systems.html">Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/17_large_scale_machine_learning.html">Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/18_photo_ocr.html">Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>Thank you for visiting my personal website. All content here is free to use, but please remember to be respectful and avoid any misuse of the site. If youâ€™d like to get in touch, feel free to reach out via my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or connect with me on <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have technical questions or ideas to share. Wishing you all the best and a fantastic life ahead!</p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
    <div id="pdf-spinner-overlay">
        <div class="spinner"></div>
    </div>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>