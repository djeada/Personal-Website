<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Linear Regression with Multiple Variables</title>
    <meta content="Multiple linear regression extends the concept of simple linear regression to multiple independent variables." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>Last modified: June 15, 2024</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="linear-regression-with-multiple-variables">Linear Regression with Multiple Variables</h2>
            <p>Multiple linear regression extends the concept of simple linear regression to multiple independent variables. This technique models a dependent variable as a linear combination of several independent variables.</p>
            <ul>
                <li><strong>Variables</strong>: The model uses several variables (or features), such as house size, number of bedrooms, floors, and age.</li>
                <li>$n$: Number of features (e.g., 4 in our example).</li>
                <li>$m$: Number of examples (rows in data).</li>
                <li>$x^i$: Feature vector of the $i^{th}$ training example.</li>
                <li>$x_j^i$: Value of the $j^{th}$ feature in the $i^{th}$ training set.</li>
            </ul>
            <h3 id="hypothesis-function">Hypothesis Function</h3>
            <p>The hypothesis in multiple linear regression combines all the features:</p>
            <p>$$ h_{\theta}(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + \theta_4x_4 $$</p>
            <p>For simplification, introduce $x_0 = 1$ (bias term), making the feature vector $n + 1$-dimensional.</p>
            <p>$$ h_{\theta}(x) = \theta^T X $$</p>
            <h3 id="cost-function">Cost Function</h3>
            <p>The cost function measures the discrepancy between the model's predictions and actual values. It is defined as:</p>
            <p>$$ J(\theta_0, \theta_1, ..., \theta_n) = \frac{1}{2m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2 $$</p>
            <h3 id="gradient-descent-for-multiple-variables">Gradient Descent for Multiple Variables</h3>
            <p>Gradient descent is employed to find the optimal parameters that minimize the cost function.</p>
            <p>
            <div>
                <pre><code class="language-plaintext">Î¸ = [0] * n
while not converged:
  for j in [0, ..., n]:
      Î¸_j := Î¸_j - Î± âˆ‚/âˆ‚Î¸_j J(Î¸_0, ..., Î¸_n)</code></pre>
            </div>
            </p>
            <ul>
                <li><strong>Simultaneous Update</strong>: Adjust each $\theta_j$â€‹ (from 0 to nn) simultaneously.</li>
                <li><strong>Learning Rate ($\alpha$)</strong>: Determines the step size in each iteration.</li>
                <li><strong>Partial Derivative</strong>: Represents the direction and rate of change of the cost function with respect to each $\theta_j$â€‹.</li>
            </ul>
            <h3 id="feature-scaling">Feature Scaling</h3>
            <p>When features have different scales, gradient descent may converge slowly.</p>
            <ul>
                <li><strong>Example</strong>: If $x_1$ = size (0 - 2000 feet) and $x_2$ = number of bedrooms (1-5), the cost function contours become skewed, leading to a longer path to convergence.</li>
                <li><strong>Solution</strong>: Scale the features to have comparable ranges.</li>
            </ul>
            <p><img alt="feature_scaling" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/feature_scaling.png" /></p>
            <h3 id="mean-normalization">Mean Normalization</h3>
            <p>Adjust each feature $x_i$ by subtracting the mean and dividing by the range (max - min).</p>
            <p><img alt="mean_normalization" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/mean_normalization.png" /></p>
            <p>Transforms the features to have approximately zero mean, aiding in faster convergence.</p>
            <h3 id="learning-rate-alpha-">Learning Rate $\alpha$</h3>
            <ul>
                <li><strong>Monitoring</strong>: Plot $\min J(\theta)$ against the number of iterations to observe how $J(\theta)$ decreases.</li>
                <li><strong>Signs of Proper Functioning</strong>: $J(\theta)$ should decrease with every iteration.</li>
                <li><strong>Iterative Adjustment</strong>: Avoid hard-coding iteration thresholds. Instead, use results to adjust future runs.</li>
            </ul>
            <p><img alt="min_cost_function" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/min_cost_function.png" /></p>
            <h3 id="automatic-convergence-tests">Automatic Convergence Tests</h3>
            <ul>
                <li><strong>Goal</strong>: Determine if $J(\theta)$ changes by a small enough threshold, indicating convergence.</li>
                <li><strong>Challenge</strong>: Choosing an appropriate threshold can be difficult.</li>
                <li><strong>Indicator of Incorrect $\alpha$</strong>: If $J(\theta)$ increases, it suggests the need for a smaller learning rate.</li>
            </ul>
            <p><img alt="alpha_big" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/alpha_big.png" /></p>
            <ul>
                <li><strong>Adjusting $\alpha$</strong>: If the learning rate is too high, you might overshoot the minimum. Conversely, if $\alpha$ is too small, convergence becomes inefficiently slow.</li>
            </ul>
            <p><img alt="alpha_small" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/alpha_small.png" /></p>
            <h3 id="features-and-polynomial-regression">Features and Polynomial Regression</h3>
            <ul>
                <li><strong>Polynomial Regression</strong>: An advanced form of linear regression where the relationship between the independent variable $x$ and the dependent variable $y$ is modeled as an $n^{th}$ degree polynomial.</li>
                <li><strong>Application</strong>: Can fit data better than simple linear regression by capturing non-linear relationships.</li>
                <li><strong>Consideration</strong>: Balancing between overfitting (too complex model) and underfitting (too simple model).</li>
            </ul>
            <p><img alt="polynomial_regression" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/polynomial_regression.png" /></p>
            <h3 id="normal-equation">Normal Equation</h3>
            <ul>
                <li><strong>Normal Equation</strong>: Provides an analytical solution to the linear regression problem.</li>
                <li><strong>Alternative to Gradient Descent</strong>: Unlike the iterative nature of gradient descent, the normal equation solves for $\theta$ directly.</li>
            </ul>
            <h4 id="procedure">Procedure</h4>
            <ul>
                <li>$\theta$ is an $n+1$ dimensional vector.</li>
                <li>The cost function $J(\theta)$ takes $\theta$ as an argument.</li>
                <li>Minimize $J(\theta)$ by setting its partial derivatives with respect to $\theta_j$ to zero and solving for each $\theta_j$.</li>
            </ul>
            <h4 id="example">Example</h4>
            <ul>
                <li>Given $m=4$ training examples and $n=4$ features.</li>
                <li>Add an extra column $x_0$ (bias term) to form an $[m \times n+1]$ matrix (design matrix X).</li>
                <li>Construct a $[m \times 1]$ column vector $y$.</li>
                <li>Calculate $\theta$ using:</li>
            </ul>
            <p>$$ \theta = (X^TX)^{-1}X^Ty $$</p>
            <p><img alt="normal_eq_table" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/normal_eq_table.png" />
                <img alt="normal_eq_matrix" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/normal_eq_matrix.png" />
            </p>
            <p>The computed $\theta$ values minimize the cost function for the given training data.</p>
            <h3 id="gradient-descent-vs-normal-equation">Gradient Descent vs Normal Equation</h3>
            <p>Comparing these two methods helps understand their practical applications:</p>
            <p>
            <table>
                <tr>
                    <td>Aspect</td>
                    <td>Gradient Descent</td>
                    <td>Normal Equation</td>
                </tr>
                <tr>
                    <td>Learning Rate</td>
                    <td>Requires selecting a learning rate</td>
                    <td>No learning rate needed</td>
                </tr>
                <tr>
                    <td>Iterations</td>
                    <td>Numerous iterations needed</td>
                    <td>Direct computation without iterations</td>
                </tr>
                <tr>
                    <td>Efficiency</td>
                    <td>Works well for large $n$ (even millions)</td>
                    <td>Becomes slow for large $n$</td>
                </tr>
                <tr>
                    <td>Use Case</td>
                    <td>Preferred for very large feature sets</td>
                    <td>Ideal for smaller feature sets</td>
                </tr>
            </table>
            </p>
            <p>Understanding when to use polynomial regression, and choosing between gradient descent and the normal equation, is crucial in developing efficient and effective linear regression models.</p>
            <h2 id="reference">Reference</h2>
            <p>These notes are derived from the free video lectures offered by Stanford University, led by Professor Andrew Ng. These lectures are part of the renowned Machine Learning course available on Coursera. For more information and to access the full course, visit the <a href="https://www.coursera.org/learn/machine-learning">Coursera course page</a>.</p>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#linear-regression-with-multiple-variables">Linear Regression with Multiple Variables</a>
                    <ol>
                        <li><a href="#hypothesis-function">Hypothesis Function</a></li>
                        <li><a href="#cost-function">Cost Function</a></li>
                        <li><a href="#gradient-descent-for-multiple-variables">Gradient Descent for Multiple Variables</a></li>
                        <li><a href="#feature-scaling">Feature Scaling</a></li>
                        <li><a href="#mean-normalization">Mean Normalization</a></li>
                        <li><a href="#learning-rate-alpha-">Learning Rate $\alpha$</a></li>
                        <li><a href="#automatic-convergence-tests">Automatic Convergence Tests</a></li>
                        <li><a href="#features-and-polynomial-regression">Features and Polynomial Regression</a></li>
                        <li><a href="#normal-equation">Normal Equation</a>
                            <ol>
                                <li><a href="#procedure">Procedure</a></li>
                                <li><a href="#example">Example</a></li>
                            </ol>
                        </li>
                        <li><a href="#gradient-descent-vs-normal-equation">Gradient Descent vs Normal Equation</a></li>
                    </ol>
                </li>
                <li><a href="#reference">Reference</a></li>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/01_introduction_to_machine_learning.html">Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/02_linear_regression.html">Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/03_review_of_linear_algebra.html">Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/04_linear_regression_multiple_variables.html">Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/06_logistic_regression.html">Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/07_regularization.html">Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/08_neural_networks_representation.html">Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/09_neural_networks_learning.html">Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/10_applying_machine_learning_advice.html">Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/11_machine_learning_system_design.html">Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/12_support_vector_machines.html">Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/13_clustering.html">Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/14_dimensionality_reduction.html">Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/15_anomaly_detection.html">Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/16_recommendation_systems.html">Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/17_large_scale_machine_learning.html">Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/18_photo_ocr.html">Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>