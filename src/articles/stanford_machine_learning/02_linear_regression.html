<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Linear Regression in Depth</title>
    <meta content="Linear Regression is a fundamental type of supervised learning algorithm in statistics and machine learning." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>Last modified: June 15, 2024</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="linear-regression-in-depth">Linear Regression in Depth</h2>
            <p>Linear Regression is a fundamental type of supervised learning algorithm in statistics and machine learning. It's utilized for modeling and analyzing the relationship between a dependent variable and one or more independent variables. The goal is to predict continuous output values based on the input variables.</p>
            <ul>
                <li><strong>Purpose</strong>: To predict a continuous outcome based on one or more predictor variables.</li>
                <li><strong>Model</strong>: The output variable (y) is assumed to be a linear combination of the input variables (x), with coefficients (Î¸) representing weights.</li>
            </ul>
            <h3 id="mathematical-model">Mathematical Model</h3>
            <p>The hypothesis function in linear regression is given by:</p>
            <p>$$
                h_{\theta}(x) = \theta_0 + \theta_1x
                $$</p>
            <p>where:</p>
            <ul>
                <li>$h_{\theta}(x)$ is the predicted value,</li>
                <li>$\theta_0$ is the y-intercept (bias term),</li>
                <li>$\theta_1$ is the slope (weight for the feature x).</li>
            </ul>
            <h3 id="cost-function-mean-squared-error-">Cost Function (Mean Squared Error)</h3>
            <p>The cost function in linear regression measures how far off the predictions are from the actual outcomes. It's commonly represented as:</p>
            <p>$$
                J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2
                $$</p>
            <p>This is the mean squared error (MSE) cost function, where:</p>
            <ul>
                <li>$m$ is the number of training examples,</li>
                <li>$x^{(i)}$ and $y^{(i)}$ are the input and output of the $i^{th}$ training example.</li>
            </ul>
            <h3 id="optimization-gradient-descent">Optimization: Gradient Descent</h3>
            <p>To find the optimal parameters ($\theta_0$ and $\theta_1$), we use gradient descent to minimize the cost function. The gradient descent algorithm iteratively adjusts the parameters to reduce the cost.</p>
            <h3 id="notation">Notation</h3>
            <ul>
                <li>$m$: Number of training examples.</li>
                <li>$x$: Input variables/features.</li>
                <li>$y$: Output variable (target variable).</li>
                <li>$(x, y)$: Single training example.</li>
                <li>$(x^{i}, y^{i})$: $i^{th}$ training example.</li>
            </ul>
            <h3 id="training-process">Training Process</h3>
            <ol>
                <li><strong>Input</strong>: Training set.</li>
                <li><strong>Algorithm</strong>: The learning algorithm processes this data.</li>
                <li><strong>Output</strong>: Hypothesis function $h$ which estimates the value of $y$ for a new input $x$.</li>
            </ol>
            <h3 id="example-house-price-prediction">Example: House Price Prediction</h3>
            <p>Using linear regression, we can predict house prices based on house size.</p>
            <p><img alt="house price table" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/house_price_table.png" /></p>
            <h3 id="analyzing-the-cost-function">Analyzing the Cost Function</h3>
            <p>Different values of $\theta_1$ yield different cost (J) values:</p>
            <ul>
                <li>For $\theta_1 = 1$, $J(\theta_1) = 0$ (Ideal scenario).</li>
                <li>For $\theta_1 = 0.5$, $J(\theta_1) = 0.58$ (Higher error).</li>
                <li>For $\theta_1 = 0$, $J(\theta_1) = 2.3$ (Maximum error).</li>
            </ul>
            <p>Optimization aims to find the value of $\theta_1$ that minimizes $J(\theta_1)$.</p>
            <p><img alt="cost_function" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/cost_function.png" /></p>
            <h3 id="a-deeper-insight-into-the-cost-function-simplified-cost-function">A Deeper Insight into the Cost Function - Simplified Cost Function</h3>
            <p>In linear regression, the cost function $J(\theta_0, \theta_1)$ is a critical component, involving two parameters: $\theta_0$ and $\theta_1$. Visualization of this function can be achieved through different plots.</p>
            <h4 id="3d-surface-plot">3D Surface Plot</h4>
            <p>The 3D surface plot illustrates the cost function where:</p>
            <ul>
                <li>$X$-axis represents $\theta_1$.</li>
                <li>$Z$-axis represents $\theta_0$.</li>
                <li>$Y$-axis represents $J(\theta_0, \theta_1)$.</li>
            </ul>
            <p><img alt="surface_cost_function" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/surface_cost_function.png" /></p>
            <h4 id="contour-plots">Contour Plots</h4>
            <p>Contour plots offer a 2D perspective:</p>
            <ul>
                <li>Each color or level represents the same value of $J(\theta_0, \theta_1)$.</li>
                <li>The center of concentric circles indicates the minimum of the cost function.</li>
            </ul>
            <p><img alt="contour_cost_function" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/contour_cost_function.png" /></p>
            <h3 id="gradient-descent-algorithm">Gradient Descent Algorithm</h3>
            <p>The gradient descent algorithm iteratively adjusts $\theta_0$ and $\theta_1$ to minimize the cost function. The algorithm proceeds as follows:</p>
            <p>
            <div>
                <pre><code class="language-plaintext">Î¸ = [0, 0]
while not converged:
    for j in [0, 1]:
        Î¸_j := Î¸_j - Î± âˆ‚/âˆ‚Î¸_j J(Î¸_0, Î¸_1)</code></pre>
            </div>
            </p>
            <ul>
                <li>Begin with initial guesses (e.g., [0,0]).</li>
                <li>Continuously adjust $\theta_0$ and $\theta_1$ to reduce $J(\theta_0, \theta_1)$.</li>
                <li>Proceed until a local minimum is reached.</li>
            </ul>
            <p><img alt="gradient_descent" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/gradient_descent.png" /></p>
            <h4 id="key-elements-of-gradient-descent">Key Elements of Gradient Descent</h4>
            <ul>
                <li><strong>Learning Rate ($\alpha$)</strong>: Determines the step size during each iteration.</li>
                <li><strong>Partial Derivative</strong>: Indicates the direction to move in the parameter space.</li>
                <li>A negative derivative suggests a decrease in $\theta_1$ to move towards a minimum.</li>
                <li>Conversely, a positive derivative suggests an increase in $\theta_1$.</li>
            </ul>
            <h4 id="partial-derivative-vs-derivative">Partial Derivative vs. Derivative</h4>
            <ul>
                <li><strong>Partial Derivative</strong>: Applied when focusing on a single variable among several.</li>
                <li><strong>Derivative</strong>: Utilized when considering all variables.</li>
            </ul>
            <h4 id="at-a-local-minimum">At a Local Minimum</h4>
            <p>At this point, the derivative equals zero, implying no further changes in $\theta_1$:</p>
            <p>$$
                \alpha \times 0 = 0 \Rightarrow \theta_1 = \theta_1 - 0
                $$</p>
            <p><img alt="local_minimum" src="https://user-images.githubusercontent.com/37275728/201476896-555ad8c4-8422-428b-937f-12cdf70d75bd.png" /></p>
            <p>Through gradient descent, the optimal $\theta_0$ and $\theta_1$ values are identified, minimizing the cost function and enhancing the linear regression model's performance.</p>
            <h4 id="linear-regression-with-gradient-descent">Linear Regression with Gradient Descent</h4>
            <p>In linear regression, gradient descent is applied to minimize the squared error cost function $J(\theta_0, \theta_1)$. The process involves calculating the partial derivatives of the cost function with respect to each parameter $\theta_0$ and $\theta_1$.</p>
            <h3 id="partial-derivatives-of-the-cost-function">Partial Derivatives of the Cost Function</h3>
            <p>The gradient of the cost function is computed as follows:</p>
            <ul>
                <li>For the squared error cost function:</li>
            </ul>
            <p>$$\frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1) = \frac{\partial}{\partial \theta_j} \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^2$$</p>
            <p>$$= \frac{\partial}{\partial \theta_j} \frac{1}{2m} \sum_{i=1}^{m} (\theta_0 + \theta_1x^{(i)} - y^{(i)})^2$$</p>
            <p>The partial derivatives for each $\theta_j$ are:</p>
            <ul>
                <li>For $j=0$:</li>
            </ul>
            <p>$$\frac{\partial}{\partial \theta_0} J(\theta_0, \theta_1)=\frac{\partial}{\partial \theta_0} \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})$$</p>
            <ul>
                <li>For $j=1$:</li>
            </ul>
            <p>$$\frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1)=\frac{\partial}{\partial \theta_1} \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}$$</p>
            <h3 id="normal-equations-method">Normal Equations Method</h3>
            <p>For solving the minimization problem $\min J(\theta_0, \theta_1)$, the normal equations method offers an alternative to gradient descent. This numerical method provides an exact solution, avoiding the iterative approach of gradient descent. It can be faster for certain problems but is more complex and will be covered in detail later.</p>
            <h3 id="extension-of-the-current-model">Extension of the Current Model</h3>
            <p>The linear regression model can be extended to include multiple features. For example, in a housing model, features could include size, age, number of bedrooms, and number of floors. However, a challenge arises when dealing with more than three dimensions, as visualization becomes difficult. To effectively manage multiple features, linear algebra concepts like matrices and vectors are employed, facilitating calculations and interpretations in higher-dimensional spaces.</p>
            <h2 id="reference">Reference</h2>
            <p>These notes are based on the free video lectures offered by Stanford University, led by Professor Andrew Ng. These lectures are part of the renowned Machine Learning course available on Coursera. For more information and to access the full course, visit the <a href="https://www.coursera.org/learn/machine-learning">Coursera course page</a>.</p>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#linear-regression-in-depth">Linear Regression in Depth</a>
                    <ol>
                        <li><a href="#mathematical-model">Mathematical Model</a></li>
                        <li><a href="#cost-function-mean-squared-error-">Cost Function (Mean Squared Error)</a></li>
                        <li><a href="#optimization-gradient-descent">Optimization: Gradient Descent</a></li>
                        <li><a href="#notation">Notation</a></li>
                        <li><a href="#training-process">Training Process</a></li>
                        <li><a href="#example-house-price-prediction">Example: House Price Prediction</a></li>
                        <li><a href="#analyzing-the-cost-function">Analyzing the Cost Function</a></li>
                        <li><a href="#a-deeper-insight-into-the-cost-function-simplified-cost-function">A Deeper Insight into the Cost Function - Simplified Cost Function</a>
                            <ol>
                                <li><a href="#3d-surface-plot">3D Surface Plot</a></li>
                                <li><a href="#contour-plots">Contour Plots</a></li>
                            </ol>
                        </li>
                        <li><a href="#gradient-descent-algorithm">Gradient Descent Algorithm</a>
                            <ol>
                                <li><a href="#key-elements-of-gradient-descent">Key Elements of Gradient Descent</a></li>
                                <li><a href="#partial-derivative-vs-derivative">Partial Derivative vs. Derivative</a></li>
                                <li><a href="#at-a-local-minimum">At a Local Minimum</a></li>
                                <li><a href="#linear-regression-with-gradient-descent">Linear Regression with Gradient Descent</a></li>
                            </ol>
                        </li>
                        <li><a href="#partial-derivatives-of-the-cost-function">Partial Derivatives of the Cost Function</a></li>
                        <li><a href="#normal-equations-method">Normal Equations Method</a></li>
                        <li><a href="#extension-of-the-current-model">Extension of the Current Model</a></li>
                    </ol>
                </li>
                <li><a href="#reference">Reference</a></li>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/01_introduction_to_machine_learning.html">Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/02_linear_regression.html">Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/03_review_of_linear_algebra.html">Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/04_linear_regression_multiple_variables.html">Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/06_logistic_regression.html">Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/07_regularization.html">Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/08_neural_networks_representation.html">Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/09_neural_networks_learning.html">Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/10_applying_machine_learning_advice.html">Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/11_machine_learning_system_design.html">Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/12_support_vector_machines.html">Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/13_clustering.html">Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/14_dimensionality_reduction.html">Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/15_anomaly_detection.html">Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/16_recommendation_systems.html">Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/17_large_scale_machine_learning.html">Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/18_photo_ocr.html">Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>