<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Linear Regression in Depth</title>
    <meta content="Linear Regression is a fundamental type of supervised learning algorithm in statistics and machine learning." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <div class="article-action-buttons"><button class="btn-suggest-edit" title="Suggest Edit">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M4 21h4l11-11-4-4L4 17v4z" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button><button class="btn-create-issue" title="Create Issue">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"></circle>
                        <line stroke="currentColor" stroke-width="2" x1="12" x2="12" y1="8" y2="12"></line>
                        <circle cx="12" cy="16" fill="currentColor" r="1"></circle>
                    </svg>
                </button><button class="btn-download" title="Download">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M12 5v14m0 0l-6-6m6 6l6-6" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button></div>
            <p style="text-align: right;"><i>Last modified: April 10, 2024</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="linear-regression-in-depth">Linear Regression in Depth</h2>
            <p>Linear Regression is a fundamental type of supervised learning algorithm in statistics and machine learning. It's utilized for modeling and analyzing the relationship between a dependent variable and one or more independent variables. The goal is to predict continuous output values based on the input variables.</p>
            <ul>
                <li><strong>Purpose</strong>: To predict a continuous outcome based on one or more predictor variables.</li>
                <li><strong>Model</strong>: The output variable (y) is assumed to be a linear combination of the input variables (x), with coefficients (Î¸) representing weights.</li>
            </ul>
            <h3 id="mathematical-model">Mathematical Model</h3>
            <p>The hypothesis function in linear regression is given by:</p>
            <p>$$
                h_{\theta}(x) = \theta_0 + \theta_1x
                $$</p>
            <p>where:</p>
            <ul>
                <li>$h_{\theta}(x)$ is the predicted value,</li>
                <li>$\theta_0$ is the y-intercept (bias term),</li>
                <li>$\theta_1$ is the slope (weight for the feature x).</li>
            </ul>
            <h3 id="cost-function-mean-squared-error-">Cost Function (Mean Squared Error)</h3>
            <p>The cost function in linear regression measures how far off the predictions are from the actual outcomes. It's commonly represented as:</p>
            <p>$$
                J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2
                $$</p>
            <p>This is the mean squared error (MSE) cost function, where:</p>
            <ul>
                <li>$m$ is the number of training examples,</li>
                <li>$x^{(i)}$ and $y^{(i)}$ are the input and output of the $i^{th}$ training example.</li>
            </ul>
            <h3 id="optimization-gradient-descent">Optimization: Gradient Descent</h3>
            <p>To find the optimal parameters ($\theta_0$ and $\theta_1$), we use gradient descent to minimize the cost function. The gradient descent algorithm iteratively adjusts the parameters to reduce the cost.</p>
            <h3 id="notation">Notation</h3>
            <ul>
                <li>$m$: Number of training examples.</li>
                <li>$x$: Input variables/features.</li>
                <li>$y$: Output variable (target variable).</li>
                <li>$(x, y)$: Single training example.</li>
                <li>$(x^{i}, y^{i})$: $i^{th}$ training example.</li>
            </ul>
            <h3 id="training-process">Training Process</h3>
            <ol>
                <li><strong>Input</strong>: Training set.</li>
                <li><strong>Algorithm</strong>: The learning algorithm processes this data.</li>
                <li><strong>Output</strong>: Hypothesis function $h$ which estimates the value of $y$ for a new input $x$.</li>
            </ol>
            <h3 id="example-house-price-prediction">Example: House Price Prediction</h3>
            <p>Using linear regression, we can predict house prices based on house size.</p>
            <p><img alt="house price table" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/house_price_table.png" /></p>
            <h3 id="analyzing-the-cost-function">Analyzing the Cost Function</h3>
            <p>Different values of $\theta_1$ yield different cost (J) values:</p>
            <ul>
                <li>For $\theta_1 = 1$, $J(\theta_1) = 0$ (Ideal scenario).</li>
                <li>For $\theta_1 = 0.5$, $J(\theta_1) = 0.58$ (Higher error).</li>
                <li>For $\theta_1 = 0$, $J(\theta_1) = 2.3$ (Maximum error).</li>
            </ul>
            <p>Optimization aims to find the value of $\theta_1$ that minimizes $J(\theta_1)$.</p>
            <p><img alt="cost_function" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/cost_function.png" /></p>
            <h3 id="a-deeper-insight-into-the-cost-function-simplified-cost-function">A Deeper Insight into the Cost Function - Simplified Cost Function</h3>
            <p>In linear regression, the cost function $J(\theta_0, \theta_1)$ is a critical component, involving two parameters: $\theta_0$ and $\theta_1$. Visualization of this function can be achieved through different plots.</p>
            <h4 id="3d-surface-plot">3D Surface Plot</h4>
            <p>The 3D surface plot illustrates the cost function where:</p>
            <ul>
                <li>$X$-axis represents $\theta_1$.</li>
                <li>$Z$-axis represents $\theta_0$.</li>
                <li>$Y$-axis represents $J(\theta_0, \theta_1)$.</li>
            </ul>
            <p><img alt="surface_cost_function" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/surface_cost_function.png" /></p>
            <h4 id="contour-plots">Contour Plots</h4>
            <p>Contour plots offer a 2D perspective:</p>
            <ul>
                <li>Each color or level represents the same value of $J(\theta_0, \theta_1)$.</li>
                <li>The center of concentric circles indicates the minimum of the cost function.</li>
            </ul>
            <p><img alt="contour_cost_function" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/contour_cost_function.png" /></p>
            <h3 id="gradient-descent-algorithm">Gradient Descent Algorithm</h3>
            <p>The gradient descent algorithm iteratively adjusts $\theta_0$ and $\theta_1$ to minimize the cost function. The algorithm proceeds as follows:</p>
            <p>
            <div>
                <pre><code class="language-plaintext">Î¸ = [0, 0]
while not converged:
    for j in [0, 1]:
        Î¸_j := Î¸_j - Î± âˆ‚/âˆ‚Î¸_j J(Î¸_0, Î¸_1)</code></pre>
            </div>
            </p>
            <ul>
                <li>Begin with initial guesses (e.g., [0,0]).</li>
                <li>Continuously adjust $\theta_0$ and $\theta_1$ to reduce $J(\theta_0, \theta_1)$.</li>
                <li>Proceed until a local minimum is reached.</li>
            </ul>
            <p><img alt="gradient_descent" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/gradient_descent.png" /></p>
            <p>Here is a complete Python code that generates mock data and implements the gradient descent algorithm to fit a linear regression model. The code also includes plotting the cost function and the path taken by the gradient descent algorithm.</p>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# Generate mock data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Feature normalization
X_mean = np.mean(X)
X_std = np.std(X)
X_norm = (X - X_mean) / X_std

# Add bias term (X0 = 1)
X_b = np.c_[np.ones((100, 1)), X_norm]

# Initialize parameters
theta = np.random.randn(2, 1) * 0.01
alpha = 0.1
n_iterations = 1000
m = len(X_b)
tolerance = 1e-7

# Cost function
def compute_cost(theta, X_b, y):
    return (1 / (2 * m)) * np.sum((X_b.dot(theta) - y) ** 2)

# Gradient Descent
cost_history = []
theta_history = [theta]

for iteration in range(n_iterations):
    gradients = (1 / m) * X_b.T.dot(X_b.dot(theta) - y)
    theta = theta - alpha * gradients
    cost = compute_cost(theta, X_b, y)
    cost_history.append(cost)
    theta_history.append(theta.copy())
    if len(cost_history) &gt; 1 and np.abs(cost_history[-1] - cost_history[-2]) &lt; tolerance:
        break

# Prepare data for contour plot
theta_0_vals = np.linspace(-10, 10, 100)
theta_1_vals = np.linspace(-4, 4, 100)
cost_vals = np.zeros((len(theta_0_vals), len(theta_1_vals)))

for i in range(len(theta_0_vals)):
    for j in range(len(theta_1_vals)):
        theta_ij = np.array([[theta_0_vals[i]], [theta_1_vals[j]]])
        cost_vals[i, j] = compute_cost(theta_ij, X_b, y)

# Plotting
fig, ax = plt.subplots(figsize=(10, 6))

# Contour plot
theta_0_vals, theta_1_vals = np.meshgrid(theta_0_vals, theta_1_vals)
CS = ax.contour(theta_0_vals, theta_1_vals, cost_vals, levels=np.logspace(-2, 3, 20), cmap='viridis')
ax.clabel(CS, inline=1, fontsize=10)

# Plot the path of the gradient descent
theta_history = np.array(theta_history)
ax.plot(theta_history[:, 0], theta_history[:, 1], 'r-o', label='Gradient Descent Path')

# Annotate the start and end points
ax.annotate('Start', xy=(theta_history[0, 0], theta_history[0, 1]), xytext=(-9, 3),
            arrowprops=dict(facecolor='black', shrink=0.05), fontsize=12, color='black')
ax.annotate('End', xy=(theta_history[-1, 0], theta_history[-1, 1]), xytext=(theta_history[-1, 0] + 1, theta_history[-1, 1]),
            arrowprops=dict(facecolor='black', shrink=0.05), fontsize=12, color='black')

ax.set_xlabel(r'$\theta_0$', fontsize=14)
ax.set_ylabel(r'$\theta_1$', fontsize=14)
ax.set_title('Gradient Descent Path')
ax.legend()

plt.show()

# Plot cost function
plt.figure(figsize=(8, 4))
plt.plot(cost_history, 'b-', label='Cost Function')
plt.xlabel('Iterations')
plt.ylabel('Cost')
plt.title('Cost Function Over Iterations')
plt.legend()
plt.show()</code></pre>
            </div>
            </p>
            <ul>
                <li>The code begins by importing the necessary libraries: numpy for numerical operations and matplotlib.pyplot for plotting.</li>
                <li>Random data generation is performed using a fixed seed for reproducibility, producing 100 data points for the feature variable X and the target variable y, which follows a linear relationship with added Gaussian noise.</li>
                <li>To normalize the features, the code calculates the mean and standard deviation of X, then scales the feature by subtracting the mean and dividing by the standard deviation.</li>
                <li>A bias term (a column of ones) is added to the feature matrix X to account for the intercept in the linear regression model.</li>
                <li>The initial parameters for the linear regression model, referred to as theta, are initialized with small random values. The learning rate (alpha) is set to 0.1, and the number of iterations for gradient descent is set to 1000. The number of training examples (m) is determined from the dataset.</li>
                <li>The cost function, which measures the mean squared error between the predicted and actual values, is defined to evaluate the performance of the model.</li>
                <li>Gradient descent is implemented to optimize the parameters theta. In each iteration, the gradients of the cost function with respect to the parameters are computed, and the parameters are updated in the direction that minimizes the cost.</li>
                <li>A history of the cost function values and parameter updates is maintained during the gradient descent iterations. The algorithm terminates early if the change in the cost function value between consecutive iterations falls below a specified tolerance level.</li>
                <li>For visualization purposes, a range of theta values is prepared, and the cost function is evaluated across this range to create a contour plot, showing the cost landscape.</li>
                <li>The code generates a contour plot that illustrates the cost function values for different theta values. It also plots the path taken by gradient descent, highlighting the start and end points of the optimization process.</li>
                <li>Additionally, the code plots the cost function values over the iterations to show how the cost decreases as gradient descent progresses.</li>
            </ul>
            <p>Below is the gradient descent path:</p>
            <p><img alt="Gradient Descent Path" src="https://github.com/djeada/Stanford-Machine-Learning/assets/37275728/78cb2177-a6de-426c-b999-c7c5c98a6b04" /></p>
            <p>And here is the cost function over iterations:</p>
            <p><img alt="Cost Function" src="https://github.com/djeada/Stanford-Machine-Learning/assets/37275728/429ada6d-4dbd-4eb7-ae69-9cde85aafeca" /></p>
            <h4 id="key-elements-of-gradient-descent">Key Elements of Gradient Descent</h4>
            <ul>
                <li><strong>Learning Rate ($\alpha$)</strong>: Determines the step size during each iteration.</li>
                <li><strong>Partial Derivative</strong>: Indicates the direction to move in the parameter space.</li>
                <li>A negative derivative suggests a decrease in $\theta_1$ to move towards a minimum.</li>
                <li>Conversely, a positive derivative suggests an increase in $\theta_1$.</li>
            </ul>
            <h4 id="partial-derivative-vs-derivative">Partial Derivative vs. Derivative</h4>
            <ul>
                <li><strong>Partial Derivative</strong>: Applied when focusing on a single variable among several.</li>
                <li><strong>Derivative</strong>: Utilized when considering all variables.</li>
            </ul>
            <h4 id="at-a-local-minimum">At a Local Minimum</h4>
            <p>At this point, the derivative equals zero, implying no further changes in $\theta_1$:</p>
            <p>$$
                \alpha \times 0 = 0 \Rightarrow \theta_1 = \theta_1 - 0
                $$</p>
            <p><img alt="local_minimum" src="https://user-images.githubusercontent.com/37275728/201476896-555ad8c4-8422-428b-937f-12cdf70d75bd.png" /></p>
            <p>Through gradient descent, the optimal $\theta_0$ and $\theta_1$ values are identified, minimizing the cost function and enhancing the linear regression model's performance.</p>
            <h4 id="linear-regression-with-gradient-descent">Linear Regression with Gradient Descent</h4>
            <p>In linear regression, gradient descent is applied to minimize the squared error cost function $J(\theta_0, \theta_1)$. The process involves calculating the partial derivatives of the cost function with respect to each parameter $\theta_0$ and $\theta_1$.</p>
            <h3 id="partial-derivatives-of-the-cost-function">Partial Derivatives of the Cost Function</h3>
            <p>The gradient of the cost function is computed as follows:</p>
            <ul>
                <li>For the squared error cost function:</li>
            </ul>
            <p>$$\frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1) = \frac{\partial}{\partial \theta_j} \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^2$$</p>
            <p>$$= \frac{\partial}{\partial \theta_j} \frac{1}{2m} \sum_{i=1}^{m} (\theta_0 + \theta_1x^{(i)} - y^{(i)})^2$$</p>
            <p>The partial derivatives for each $\theta_j$ are:</p>
            <ul>
                <li>For $j=0$:</li>
            </ul>
            <p>$$\frac{\partial}{\partial \theta_0} J(\theta_0, \theta_1)=\frac{\partial}{\partial \theta_0} \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})$$</p>
            <ul>
                <li>For $j=1$:</li>
            </ul>
            <p>$$\frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1)=\frac{\partial}{\partial \theta_1} \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}$$</p>
            <h3 id="normal-equations-method">Normal Equations Method</h3>
            <p>For solving the minimization problem $\min J(\theta_0, \theta_1)$, the normal equations method offers an alternative to gradient descent. This numerical method provides an exact solution, avoiding the iterative approach of gradient descent. It can be faster for certain problems but is more complex and will be covered in detail later.</p>
            <p>Below is the Python code to generate mock data and solve the minimization problem (\min J(\theta_0, \theta_1)) using the normal equations method.</p>
            <p>
            <div>
                <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# Generate mock data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Add bias term (X0 = 1)
X_b = np.c_[np.ones((100, 1)), X]

# Normal Equations method
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)

# Display the results
print(f"Theta obtained by the Normal Equations method: {theta_best.ravel()}")

# Plotting the results
plt.figure(figsize=(10, 6))
plt.plot(X, y, 'b.', label='Data Points')
plt.plot(X, X_b.dot(theta_best), 'r-', label='Prediction', linewidth=2)
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear Regression using Normal Equations')
plt.legend()
plt.show()</code></pre>
            </div>
            </p>
            <ul>
                <li>The data generation begins with setting a random seed using <code>np.random.seed(42)</code> to ensure reproducibility of the results.</li>
                <li>The feature variable $X$ is generated as 100 random values between 0 and 2, using the $np.random.rand$ function and scaling by 2.</li>
                <li>The target variable $y$ is generated using a linear relationship with the feature variable $X$, specifically $y = 4 + 3X$, and includes added Gaussian noise using $np.random.randn$ to simulate real-world data variability.</li>
                <li>A bias term, represented as a column of ones, is added to the feature matrix $X$ to account for the intercept term in the linear regression model. This is achieved by concatenating the bias term with $X$ to form $X_b$.</li>
                <li>The normal equations method is used to solve for the optimal parameters $\theta$. This method calculates the parameters directly by solving the equation $(X_b^T X_b)^{-1} X_b^T y$.</li>
                <li>This approach avoids the iterative process used in gradient descent and provides an exact solution for the linear regression parameters.</li>
                <li>The computed parameters $\theta_{\text{best}}$ are then printed to the console, showing the values obtained by the normal equations method.</li>
                <li>To visualize the results, a plot is generated with matplotlib. The data points are plotted as blue dots, and the fitted linear regression line is plotted as a red line.</li>
            </ul>
            <p><img alt="Normal Equation Fit" src="https://github.com/djeada/Stanford-Machine-Learning/assets/37275728/ad2629de-288d-41cf-8215-beabe9d08c82" /></p>
            <h3 id="extension-of-the-current-model">Extension of the Current Model</h3>
            <p>The linear regression model can be extended to include multiple features. For example, in a housing model, features could include size, age, number of bedrooms, and number of floors. However, a challenge arises when dealing with more than three dimensions, as visualization becomes difficult. To effectively manage multiple features, linear algebra concepts like matrices and vectors are employed, facilitating calculations and interpretations in higher-dimensional spaces.</p>
            <h2 id="reference">Reference</h2>
            <p>These notes are based on the free video lectures offered by Stanford University, led by Professor Andrew Ng. These lectures are part of the renowned Machine Learning course available on Coursera. For more information and to access the full course, visit the <a href="https://www.coursera.org/learn/machine-learning">Coursera course page</a>.</p>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#linear-regression-in-depth">Linear Regression in Depth</a>
                    <ol>
                        <li><a href="#mathematical-model">Mathematical Model</a></li>
                        <li><a href="#cost-function-mean-squared-error-">Cost Function (Mean Squared Error)</a></li>
                        <li><a href="#optimization-gradient-descent">Optimization: Gradient Descent</a></li>
                        <li><a href="#notation">Notation</a></li>
                        <li><a href="#training-process">Training Process</a></li>
                        <li><a href="#example-house-price-prediction">Example: House Price Prediction</a></li>
                        <li><a href="#analyzing-the-cost-function">Analyzing the Cost Function</a></li>
                        <li><a href="#a-deeper-insight-into-the-cost-function-simplified-cost-function">A Deeper Insight into the Cost Function - Simplified Cost Function</a>
                            <ol>
                                <li><a href="#3d-surface-plot">3D Surface Plot</a></li>
                                <li><a href="#contour-plots">Contour Plots</a></li>
                            </ol>
                        </li>
                        <li><a href="#gradient-descent-algorithm">Gradient Descent Algorithm</a>
                            <ol>
                                <li><a href="#key-elements-of-gradient-descent">Key Elements of Gradient Descent</a></li>
                                <li><a href="#partial-derivative-vs-derivative">Partial Derivative vs. Derivative</a></li>
                                <li><a href="#at-a-local-minimum">At a Local Minimum</a></li>
                                <li><a href="#linear-regression-with-gradient-descent">Linear Regression with Gradient Descent</a></li>
                            </ol>
                        </li>
                        <li><a href="#partial-derivatives-of-the-cost-function">Partial Derivatives of the Cost Function</a></li>
                        <li><a href="#normal-equations-method">Normal Equations Method</a></li>
                        <li><a href="#extension-of-the-current-model">Extension of the Current Model</a></li>
                    </ol>
                </li>
                <li><a href="#reference">Reference</a></li>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/01_introduction_to_machine_learning.html">Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/02_linear_regression.html">Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/03_review_of_linear_algebra.html">Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/04_linear_regression_multiple_variables.html">Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/06_logistic_regression.html">Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/07_regularization.html">Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/08_neural_networks_representation.html">Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/09_neural_networks_learning.html">Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/10_applying_machine_learning_advice.html">Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/11_machine_learning_system_design.html">Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/12_support_vector_machines.html">Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/13_clustering.html">Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/14_dimensionality_reduction.html">Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/15_anomaly_detection.html">Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/16_recommendation_systems.html">Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/17_large_scale_machine_learning.html">Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/18_photo_ocr.html">Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All content here is free to use,
                    but please remember to be respectful and avoid any misuse of the site.
                    If youâ€™d like to get in touch, feel free to reach out via my
                    <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a>
                    or connect with me on
                    <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a>
                    if you have technical questions or ideas to share.
                    Wishing you all the best and a fantastic life ahead!
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
    <div id="pdf-spinner-overlay">
        <div class="spinner"></div>
    </div>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>