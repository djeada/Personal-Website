<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Dimensionality Reduction with Principal Component Analysis (PCA)</title>
    <meta content="Principal Component Analysis (PCA) is a widely used technique in machine learning for dimensionality reduction." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>Last modified: June 15, 2024</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="dimensionality-reduction-with-principal-component-analysis-pca-">Dimensionality Reduction with Principal Component Analysis (PCA)</h2>
            <p>Principal Component Analysis (PCA) is a widely used technique in machine learning for dimensionality reduction. It simplifies the complexity in high-dimensional data while retaining trends and patterns.</p>
            <h3 id="understanding-pca">Understanding PCA</h3>
            <ul>
                <li><strong>Objective</strong>: PCA seeks to find a lower-dimensional surface onto which data points can be projected with minimal loss of information.</li>
                <li><strong>Methodology</strong>: It involves computing the covariance matrix of the data, finding its eigenvectors (principal components), and projecting the data onto a space spanned by these eigenvectors.</li>
                <li><strong>Applications</strong>: PCA is used for tasks such as data compression, noise reduction, visualization, feature selection, and enhancing the performance of machine learning algorithms.</li>
            </ul>
            <h3 id="compression-with-pca">Compression with PCA</h3>
            <ul>
                <li>Speeds up learning algorithms.</li>
                <li>Saves storage space.</li>
                <li>Focuses on the most relevant features, discarding less important ones.</li>
                <li><strong>Example</strong>: Different units of the same attribute can be reduced to a single, more representative dimension.</li>
            </ul>
            <p><img alt="Example of Dimension Reduction" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/compression_units.png" /></p>
            <h3 id="visualization-through-pca">Visualization through PCA</h3>
            <ul>
                <li><strong>Challenge</strong>: High-dimensional data is difficult to visualize.</li>
                <li><strong>Solution</strong>: PCA can reduce dimensions to make data visualization more feasible and interpretable.</li>
                <li><strong>Example</strong>: Representing 50 features of a dataset as a 2D plot, simplifying analysis and interpretation.</li>
            </ul>
            <p><img alt="Example of Data Table" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/table.png" /></p>
            <h3 id="pca-problem-formulationfrom-itertools-import-accumulate">PCA Problem Formulationfrom itertools import accumulate</h3>
            <p>def move(position, speed, time):
                return position + speed * time</p>
            <p>def get_positions(position, speed, time_list):
                positions = accumulate(time_list, lambda pos, time: move(pos, speed, time), initial=position)
                return list(positions)[1:] # Pomijamy pierwszÄ… pozycjÄ™, ktÃ³ra jest poczÄ…tkowÄ… pozycjÄ…</p>
            <p>def get_path(position, speed, time_list):
                return get_positions(position, speed, time_list)</p>
            <p>print(get_path(0, 10, [1, 0.5])) # [10, 15]</p>
            <ol>
                <li>
                    <p><strong>Goal</strong>: The primary goal of Principal Component Analysis (PCA) is to identify a lower-dimensional representation of a dataset that retains as much variability (information) as possible. This is achieved by minimizing the projection error, which is the distance between the original data points and their projections onto the lower-dimensional subspace.</p>
                </li>
                <li>
                    <p><strong>Projection Error</strong>: In PCA, the projection error is defined as the sum of the squared distances between each data point and its projection onto the lower-dimensional subspace. PCA seeks to minimize this error, thereby ensuring that the chosen subspace captures the maximum variance in the data.</p>
                </li>
                <li>
                    <p><strong>Mathematical Formulation</strong>:</p>
                </li>
                <li>
                    <p>Let $X$ be an $m \times n$ matrix representing the dataset, where $m$ is the number of samples and $n$ is the number of features.</p>
                </li>
                <li>The goal is to find a set of orthogonal vectors (principal components) that form a basis for the lower-dimensional subspace.</li>
                <li>These principal components are the eigenvectors of the covariance matrix $\Sigma = \frac{1}{m} X^T X$, corresponding to the largest eigenvalues.</li>
                <li>
                    <p>If $k$ is the desired lower dimension ($k &lt; n$), PCA seeks the top $k$ eigenvectors of $\Sigma$.</p>
                </li>
                <li>
                    <p><strong>Variance Maximization</strong>: An equivalent formulation of PCA is to maximize the variance of the projections of the data points onto the principal components. By maximizing the variance, PCA ensures that the selected components capture the most significant patterns in the data.</p>
                </li>
            </ol>
            <p><img alt="Visualizing PCA Projection" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/pca.png" /></p>
            <h3 id="pca-vs-linear-regression-"><strong>PCA vs. Linear Regression</strong>:</h3>
            <ul>
                <li>Linear Regression: Minimizes the vertical distances between data points and the fitted line (predictive model).</li>
                <li>PCA: Minimizes the orthogonal distances to the line (data representation model), without distinguishing between dependent and independent variables.</li>
            </ul>
            <h3 id="selecting-principal-components">Selecting Principal Components</h3>
            <ul>
                <li><strong>Number of Components</strong>: The choice of how many principal components to retain depends on the trade-off between minimizing projection error and reducing dimensionality.</li>
                <li><strong>Variance Retained</strong>: Ideally, the selected components should retain most of the variance of the original data.</li>
            </ul>
            <h3 id="principal-component-analysis-pca-algorithm">Principal Component Analysis (PCA) Algorithm</h3>
            <p>Principal Component Analysis (PCA) is a systematic process for reducing the dimensionality of data. Here's a breakdown of the PCA algorithm:</p>
            <ol>
                <li><strong>Covariance Matrix Computation</strong>: Calculate the covariance matrix $\Sigma$:</li>
            </ol>
            <p>$$\Sigma = \frac{1}{m} \sum_{i=1}^{n} (x^{(i)})(x^{(i)})^T$$</p>
            <p>Here, $\Sigma$ is an $[n \times n]$ matrix, with each $x^{(i)}$ being an $[n \times 1]$ matrix.</p>
            <ol>
                <li><strong>Eigenvector Calculation</strong>: Compute the eigenvectors of the covariance matrix $\Sigma$:</li>
            </ol>
            <p>$$
                [U,S,V] = \text{svd}(\Sigma)
                $$</p>
            <p>The matrix $U$ will also be an $[n \times n]$ matrix, with its columns being the eigenvectors we seek.</p>
            <ol>
                <li>
                    <p><strong>Choosing Principal Components</strong>: Select the first $k$ eigenvectors from $U$, this is $U_{\text{reduce}}$.</p>
                </li>
                <li>
                    <p><strong>Calculating Compressed Representation</strong>: For each data point $x$, compute its new representation $z$:</p>
                </li>
            </ol>
            <p>$$z = (U_{\text{reduce}})^T \cdot x$$</p>
            <h3 id="reconstruction-from-compressed-representation">Reconstruction from Compressed Representation</h3>
            <p>Is it possible to go back from a lower dimension to a higher one? While exact reconstruction is not possible (since some information is lost), an approximation can be obtained:</p>
            <p>$$x_{\text{approx}} = U_{\text{reduce}} \cdot z$$</p>
            <p>This approximates the original data in the higher-dimensional space but aligned along the principal components.</p>
            <h3 id="choosing-the-number-of-principal-components">Choosing the Number of Principal Components</h3>
            <p>The number of principal components ($k$) is a crucial choice in PCA. The objective is to minimize the average squared projection error while retaining most of the variance in the data:</p>
            <ul>
                <li><strong>Average Squared Projection Error</strong>:</li>
            </ul>
            <p>$$\frac{1}{m} \sum_{i=1}^{m} ||x^{(i)} - x_{\text{approx}}^{(i)}||^2$$</p>
            <ul>
                <li><strong>Total Data Variation</strong>:</li>
            </ul>
            <p>$$\frac{1}{m} \sum_{i=1}^{m} ||x^{(i)}||^2$$</p>
            <ul>
                <li><strong>Choosing $k$</strong>:
                    The fraction of variance retained is often set as a threshold (e.g., 99%):</li>
            </ul>
            <p>$$
                \frac{\frac{1}{m} \sum_{i=1}^{m} ||x^{(i)} - x_{\text{approx}}^{(i)}||^2}
                {\frac{1}{m} \sum_{i=1}^{m} ||x^{(i)}||^2}
                \leq 0.01
                $$</p>
            <h3 id="applications-of-pca">Applications of PCA</h3>
            <ol>
                <li><strong>Compression</strong>: Reducing data size for storage or faster processing.</li>
                <li><strong>Visualization</strong>: With $k=2$ or $k=3$, data can be visualized in 2D or 3D space.</li>
                <li><strong>Limitation</strong>: PCA should not be used indiscriminately to prevent overfitting. It removes data dimensions without understanding their importance.</li>
                <li><strong>Usage Advice</strong>: It's recommended to try understanding the data without PCA first and apply PCA if it is believed to aid in achieving specific objectives.</li>
            </ol>
            <h2 id="reference">Reference</h2>
            <p>These notes are derived from the free video lectures offered by Stanford University, led by Professor Andrew Ng. These lectures are part of the renowned Machine Learning course available on Coursera. For more information and to access the full course, visit the <a href="https://www.coursera.org/learn/machine-learning">Coursera course page</a>.</p>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#dimensionality-reduction-with-principal-component-analysis-pca-">Dimensionality Reduction with Principal Component Analysis (PCA)</a>
                    <ol>
                        <li><a href="#understanding-pca">Understanding PCA</a></li>
                        <li><a href="#compression-with-pca">Compression with PCA</a></li>
                        <li><a href="#visualization-through-pca">Visualization through PCA</a></li>
                        <li><a href="#pca-problem-formulationfrom-itertools-import-accumulate">PCA Problem Formulationfrom itertools import accumulate</a></li>
                        <li><a href="#pca-vs-linear-regression-">PCA vs. Linear Regression:</a></li>
                        <li><a href="#selecting-principal-components">Selecting Principal Components</a></li>
                        <li><a href="#principal-component-analysis-pca-algorithm">Principal Component Analysis (PCA) Algorithm</a></li>
                        <li><a href="#reconstruction-from-compressed-representation">Reconstruction from Compressed Representation</a></li>
                        <li><a href="#choosing-the-number-of-principal-components">Choosing the Number of Principal Components</a></li>
                        <li><a href="#applications-of-pca">Applications of PCA</a></li>
                    </ol>
                </li>
                <li><a href="#reference">Reference</a></li>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/01_introduction_to_machine_learning.html">Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/02_linear_regression.html">Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/03_review_of_linear_algebra.html">Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/04_linear_regression_multiple_variables.html">Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/06_logistic_regression.html">Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/07_regularization.html">Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/08_neural_networks_representation.html">Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/09_neural_networks_learning.html">Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/10_applying_machine_learning_advice.html">Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/11_machine_learning_system_design.html">Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/12_support_vector_machines.html">Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/13_clustering.html">Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/14_dimensionality_reduction.html">Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/15_anomaly_detection.html">Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/16_recommendation_systems.html">Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/17_large_scale_machine_learning.html">Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/18_photo_ocr.html">Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>