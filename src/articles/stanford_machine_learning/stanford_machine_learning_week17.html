<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <title>Large Scale Machine Learning</title>
    <meta charset="utf-8" />
    <meta content="Training machine learning models on large datasets can be challenging due to the computational resources required." name="description" />
    <meta content="Adam Djellouli" name="keywords" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" type="text/css" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <link href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="ie-edge" http-equiv="X-UA-Compatible" />
</head>

<body>
    <nav>
        <a class="logo" href="../index.html" title="Adam Djellouli - Home">
            <img alt="Adam Djellouli Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul>
            <li> <a href="../../index.html" title="Home"> Home </a> </li>
            <li> <a class="active" href="../../core/blog.html" title="Adam Djellouli's Blog - Programming, technology and more"> Blog </a> </li>
            <li> <a href="../../core/tools.html" title="Useful Tools by Adam Djellouli"> Tools </a> </li>
            <li> <a href="../../core/projects.html" title="Projects by Adam Djellouli"> Projects </a> </li>
            <li> <a href="../../core/resume.html" title="Adam Djellouli's Resume"> Resume </a> </li>
            <li> <a href="../../core/about.html" title="About Adam Djellouli"> About </a> </li>
            <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
        </ul>
    </nav>
    <section id="article-body"></section>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="large-scale-machine-learning">Large Scale Machine Learning</h2>
            <p>Training machine learning models on large datasets can be challenging due to the computational resources required. However, there are techniques that can make this process more efficient and effective. One such technique is using a low bias algorithm and training it on a large amount of data. This can help you achieve high performance. In this discussion, we will explore techniques such as stochastic gradient descent and online learning, which allow us to efficiently train models on large datasets. We will also examine how to determine if our model is suffering from high bias or high variance and how to address these issues. By the end of this discussion, you should have a better understanding of how to effectively train models on large datasets.</p>
            <h2 id="learning-with-large-datasets">Learning with large datasets</h2>
            <p>Taking a low bias algorithm and training it on a large amount of data is one of the greatest approaches to get high performance.</p>
            <ul>
                <li>Assume we have a data collection with m = 100,000, 000.</li>
                <li>On such a large system, how can we train a logistic regression model?</li>
            </ul>
            <p>$$\theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m(h_{\theta}(x^{(i)})- y^{(i)})x_j^{(i)}$$</p>
            <ul>
                <li>As a result, for each gradient descent step, you must add up more than 100,000,000 terms.</li>
                <li>The first step is to inquire whether we may train on 1000 examples rather than 100 000 000.</li>
                <li>Pick a small sample at random. Can you build a system that performs well?</li>
            </ul>
            <p><img alt="learning_curve" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/learning_curve.png" /></p>
            <ul>
                <li>If the gap is large, it is a high variance problem. More examples should lead to better results.</li>
                <li>If the gap is tiny, it is a high bias problem. More examples may not be beneficial.</li>
            </ul>
            <h2 id="stochastic-gradient-descent">Stochastic Gradient Descent</h2>
            <p>Hypothesis:
                $$h_{\theta}(x) = \sum_{n}^{j=0}\theta_jx_j$$</p>
            <p>Cost function:
                $$\quad J_{train}(\theta) = \frac{1}{2m} \sum_{m}^{i=1}(h_{\theta}(x^{(i)}) - y^{(i)})^2$$</p>
            <p>We get bowl shape surface plots if we plot our two parameters against the cost function:</p>
            <p><img alt="surface_cost_function" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/surface_cost_function.png" /></p>
            <p>How does gradient descent work?</p>
            <pre><div><pre><code class="language-shell">\theta = [0] * n
while not converged:
  for j in [0, ..., n]:
      \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, ..., \theta_n)</code></pre>
    </div>
    </pre>
    <p>Although we have just referred to it as gradient descent so far, this type of gradient descent is known as batch gradient descent. This simply implies that we examine all of the examples at the same time. Batch gradient descent is unsuitable for large datasets.</p>
    <p>When discussing stochastic gradient descent, we will use linear regression as an algorithmic example, however the concepts apply to other algorithms as well, such as logistic regression and neural networks.</p>
    <p>Define our cost function in a slightly different way, as follows:</p>
    <pre><div><pre><code class="language-randomly">shuffle training examples
for i in [0, ..., m]:
  for j in [0, ..., n]:
      $\theta_j := \theta_j - \alpha (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$</code></pre>
    </div>
    </pre>
    <p>The random shuffling at the start ensures that the data is in a random sequence, preventing bias in the movement.</p>
    <p>Although stochastic gradient descent is similar to batch gradient descent, instead of waiting for the gradient terms to be summed across all m examples, we pick only one example and make work on enhancing the parameters right away.</p>
    <p>This means that we change the parameters on EVERY step through the data, rather than at the end of each loop over all of the data.</p>
    <p><img alt="stochastic" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/stochastic.png" /></p>
    <p>For our stochastic gradient descent, we might plot the cost function vs the number of iterations.
        We should be able to see if convergence is occurring by looking at the graphs.
        A smoother curve may be obtained by averaging across many (e.g. 1000 and 5000) instances.</p>
    <p><img alt="stochastic_convergence" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/stochastic_convergence.png" /></p>
    <h2 id="online-learning">Online learning</h2>
    <ul>
        <li>Allows us to model problems in which there is a continuous stream of data from which an algorithm should learn.</li>
        <li>In the same way that stochastic gradient descent is used, slow updates are performed.</li>
        <li>To learn from traffic, web companies employ several sorts of online learning algorithms.</li>
    </ul>
    <h3 id="example-product-search">Example - product search</h3>
    <ul>
        <li>Assume you own a cellphone-selling website.</li>
        <li>You have a user interface where the user may enter in a query such as "Android phone 1080p camera."</li>
        <li>We want to provide the user ten phones per query, with the phones ranked from most appealing to the user.</li>
        <li>We generate a feature vector (x) for each phone based on a specific user query.</li>
        <li>We want to determine the likelihood of a user picking a phone.</li>
        <li>$y = 1$ if a user clicks on a link.</li>
        <li>$y = 0$ otherwise.</li>
        <li>We learn $p(y=1|x;\theta)$ - this is the problem of learning the predicted click through rate (CTR).</li>
        <li>If you can estimate the CTR for any phone, we can utilize it to show the phones with the highest likelihood first.</li>
    </ul>
    <h2 id="map-reduce-and-data-parallelism">Map reduce and data parallelism</h2>
    <p>Some problems are simply too large for a single CPU to handle.</p>
    <p><img alt="map_reduce" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/map_reduce.png" /></p>
    <p>Parallelization can come from:</p>
    <ul>
        <li>Multiple machines.</li>
        <li>Multiple CPUs.</li>
        <li>Multiple cores in each CPU.</li>
    </ul>
    <p>Certain numerical linear algebra libraries can automatically parallelize your calculations over several cores, depending on the implementation details.
        So, if this is the case and you have a decent vectorization implementation, you don't have to worry about local parallelization because the local libraries will handle optimization for you.</p>
    <p>Hadoop is an example of Map Reduce implementation.</p>
    </section>
    <div id="table-of-contents">
        <h2>Table of Contents</h2>
        <ol>
            <li><a href="#large-scale-machine-learning">Large Scale Machine Learning</a></li>
            <li><a href="#learning-with-large-datasets">Learning with large datasets</a></li>
            <li><a href="#stochastic-gradient-descent">Stochastic Gradient Descent</a></li>
            <li><a href="#online-learning">Online learning</a>
                <ol>
                    <li><a href="#example-product-search">Example - product search</a></li>
                </ol>
            </li>
            <li><a href="#map-reduce-and-data-parallelism">Map reduce and data parallelism</a></li>
        </ol>
    </div>
    </div>
    <div id="article-wrapper">
        <section id="article-body"></section>
        <div id="related-articles">
            <h2>Related Articles</h2>
            <ol>
                <li><a href="./stanford_machine_learning_week17.html">Stanford Machine Learning Week17</a></li>
                <li><a href="./stanford_machine_learning_week08.html">Stanford Machine Learning Week08</a></li>
                <li><a href="./stanford_machine_learning_week10.html">Stanford Machine Learning Week10</a></li>
                <li><a href="./stanford_machine_learning_week06.html">Stanford Machine Learning Week06</a></li>
                <li><a href="./stanford_machine_learning_week14.html">Stanford Machine Learning Week14</a></li>
                <li><a href="./stanford_machine_learning_week07.html">Stanford Machine Learning Week07</a></li>
                <li><a href="./stanford_machine_learning_week02.html">Stanford Machine Learning Week02</a></li>
                <li><a href="./stanford_machine_learning_week04.html">Stanford Machine Learning Week04</a></li>
                <li><a href="./stanford_machine_learning_week16.html">Stanford Machine Learning Week16</a></li>
                <li><a href="./stanford_machine_learning_week09.html">Stanford Machine Learning Week09</a></li>
                <li><a href="./stanford_machine_learning_week18.html">Stanford Machine Learning Week18</a></li>
                <li><a href="./stanford_machine_learning_week01.html">Stanford Machine Learning Week01</a></li>
                <li><a href="./stanford_machine_learning_week11.html">Stanford Machine Learning Week11</a></li>
                <li><a href="./stanford_machine_learning_week03.html">Stanford Machine Learning Week03</a></li>
                <li><a href="./stanford_machine_learning_week13.html">Stanford Machine Learning Week13</a></li>
                <li><a href="./stanford_machine_learning_week12.html">Stanford Machine Learning Week12</a></li>
                <li><a href="./stanford_machine_learning_week15.html">Stanford Machine Learning Week15</a></li>
            </ol>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/addjellouli/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
</body>

</html>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>