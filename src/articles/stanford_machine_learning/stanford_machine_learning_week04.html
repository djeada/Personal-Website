<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <title>Linear Regression with Multiple Variables</title>
    <meta charset="utf-8" />
    <meta content="XXX" name="description" />
    <meta content="Adam Djellouli" name="keywords" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" type="text/css" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <link href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport">
    <meta content="ie-edge" http-equiv="X-UA-Compatible">
    </meta>
    </meta>
</head>

<body>
    <nav>
        <a class="logo" href="../index.html" title="Adam Djellouli - Home">
            <img alt="Adam Djellouli Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul>
            <li> <a href="../../index.html" title="Home"> Home </a> </li>
            <li> <a class="active" href="../../core/blog.html" title="Adam Djellouli's Blog - Programming, technology and more"> Blog </a> </li>
            <li> <a href="../../core/tools.html" title="Useful Tools by Adam Djellouli"> Tools </a> </li>
            <li> <a href="../../core/projects.html" title="Projects by Adam Djellouli"> Projects </a> </li>
            <li> <a href="../../core/resume.html" title="Adam Djellouli's Resume"> Resume </a> </li>
            <li> <a href="../../core/about.html" title="About Adam Djellouli"> About </a> </li>
            <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
        </ul>
    </nav>
    <section id="article-body"></section>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="linear-regression-with-multiple-variables">Linear Regression with Multiple Variables</h2>
            <p>Multiple linear regression is a statistical method used to predict a dependent variable based on multiple independent variables. It is a extension of simple linear regression, which is used to predict a dependent variable based on only one independent variable. In multiple linear regression, we try to find the values of the parameters, known as theta, that result in the best fit line for the data. The cost function, which measures the difference between the predicted values and the actual values, is used to determine the optimal values for theta. An optimization algorithm called gradient descent is used to find the values of theta that minimize the cost function. It is important to scale the features and choose an appropriate learning rate for the algorithm to work efficiently. Techniques like mean normalization and feature scaling can be used to improve the performance of multiple linear regression.</p>
            <h2 id="the-algorithm">The Algorithm</h2>
            <ul>
                <li>Multiple variables = multiple features.</li>
                <li>$x_1$ - size, $x_2$ - number of bedrooms, $x_3$ - number of floors, $x_4$ - age of home.</li>
                <li>$n$ - number of features (n = 4).</li>
                <li>$m$ - number of examples (i.e. number of rows in a table).</li>
                <li>$x^i$ - vector of the input (in our example a vector of the four parameters for the $i^{th}$ input example), where $i$ is the training set's index.</li>
                <li>$x_j^i$ the value of $j^{th}$ feature in the $i^{th}$ training set. For example $x_2^3$ represents the number of bedrooms in the third house.</li>
            </ul>
            <p>Previously, our hypothesis had the following form:</p>
            <p>$$h_{\theta}(x) = \theta_0 + \theta_1x$$</p>
            <p>Now we have multiple features:</p>
            <p>$$h_{\theta}(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + + \theta_4x_4$$</p>
            <p>For convenience of notation, we can introduce $x_0 = 1$. As a result, your feature vector is now n + 1 dimensional and indexed from 0.</p>
            <p>$$h_{\theta}(x) = \theta^T X$$</p>
            <h2 id="gradient-descent-for-multiple-variables">Gradient descent for multiple variables</h2>
            <p>$$J(\theta_0, \theta_1, ..., \theta_n) = \frac{1}{2m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2$$</p>
            <pre><div><pre><code class="language-shell">\theta = [0] * n
while not converged:
  for j in [0, ..., n]:
      \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, ..., \theta_n)</code></pre>
    </div>
    </pre>
    <ul>
        <li>For each $\theta_j$ (0 until n) we make an simultaneous update.</li>
        <li>$\theta_j$ is now equal to it's previous value subtracted by learning rate ($\alpha$) times the partial derivative of of the $\theta$ vector with respect to $\theta_j$.</li>
    </ul>
    <h2 id="gradient-decent-in-practice-feature-scaling">Gradient Decent in practice: Feature Scaling</h2>
    <ul>
        <li>If you have a problem with multiple features, make sure they all have a comparable scale.</li>
        <li>For example, x1 = size (0 - 2000 feet) and x2 = number of bedrooms (1-5). Means the contours generated if we plot $\theta_1$ vs. $\theta_2$ give a very tall and thin shape due to the huge range difference.</li>
        <li>Finding the global minimum using gradient descent on this type of cost function might take a long time.</li>
    </ul>
    <p><img alt="feature_scaling" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/feature_scaling.png" /></p>
    <h2 id="mean-normalization">Mean normalization</h2>
    <ul>
        <li>Take a feature $x_i$.</li>
        <li>Replace it by $\frac{x_i - mean}{max}$.</li>
        <li>So your values all have an average of about 0.</li>
    </ul>
    <p><img alt="mean_normalization" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/mean_normalization.png" /></p>
    <h3 id="learning-rate-alpha-">Learning Rate $\alpha$</h3>
    <ul>
        <li>Plot the $min J(\theta)$ vs. the number of iterations (i.e. plot $J(\theta)$ throughout the course of gradient descent).</li>
        <li>$J(\theta)$ should decrease after each iteration if gradient descent is operating.</li>
        <li>Can also show if you're not making significant progress after a specific amount of days.</li>
        <li>If necessary, heuristics can be used to decrease the number of iterations.</li>
        <li>If, after 1000 iterations, $J(\theta)$ stops decreasing, you may choose to only conduct 1000 iterations in the future.</li>
        <li>DON'T hard-code thresholds like these in and then forget why they're there!</li>
    </ul>
    <p><img alt="min_cost_function" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/min_cost_function.png" /></p>
    <h2 id="automatic-convergence-tests">Automatic convergence tests</h2>
    <ul>
        <li>Check if $J(\theta)$ changes by a small threshold or less.</li>
        <li>Choosing this threshold is hard.</li>
        <li>Itâ€™s easier to check for a straight line.</li>
        <li>Why? - Because weâ€™re seeing the straightness in the context of the whole algorithm.</li>
    </ul>
    <p>If you plot $J(\theta)$ vs iterations and see the value is increasing - means you probably
        need a smaller $\alpha$.</p>
    <p><img alt="alpha_big" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/alpha_big.png" /></p>
    <p>However, you might overshoot, therefore lower your learning rate so that youn can reach the minimum (green line).</p>
    <p><img alt="alpha_small" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/alpha_small.png" /></p>
    <p>BUT, if $\alpha$ is too small then rate is too slow.</p>
    <h2 id="features-and-polynomial-regression">Features and polynomial regression</h2>
    <ul>
        <li>May fit the data better.</li>
        <li>Overfitting vs underfitting.</li>
    </ul>
    <p><img alt="polynomial_regression" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/polynomial_regression.png" /></p>
    <h2 id="normal-equation">Normal equation</h2>
    <ul>
        <li>The normal equation is a superior approach for some linear regression problems.</li>
        <li>Weâ€™ve been using a gradient descent - iterative method that takes steps to converge.</li>
        <li>Normal equation gives us $\theta$ analytically.</li>
    </ul>
    <h3 id="how-does-it-work-">How does it work?</h3>
    <ul>
        <li>Here $\theta$ is an n+1 dimensional vector of real numbers.</li>
        <li>Cost function is a function that takes that vector as an argument.</li>
        <li>How do we minimize this function?</li>
        <li>Take the partial derivative of $J(\theta)$ with respect $\theta_j$ and set to $0$ for every j. Solve for $\theta_0$ to $\theta_n$.</li>
    </ul>
    <h3 id="example">Example</h3>
    <p><img alt="normal_eq_table" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/normal_eq_table.png" /></p>
    <p>Steps:
        * $m=4$, $n=4$.
        * Add an extra column ($x_0$ feature).
        * Construct a matrix (X - the design matrix) which contains all the training data features in an $[m \times n+1]$ matrix.
        * Construct a column vector y vector $[m x 1]$ matrix.
        * Use the following equation for $\theta$:</p>
    <p>$$\theta = (X^TX)^{-1}X^Ty$$</p>
    <p><img alt="normal_eq_matrix" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/normal_eq_matrix.png" /></p>
    <p>If you compute this, you get the value of theta which minimize the cost function.</p>
    <h2 id="gradient-descent-vs-normal-equation">Gradient descent vs normal equation</h2>
    <p>
    <table>
        <tr>
            <td>Gradient Descent</td>
            <td>Normal Equation</td>
        </tr>
        <tr>
            <td>Need to choose learning rate</td>
            <td>No need to choose a learning rate</td>
            <td><br /></td>
        </tr>
        <tr>
            <td>Needs many iterations - could make it slower</td>
            <td>No need to iterate, check for convergence etc.</td>
        </tr>
        <tr>
            <td>Works well even when n is massive (millions)</td>
            <td>Slow of n is large</td>
            <td></td>
        </tr>
    </table>
    </p>
    </section>
    <div id="table-of-contents">
        <h2>Table of Contents</h2>
        <ol>
            <li><a href="#linear-regression-with-multiple-variables">Linear Regression with Multiple Variables</a></li>
            <li><a href="#the-algorithm">The Algorithm</a></li>
            <li><a href="#gradient-descent-for-multiple-variables">Gradient descent for multiple variables</a></li>
            <li><a href="#gradient-decent-in-practice-feature-scaling">Gradient Decent in practice: Feature Scaling</a></li>
            <li><a href="#mean-normalization">Mean normalization</a>
                <ol>
                    <li><a href="#learning-rate-alpha-">Learning Rate $\alpha$</a></li>
                </ol>
            </li>
            <li><a href="#automatic-convergence-tests">Automatic convergence tests</a></li>
            <li><a href="#features-and-polynomial-regression">Features and polynomial regression</a></li>
            <li><a href="#normal-equation">Normal equation</a>
                <ol>
                    <li><a href="#how-does-it-work-">How does it work?</a></li>
                    <li><a href="#example">Example</a></li>
                </ol>
            </li>
            <li><a href="#gradient-descent-vs-normal-equation">Gradient descent vs normal equation</a></li>
        </ol>
        <div id="related-articles">
            <h2>Related Articles</h2>
            <ol>
                <li><a href="./stanford_machine_learning_week01.html">Stanford Machine Learning Week01</a></li>
                <li><a href="./stanford_machine_learning_week02.html">Stanford Machine Learning Week02</a></li>
                <li><a href="./stanford_machine_learning_week03.html">Stanford Machine Learning Week03</a></li>
                <li><a href="./stanford_machine_learning_week04.html">Stanford Machine Learning Week04</a></li>
                <li><a href="./stanford_machine_learning_week06.html">Stanford Machine Learning Week06</a></li>
                <li><a href="./stanford_machine_learning_week07.html">Stanford Machine Learning Week07</a></li>
                <li><a href="./stanford_machine_learning_week08.html">Stanford Machine Learning Week08</a></li>
                <li><a href="./stanford_machine_learning_week09.html">Stanford Machine Learning Week09</a></li>
                <li><a href="./stanford_machine_learning_week10.html">Stanford Machine Learning Week10</a></li>
                <li><a href="./stanford_machine_learning_week11.html">Stanford Machine Learning Week11</a></li>
                <li><a href="./stanford_machine_learning_week12.html">Stanford Machine Learning Week12</a></li>
                <li><a href="./stanford_machine_learning_week13.html">Stanford Machine Learning Week13</a></li>
                <li><a href="./stanford_machine_learning_week14.html">Stanford Machine Learning Week14</a></li>
                <li><a href="./stanford_machine_learning_week15.html">Stanford Machine Learning Week15</a></li>
                <li><a href="./stanford_machine_learning_week16.html">Stanford Machine Learning Week16</a></li>
                <li><a href="./stanford_machine_learning_week17.html">Stanford Machine Learning Week17</a></li>
                <li><a href="./stanford_machine_learning_week18.html">Stanford Machine Learning Week18</a></li>
            </ol>
        </div>
    </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/addjellouli/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
</body>

</html>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>