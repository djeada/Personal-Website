<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Logistic Regression</title>
    <meta content="Logistic regression is a statistical method used for classification in machine learning." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>Last modified: June 11, 2024</i></p>
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="logistic-regression">Logistic Regression</h2>
            <p>Logistic regression is a statistical method used for classification in machine learning. Unlike linear regression, which predicts continuous values, logistic regression predicts discrete outcomes, like classifying an email as spam or not spam.</p>
            <h3 id="classification">Classification</h3>
            <p>Yields discrete values (e.g., 0 or 1, representing classes).</p>
            <p>Examples:</p>
            <ul>
                <li>Email: Spam (1) or Not Spam (0).</li>
                <li>Online Transaction: Fraudulent (1) or Not Fraudulent (0).</li>
                <li>Tumor Diagnosis: Malignant (1) or Benign (0).</li>
            </ul>
            <h3 id="logistic-regression-vs-linear-regression">Logistic Regression vs Linear Regression</h3>
            <p>Applying linear regression to classification tasks, like cancer diagnosis, may not yield effective results, especially when the data doesn't fit well into a linear model.</p>
            <p><img alt="cancer_classification" src="https://user-images.githubusercontent.com/37275728/201496614-36ec47d4-437e-4d25-82bf-27289489a5a7.png" /></p>
            <h3 id="hypothesis-representation">Hypothesis Representation</h3>
            <ul>
                <li>Classifier output should be between 0 and 1 (probability).</li>
                <li>Hypothesis $h_{\theta}(x) = g(\theta^Tx)$.</li>
                <li>$g(z)$ is the sigmoid or logistic function:</li>
            </ul>
            <p>$$
                g(z) = \frac{1}{1 + e^{-z}}
                $$</p>
            <p>Hypothesis Equation:</p>
            <p>$$
                h_{\theta}(x) = \frac{1}{1+e^{-\theta^Tx}}
                $$</p>
            <ul>
                <li>The output of $h_{\theta}(x)$ is interpreted as the probability of the positive class given the input $x$.
                    $$ h_{\theta}(x) = P(y=1|x\ ;\ \theta) $$</li>
                <li><strong>Example</strong>: If $h_{\theta}(x) = 0.7$ for a tumor, it implies a 70% chance of the tumor being malignant.</li>
            </ul>
            <h3 id="sigmoid-function">Sigmoid Function</h3>
            <p>Visualizes how $h_{\theta}(x)$ translates a linear combination of inputs into a probability:</p>
            <p><img alt="sigmoid" src="https://user-images.githubusercontent.com/37275728/201496643-38a45685-61a5-4af4-bf24-2acaa22ef1ff.png" /></p>
            <h3 id="decision-boundary-in-logistic-regression">Decision Boundary in Logistic Regression</h3>
            <p>The decision boundary in logistic regression is critical for classification tasks. It separates the different classes based on the probability calculated using the sigmoid function.</p>
            <h4 id="linear-decision-boundary">Linear Decision Boundary</h4>
            <ul>
                <li><strong>Principle</strong>: Predict $y = 1$ if the probability is greater than 0.5, else predict $y = 0$.</li>
                <li><strong>Hypothesis</strong>: $h_{\theta}(x) = g(\theta^T x)$, where $g$ is the sigmoid function.</li>
                <li><strong>Predicting $y = 1$</strong>: Occurs when $\theta^T x \geq 0$.</li>
                <li><strong>Predicting $y = 0$</strong>: Occurs when $\theta^T x \leq 0$.</li>
            </ul>
            <h4 id="example-of-a-linear-decision-boundary">Example of a Linear Decision Boundary</h4>
            <p>Hypothesis: </p>
            <p>$$
                h_{\theta}(x) = g(\theta_0 + \theta_1x_1 + \theta_2x_2)
                $$</p>
            <p>Theta Vector:</p>
            <p>$$
                \theta = \begin{bmatrix}
                -3 \\
                1 \\
                1
                \end{bmatrix}
                $$</p>
            <p>Condition for $y = 1$:</p>
            <p>$$
                -3 + x_1 + x_2 \geq 0
                $$</p>
            <p>Hence, the decision boundary is a straight line: $x_2 = -x_1 + 3$.</p>
            <p><img alt="linear_decision_boundary" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/linear_decision_boundary.png" /></p>
            <h4 id="non-linear-decision-boundaries">Non-linear Decision Boundaries</h4>
            <ul>
                <li><strong>Purpose</strong>: To fit more complex, non-linear datasets.</li>
                <li><strong>Approach</strong>: Introduce polynomial terms in the hypothesis.</li>
            </ul>
            <h4 id="example-of-a-non-linear-decision-boundary">Example of a Non-linear Decision Boundary</h4>
            <p>Hypothesis: </p>
            <p>$$
                h_{\theta}(x) = g(\theta_0 + \theta_1x_1 + \theta_2x_1^2 + \theta_3x_2^2)
                $$</p>
            <p>Theta Vector:</p>
            <p>$$
                \theta = \begin{bmatrix}
                -1 \\
                0 \\
                1 \\
                1
                \end{bmatrix}
                $$</p>
            <p>Condition for $y = 1$:</p>
            <p>$$
                x_1^2 + x_2^2 \geq 1
                $$</p>
            <p>This forms a circular decision boundary with radius 1 around the origin: $x_1^2 + x_2^2 = 1$.</p>
            <p><img alt="non_linear_decision_boundary" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/non_linear_decision_boundary.png" /></p>
            <h3 id="cost-function-for-logistic-regression">Cost Function for Logistic Regression</h3>
            <p>Logistic regression uses a different cost function compared to linear regression, tailored to the classification setting.</p>
            <h4 id="training-set-representation">Training Set Representation</h4>
            <p>Consider a training set of $m$ examples:</p>
            <p>$$ {(x^{(1)}, y^{(1)}), ..., (x^{(m)}, y^{(m)})} $$</p>
            <p>where</p>
            <p>$$<br />
                x = \begin{bmatrix}
                x_0 \\
                x_1 \\
                ... \\
                x_n
                \end{bmatrix}
                $$</p>
            <p>with $x_0 = 1$ and $y$ being either 0 or 1.</p>
            <h4 id="linear-regression-cost-function">Linear Regression Cost Function</h4>
            <p>In linear regression, the cost function $J(\theta)$ is defined as:</p>
            <p>$$ J(\theta) = \frac{1}{2m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2 $$</p>
            <h4 id="defining-cost-for-logistic-regression">Defining Cost for Logistic Regression</h4>
            <p>For logistic regression, we define a different "cost" function:</p>
            <p>$$ cost(h_{\theta}(x^{(i)}), y^{(i)}) = \frac{1}{2} (h_{\theta}(x^{(i)}) - y^{(i)})^2 $$</p>
            <p>Redefining $J(\theta)$:</p>
            <p>$$ J(\theta) = \frac{1}{m} \sum_{i=1}^{m}cost(h_{\theta}(x^{(i)}), y^{(i)}) $$</p>
            <p>This cost function for logistic regression is not convex, leading to potential issues with local optima.</p>
            <h4 id="logistic-regression-cost-function">Logistic Regression Cost Function</h4>
            <p>The logistic regression cost function is defined as:</p>
            <p>$$
                cost(h_{\theta}(x), y) = \begin{cases}
                -\log(h_{\theta}(x)) &amp; \text{if } y=1 \\
                -\log(1 - h_{\theta}(x)) &amp; \text{if } y=0
                \end{cases}
                $$</p>
            <p>Then, the overall cost function $J(\theta)$ becomes:</p>
            <p>$$J(\theta) = \frac{1}{m} \sum_{i=1}^{m}[-y^{(i)}\log(h_{\theta}(x^{(i)})) - (1-y^{(i)})\log(1 - h_{\theta}(x^{(i)}))]
                $$</p>
            <h4 id="gradient-of-the-cost-function">Gradient of the Cost Function</h4>
            <p>The gradient of $J(\theta)$ for logistic regression is:</p>
            <p>$$
                \frac{\partial}{\partial \theta_j} J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}
                $$</p>
            <p>Note: While this gradient looks identical to that of linear regression, the formulae differ due to the different definitions of $h_{\theta}(x)$ in linear and logistic regression.</p>
            <h3 id="multiclass-classification-problems">Multiclass Classification Problems</h3>
            <p>Logistic regression can be extended to handle multiclass classification problems through the "one-vs-all" (or "one-vs-rest") method.</p>
            <h4 id="one-vs-all-approach">One-vs-All Approach</h4>
            <p>The one-vs-all strategy involves training multiple binary classifiers, each focused on distinguishing one class from all other classes.</p>
            <h4 id="visualization-of-multiclass-classification">Visualization of Multiclass Classification</h4>
            <p>Consider a dataset with three classes: triangles, crosses, and squares.</p>
            <p><img alt="multiclass_classification" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/multiclass_classification.png" /></p>
            <h4 id="implementing-one-vs-all">Implementing One-vs-All</h4>
            <p>The process involves splitting the training set into separate binary classification problems:</p>
            <ol>
                <li><strong>Triangle vs Others</strong>: Train a classifier $h_{\theta}^{(1)}(x)$ to distinguish triangles (1) from crosses and squares (0).</li>
                <li><strong>Crosses vs Others</strong>: Train another classifier $h_{\theta}^{(2)}(x)$ to distinguish crosses (1) from triangles and squares (0).</li>
                <li><strong>Squares vs Others</strong>: Lastly, train a classifier $h_{\theta}^{(3)}(x)$ to distinguish squares (1) from crosses and triangles (0).</li>
            </ol>
            <p><img alt="one_vs_all" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/one_vs_all.png" /></p>
            <h4 id="classification-decision">Classification Decision</h4>
            <ul>
                <li>When classifying a new example, compute the probability that it belongs to each class using the respective classifiers.</li>
                <li>The class with the highest probability is chosen as the prediction.</li>
            </ul>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol><a href="#logistic-regression">Logistic Regression</a>
                <ol>
                    <li><a href="#classification">Classification</a></li>
                    <li><a href="#logistic-regression-vs-linear-regression">Logistic Regression vs Linear Regression</a></li>
                    <li><a href="#hypothesis-representation">Hypothesis Representation</a></li>
                    <li><a href="#sigmoid-function">Sigmoid Function</a></li>
                    <li><a href="#decision-boundary-in-logistic-regression">Decision Boundary in Logistic Regression</a>
                        <ol>
                            <li><a href="#linear-decision-boundary">Linear Decision Boundary</a></li>
                            <li><a href="#example-of-a-linear-decision-boundary">Example of a Linear Decision Boundary</a></li>
                            <li><a href="#non-linear-decision-boundaries">Non-linear Decision Boundaries</a></li>
                            <li><a href="#example-of-a-non-linear-decision-boundary">Example of a Non-linear Decision Boundary</a></li>
                        </ol>
                    </li>
                    <li><a href="#cost-function-for-logistic-regression">Cost Function for Logistic Regression</a>
                        <ol>
                            <li><a href="#training-set-representation">Training Set Representation</a></li>
                            <li><a href="#linear-regression-cost-function">Linear Regression Cost Function</a></li>
                            <li><a href="#defining-cost-for-logistic-regression">Defining Cost for Logistic Regression</a></li>
                            <li><a href="#logistic-regression-cost-function">Logistic Regression Cost Function</a></li>
                            <li><a href="#gradient-of-the-cost-function">Gradient of the Cost Function</a></li>
                        </ol>
                    </li>
                    <li><a href="#multiclass-classification-problems">Multiclass Classification Problems</a>
                        <ol>
                            <li><a href="#one-vs-all-approach">One-vs-All Approach</a></li>
                            <li><a href="#visualization-of-multiclass-classification">Visualization of Multiclass Classification</a></li>
                            <li><a href="#implementing-one-vs-all">Implementing One-vs-All</a></li>
                            <li><a href="#classification-decision">Classification Decision</a></li>
                        </ol>
                    </li>
                </ol>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_01_introduction_to_machine_learning.html">Week 01 Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_02_linear_regression.html">Week 02 Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_03_review_of_linear_algebra.html">Week 03 Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_04_linear_regression_multiple_variables.html">Week 04 Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_06_logistic_regression.html">Week 06 Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_07_regularization.html">Week 07 Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_08_neural_networks_representation.html">Week 08 Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_09_neural_networks_learning.html">Week 09 Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_10_applying_machine_learning_advice.html">Week 10 Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_11_machine_learning_system_design.html">Week 11 Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_12_support_vector_machines.html">Week 12 Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_13_clustering.html">Week 13 Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_14_dimensionality_reduction.html">Week 14 Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_15_anomaly_detection.html">Week 15 Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_16_recommendation_systems.html">Week 16 Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_17_large_scale_machine_learning.html">Week 17 Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford_machine_learning/week_18_photo_ocr.html">Week 18 Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>