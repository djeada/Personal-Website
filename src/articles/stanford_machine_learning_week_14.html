<!DOCTYPE html>
<html lang="en">

<head>
    <title>Adam Djellouli - Blog</title>
    <meta charset="utf-8">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" />
    <link rel="icon" href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico">
    <link rel="stylesheet" type="text/css" href="../resources/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie-edge" />
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089" crossorigin="anonymous"></script>
</head>

<body>
    <nav>
        <a class="logo" href="../index.html">
            <img id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" alt="Adam Djellouli">
        </a>
        <input id="navbar-toggle" type="checkbox" />
        <ul>
            <li> <a href="../index.html"> Home </a> </li>
            <li> <a href="../core/blog.html" class="active"> Blog </a> </li>
            <li> <a href="../core/tools.html"> Tools </a> </li>
            <li> <a href="../core/projects.html"> Projects </a> </li>
            <li> <a href="../core/resume.html"> Resume </a> </li>
            <li> <a href="../core/about.html"> About </a> </li>
            <button id="dark-mode-button"></button>
        </ul>
    </nav>
    <section id="article-body">
        <p style='text-align: right;'><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>

        <h2>Dimensionality Reduction</h2>
        <p>Principle Component Analysis (PCA) is a technique used in machine learning to reduce the dimensionality of data and improve the performance of algorithms. It works by finding a lower dimensional surface that minimizes the projection error, or the distance between each point and the projected version of the point. PCA can be used for tasks such as compression, visualization, and noise reduction. It is also useful for feature selection and can be used to improve the performance of machine learning algorithms. To use PCA, you need to compute the covariance matrix and find the eigenvectors of this matrix, then choose the first k eigenvectors and use them to calculate a new feature representation. You can choose the number of principle components by comparing the projection error to the total data variation.</p>
        <h2>Compression</h2>
        <ul>
            <li>Speeds up algorithms.</li>
            <li>Saves space.</li>
            <li>Dimension reduction: not all features are needed.</li>
            <li>Example: different units for same attribute.</li>
        </ul>
        <p><img alt="compression_units" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/compression_units.png" /></p>
        <p>Now we can represent x1 as a 1D number (Z dimension).</p>
        <h2>Visualization</h2>
        <ul>
            <li>It is difficult to visualize higher dimensional data.</li>
            <li>Dimensionality reduction can help us show information in a more readable fashion for human consumption.</li>
            <li>Collect a huge data set including numerous facts about a country from around the world.</li>
        </ul>
        <p><img alt="table" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/table.png" /></p>
        <ul>
            <li>Assume each country has 50 characteristics.</li>
            <li>How can we better comprehend this data?</li>
            <li>Plotting 50-dimensional data is quite difficult.</li>
            <li>Create a new feature representation (2 z values) that summarizes these features.</li>
            <li>Reduce $50D\ -&gt;\ 2D$ (now possible to plot).</li>
        </ul>
        <h2>Principle Component Analysis (PCA): problem formulation</h2>
        <ul>
            <li>Assume we have a 2D data collection that we want to reduce to 1D.</li>
            <li>How can we choose a single line that best fits our data?</li>
            <li>The distance between each point and the projected version should be as little as possible (blue lines below are short).</li>
            <li>PCA tries to find a lower dimensional surface so the sum of squares onto that surface is minimized.</li>
            <li>PCA tries to find the surface (a straight line in this case) which has the minimum projection error.</li>
        </ul>
        <p><img alt="pca" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/pca.png" /></p>
        <ul>
            <li>PCA is not linear regression.</li>
            <li>For linear regression, fitting a straight line to minimize the straight line between a point and a squared line. VERTICAL distance between point.</li>
            <li>For PCA minimizing the magnitude of the shortest orthogonal distance.</li>
            <li>With PCA there is no $y$ - instead we have a list of features and all features are treated equally.</li>
        </ul>
        <h2>PCA algorithm</h2>
        <ul>
            <li>Compute the covariance matrix.</li>
        </ul>
        <p>$$\Sigma = \frac{1}{m} \sum_{i=1}^{n} (x^{(i)})(x^{(i)})^T$$</p>
        <ul>
            <li>This is an $[n x n]$ matrix (Remember than $x^i$ is a $[n \times 1]$ matrix).</li>
            <li>Next, compute eigenvectors of matrix $\Sigma$.</li>
            <li>[U,S,V] = svd(sigma)</li>
            <li>$U$ matrix is also an $[n \times n]$ matrix. Turns out the columns of $U$ are the u vectors we want!</li>
            <li>Just take the first k-vectors from U.</li>
            <li>Next, calculate $z$. $$z = (U_{reduce})^T \cdot x$$</li>
        </ul>
        <h2>Reconstruction from compressed representation</h2>
        <ul>
            <li>Is it possible to decompress data from a low dimensionality format to a higher dimensionality format?</li>
        </ul>
        <p>$$x_{approx} = U_{reduce} \cdot z$$</p>
        <ul>
            <li>We lose some information (everything is now precisely aligned on that line), but it is now projected into 2D space.</li>
        </ul>
        <h2>Choosing the number of principle components</h2>
        <ul>
            <li>PCA attempts to minimize the averaged squared projection error.</li>
        </ul>
        <p>$$\frac{1}{m} \sum_{i=1}^{m} ||x^{(i)} - x_{approx}^{(i)}||^2$$</p>
        <ul>
            <li>Total data variation may be defined as the average over data indicating how distant the training instances are from the origin.</li>
        </ul>
        <p>$$\frac{1}{m} \sum_{i=1}^{m} ||x^{(i)}||^2$$</p>
        <ul>
            <li>To determine k, we may use the following formula:</li>
        </ul>
        <p>$$
            \frac{\frac{1}{m} \sum_{i=1}^{m} ||x^{(i)} - x_{approx}^{(i)}||^2}
            {\frac{1}{m} \sum_{i=1}^{m} ||x^{(i)}||^2}
            \leq 0.01
            $$</p>
        <h2>Applications of PCA</h2>
        <ul>
            <li>Compression: Reduce the amount of memory/disk space required to hold data.</li>
            <li>Visualization: k=2 or k=3 for plotting.</li>
            <li>A poor application of PCA is to avoid over-fitting. PCA discards certain data without understanding what values it is discarding.</li>
            <li>Examine how a system works without PCA first, and then apply PCA only if you have reason to believe it will help.</li>
        </ul>

    </section>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" alt="Adam Djellouli">

            </div>
            <div class="footer-column">

                <p>
                    Thank you for visiting my personal website. All of the </br>
                    content on this site is free to use, but please remember </br>
                    to be a good human being and refrain from any abuse</br>
                    of the site. If you would like to contact me, please use </br>
                    my LinkedIn profile or my GitHub if you have any technical </br>
                    issues or ideas to share. I wish you the best and hope you </br>
                    have a fantastic life. </br>
                </p>

            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" class="fa fa-youtube" target="_blank">

                        </a>YouTube
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" class="fa fa-linkedin" target="_blank">

                        </a>LinkedIn
                    </li>
                    <li>
                        <a href="https://www.instagram.com/addjellouli/" class="fa fa-instagram" target="_blank">
                        </a>Instagram

                    </li>
                    <li>
                        <a href="https://github.com/djeada" class="fa fa-github">
                        </a>Github

                    </li>

                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                &copy; Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../app.js"></script>
    </footer>
</body>

</html>

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>

<script type="text/javascript" id="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>