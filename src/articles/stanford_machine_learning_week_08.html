<!DOCTYPE html>
<html lang="en">

<head>
    <title>Adam Djellouli - Blog</title>
    <meta charset="utf-8">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" />
    <link rel="icon" href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico">
    <link rel="stylesheet" type="text/css" href="../resources/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie-edge" />
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089" crossorigin="anonymous"></script>
</head>

<body>
    <nav>
        <a class="logo" href="../index.html">
            <img id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" alt="Adam Djellouli">
        </a>
        <input id="navbar-toggle" type="checkbox" />
        <ul>
            <li> <a href="../index.html"> Home </a> </li>
            <li> <a href="../core/blog.html" class="active"> Blog </a> </li>
            <li> <a href="../core/tools.html"> Tools </a> </li>
            <li> <a href="../core/projects.html"> Projects </a> </li>
            <li> <a href="../core/resume.html"> Resume </a> </li>
            <li> <a href="../core/about.html"> About </a> </li>
            <button id="dark-mode-button"></button>
        </ul>
    </nav>
    <section id="article-body">
        <p style='text-align: right;'><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>

        <h2>Neural Networks - Representation</h2>
        <p>Neural networks are a type of machine learning model that is designed to mimic the way the brain works. They are particularly useful for handling large and complex data sets that may have many features. Logistic regression, a common method for analyzing such data, can become computationally expensive when dealing with a large number of features. To avoid this issue, a subset of the features must be used, which can reduce the accuracy of the analysis. Neural networks have become more popular in recent years due to the increasing computational power of computers, which has made it possible to build large-scale neural networks. These networks are made up of neurons, which are connected by input and output wires and communicate through electric spikes. In an artificial neural network, input is fed through input wires and processed before being sent out through output wires. The input layer is the first layer, the output layer is the last, and any layers in between are called hidden layers. The weights of the model are represented by the $\theta$ vector, and the bias unit is represented by $x_0$. Neural networks are particularly useful in fields such as computer vision, where the data sets can be very large.</p>
        <h3>Why do we need neural networks?</h3>
        <ul>
            <li>We can have complex data sets with many (1000's) important features.</li>
            <li>Using logistic regression becomes expensive really fast.</li>
            <li>The only way to get around this is to use a subset of features. This, however, may result in less accuracy.</li>
        </ul>
        <p><img alt="many_features_classifier" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/many_features_classifier.png" /></p>
        <h2>Problems where n is large - computer vision</h2>
        <ul>
            <li>Computer vision sees a matrix of pixel intensity values.</li>
            <li>To build a car detector: Build a training set of cars and not cars. Then test against a car.</li>
            <li>Plot two pixels (two pixel locations) and car or not car on the graph.</li>
        </ul>
        <p><img alt="cars" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/cars.png" /></p>
        <p>Feature space</p>
        <h2>Problems where n is large - computer vision</h2>
        <ul>
            <li>If we used 50 x 50 pixels $--&gt;$ 2500 pixels, so n = 2500.</li>
            <li>If RGB then 7500.</li>
            <li>If 100 x 100 RB then $--&gt;$ 50 000 000 features.</li>
            <li>Way too big.</li>
        </ul>
        <h3>Neurons and the brain</h3>
        <ul>
            <li>The desire to construct computers that mimicked brain activities drove the creation of neural networks (NNs).</li>
            <li>Used a lot in the 80s. Popularity diminished in 90s.</li>
            <li>Large-scale neural networks have just recently become computationally feasible due to the computational cost of NNs.</li>
        </ul>
        <h2>Brain</h2>
        <ul>
            <li>Hypothesis is that the brain has a single learning algorithm.</li>
            <li>If the optic nerve is rerouted to the auditory cortex, the auditory cortex is learns to see.</li>
            <li>If you rewrite the optic nerve to the somatosensory cortex then it learns to see.</li>
        </ul>
        <h3>Model representation I</h3>
        <p>Neuron:</p>
        <ul>
            <li>Cell body</li>
            <li>Number of input wires (dendrites)</li>
            <li>Output wire (axon)</li>
        </ul>
        <p><img alt="neuron" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/neuron.png" /></p>
        <ul>
            <li>Neurone gets one or more inputs through dendrites.</li>
            <li>Does processing.</li>
            <li>The output is sent along the axon.</li>
            <li>Neurons communicate through electric spikes.</li>
        </ul>
        <h2>Artificial neural network - representation of a neurone</h2>
        <ul>
            <li>Feed input via input wires.</li>
            <li>Logistic unit does computation.</li>
            <li>Sends output down output wires.</li>
        </ul>
        <p><img alt="neuron" src="https://user-images.githubusercontent.com/37275728/201517992-cdc14304-2af9-4821-bcae-71caa1a62663.png" /></p>
        <p>$$
            x = \begin{bmatrix}
            x_{0} \\
            x_{1} \\
            x_2 \\
            x_3
            \end{bmatrix}
            $$</p>
        <p>$$
            \theta = \begin{bmatrix}
            \theta_{0} \\
            \theta_{1} \\
            \theta_2 \\
            \theta_3
            \end{bmatrix}
            $$</p>
        <ul>
            <li>The diagram above represents a single neurone.</li>
            <li>$x_0$ is called the bias unit.</li>
            <li>$\theta$ vector is called the weights of a model.</li>
        </ul>
        <p><img alt="hidden_layer" src="https://user-images.githubusercontent.com/37275728/201517995-ff2af22c-ea22-4be9-9bfc-b7e6c771d69c.png" /></p>
        <ul>
            <li>First layer is the input layer.</li>
            <li>Final layer is the output layer - produces value computed by a hypothesis.</li>
            <li>Middle layer(s) are called the hidden layers.</li>
            <li>You don't observe the values processed in the hidden layer.</li>
            <li>Every input/activation goes to every node in following layer.</li>
        </ul>
        <p>$$a^{(2)}<em>1 = g(\Theta^{(1)}</em>{10}x_0+\Theta^{(1)}<em>{11}x_1+\Theta^{(1)}</em>{12}x_2+\Theta^{(1)}<em>{13}x_3)$$
                $$a^{(2)}_2 = g(\Theta^{(1)}</em>{20}x_0+\Theta^{(1)}<em>{21}x_1+\Theta^{(1)}</em>{22}x_2+\Theta^{(1)}<em>{23}x_3)$$
                $$a^{(2)}_1 = g(\Theta^{(1)}</em>{30}x_0+\Theta^{(1)}<em>{31}x_1+\Theta^{(1)}</em>{32}x_2+\Theta^{(1)}<em>{33}x_3)$$
                $$h</em>{\Theta}(x) = g(\Theta^{(2)}<em>{10}a^{(2)}_0+\Theta^{(2)}</em>{11}a^{(2)}<em>1+\Theta^{(2)}</em>{12}a^{(2)}<em>2+\Theta^{(2)}</em>{13}a^{(2)}_3)$$</p>
        <h3>Model representation II</h3>
        <p>In this section, we'll look at how to do the computation efficiently using a vectorized approach. We'll also look at why NNs are useful and how we can use them to learn complicated nonlinear things.</p>
        <p>Let's define a few more terms:</p>
        <p>$$z^{(2)}<em>1 = \Theta^{(1)}</em>{10}x_0+\Theta^{(1)}<em>{11}x_1+\Theta^{(1)}</em>{12}x_2+\Theta^{(1)}_{13}x_3$$</p>
        <p>We can now write:
            $$a^{(2)}_1 = g(z^{(2)}_1)$$</p>
        <p>$$
            x = \begin{bmatrix}
            x_{0} \\
            x_{1} \\
            x_2 \\
            x_3
            \end{bmatrix}
            $$</p>
        <p>$$
            z^{(2)} = \begin{bmatrix}
            z^{(2)}_1 \\
            z^{(2)}_2 \\
            z^{(2)}_3
            \end{bmatrix}
            $$</p>
        <ul>
            <li>$z^{(2)}$ is a $3x1$ vector.</li>
            <li>$a^{(2)}$ ia also a $3x1$ vector.</li>
            <li>Middle layer(s) are called the hidden layers.</li>
            <li>$g()$ applies the sigmoid (logistic) function element wise to each member of the $z^{(2)}$ vector.</li>
            <li>Obviously the "activation" for the input layer is just the input!</li>
            <li>$a^{(1)}$ is the vector of inputs.</li>
            <li>$a^{(2)}$ is the vector of values calculated by the $g(z^{(2)})$ function.</li>
            <li>We send our input values to the hidden layers and let them learn which values produce the best final result to feed into the final output layer.</li>
            <li>This process is also called forward propagation.</li>
        </ul>
        <p>Other architectural designs are also possible:</p>
        <ul>
            <li>More/less nodes per layer.</li>
            <li>More layers.</li>
        </ul>
        <p><img alt="multi_layer" src="https://user-images.githubusercontent.com/37275728/201517998-e5f9f245-a6f1-4aed-8a58-fcb0178f38c4.png" /></p>
        <p>Layer 2 has three hidden units (plus bias), layer 3 has two hidden units (plus bias), and by the time you get to the output layer, you have a really intriguing non-linear hypothesis.</p>
        <h3>AND function</h3>
        <p><img alt="and_function" src="https://user-images.githubusercontent.com/37275728/201518002-72b41fb7-ca3f-4612-aa65-c34f58138737.png" /></p>
        <p>Let $x_0 = 1$ and theta vector be:</p>
        <p>$$
            \Theta^{(1)}_1 = \begin{bmatrix}
            -30 \\
            20 \\
            20
            \end{bmatrix}
            $$</p>
        <p>Then hypothesis is:</p>
        <p>$$h_{\Theta}(x) = g(-30 \cdot 1 + 20 \cdot x_1 + 20 \cdot x_2)$$</p>
        <p><img alt="sigmoid" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/sigmoid.png" /></p>
        <h3>XNOR function</h3>
        <p><img alt="xnor" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/xnor.png" /></p>

    </section>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" alt="Adam Djellouli">

            </div>
            <div class="footer-column">

                <p>
                    Thank you for visiting my personal website. All of the </br>
                    content on this site is free to use, but please remember </br>
                    to be a good human being and refrain from any abuse</br>
                    of the site. If you would like to contact me, please use </br>
                    my LinkedIn profile or my GitHub if you have any technical </br>
                    issues or ideas to share. I wish you the best and hope you </br>
                    have a fantastic life. </br>
                </p>

            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" class="fa fa-youtube" target="_blank">

                        </a>YouTube
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" class="fa fa-linkedin" target="_blank">

                        </a>LinkedIn
                    </li>
                    <li>
                        <a href="https://www.instagram.com/addjellouli/" class="fa fa-instagram" target="_blank">
                        </a>Instagram

                    </li>
                    <li>
                        <a href="https://github.com/djeada" class="fa fa-github">
                        </a>Github

                    </li>

                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                &copy; Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../app.js"></script>
    </footer>
</body>

</html>

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"], ["\(","\)"] ], displayMath: [ ["$$","$$"], ["\[", "\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>

<script type="text/javascript" id="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>