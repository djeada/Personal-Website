<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <title>Dimensionality Reduction with Principal Component Analysis (PCA)</title>
    <meta charset="utf-8" />
    <meta content="Principal Component Analysis (PCA) is a widely used technique in machine learning for dimensionality reduction." name="description" />
    <meta content="Adam Djellouli" name="keywords" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" type="text/css" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <link href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="ie-edge" http-equiv="X-UA-Compatible" />
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul role="menu">
            <li role="menuitem"> <a href="../../index.html" title="Go to Home Page"> Home </a> </li>
            <li role="menuitem"> <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a> </li>
            <li role="menuitem"> <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a> </li>
            <li role="menuitem"> <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a> </li>
            <li role="menuitem"> <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a> </li>
            <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
        </ul>
    </nav>
    <section id="article-body"></section>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="dimensionality-reduction-with-principal-component-analysis-pca-">Dimensionality Reduction with Principal Component Analysis (PCA)</h2>
            <p>Principal Component Analysis (PCA) is a widely used technique in machine learning for dimensionality reduction. It simplifies the complexity in high-dimensional data while retaining trends and patterns.</p>
            <h3 id="understanding-pca">Understanding PCA</h3>
            <ul>
                <li><strong>Objective</strong>: PCA seeks to find a lower-dimensional surface onto which data points can be projected with minimal loss of information.</li>
                <li><strong>Methodology</strong>: It involves computing the covariance matrix of the data, finding its eigenvectors (principal components), and projecting the data onto a space spanned by these eigenvectors.</li>
                <li><strong>Applications</strong>: PCA is used for tasks such as data compression, noise reduction, visualization, feature selection, and enhancing the performance of machine learning algorithms.</li>
            </ul>
            <h3 id="compression-with-pca">Compression with PCA</h3>
            <ul>
                <li>Speeds up learning algorithms.</li>
                <li>Saves storage space.</li>
                <li>Focuses on the most relevant features, discarding less important ones.</li>
                <li><strong>Example</strong>: Different units of the same attribute can be reduced to a single, more representative dimension.</li>
            </ul>
            <p><img alt="Example of Dimension Reduction" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/compression_units.png" /></p>
            <h3 id="visualization-through-pca">Visualization through PCA</h3>
            <ul>
                <li><strong>Challenge</strong>: High-dimensional data is difficult to visualize.</li>
                <li><strong>Solution</strong>: PCA can reduce dimensions to make data visualization more feasible and interpretable.</li>
                <li><strong>Example</strong>: Representing 50 features of a dataset as a 2D plot, simplifying analysis and interpretation.</li>
            </ul>
            <p><img alt="Example of Data Table" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/table.png" /></p>
            <h3 id="pca-problem-formulation">PCA Problem Formulation</h3>
            <ol>
                <li><strong>Goal</strong>: To find a lower-dimensional representation that minimizes the projection error (distance between original and projected points).</li>
                <li><strong>Projection Error</strong>: PCA aims to minimize the sum of the squares of these distances.</li>
            </ol>
            <p><img alt="Visualizing PCA Projection" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/pca.png" /></p>
            <ol>
                <li>
                    <p><strong>PCA vs. Linear Regression</strong>:</p>
                </li>
                <li>
                    <p>Linear Regression: Minimizes the vertical distances between data points and the fitted line (predictive model).</p>
                </li>
                <li>PCA: Minimizes the orthogonal distances to the line (data representation model), without distinguishing between dependent and independent variables.</li>
            </ol>
            <h3 id="selecting-principal-components">Selecting Principal Components</h3>
            <ul>
                <li><strong>Number of Components</strong>: The choice of how many principal components to retain depends on the trade-off between minimizing projection error and reducing dimensionality.</li>
                <li><strong>Variance Retained</strong>: Ideally, the selected components should retain most of the variance of the original data.</li>
            </ul>
            <h3 id="principal-component-analysis-pca-algorithm">Principal Component Analysis (PCA) Algorithm</h3>
            <p>Principal Component Analysis (PCA) is a systematic process for reducing the dimensionality of data. Here's a breakdown of the PCA algorithm:</p>
            <ol>
                <li><strong>Covariance Matrix Computation</strong>: Calculate the covariance matrix $\Sigma$:</li>
            </ol>
            <p>$$\Sigma = \frac{1}{m} \sum_{i=1}^{n} (x^{(i)})(x^{(i)})^T$$</p>
            <p>Here, $\Sigma$ is an $[n \times n]$ matrix, with each $x^{(i)}$ being an $[n \times 1]$ matrix.</p>
            <ol>
                <li><strong>Eigenvector Calculation</strong>: Compute the eigenvectors of the covariance matrix $\Sigma$:</li>
            </ol>
            <p>$$
                [U,S,V] = \text{svd}(\Sigma)
                $$</p>
            <p>The matrix $U$ will also be an $[n \times n]$ matrix, with its columns being the eigenvectors we seek.</p>
            <ol>
                <li>
                    <p><strong>Choosing Principal Components</strong>: Select the first $k$ eigenvectors from $U$, this is $U_{\text{reduce}}$.</p>
                </li>
                <li>
                    <p><strong>Calculating Compressed Representation</strong>: For each data point $x$, compute its new representation $z$:</p>
                </li>
            </ol>
            <p>$$z = (U_{\text{reduce}})^T \cdot x$$</p>
            <h3 id="reconstruction-from-compressed-representation">Reconstruction from Compressed Representation</h3>
            <p>Is it possible to go back from a lower dimension to a higher one? While exact reconstruction is not possible (since some information is lost), an approximation can be obtained:</p>
            <p>$$x_{\text{approx}} = U_{\text{reduce}} \cdot z$$</p>
            <p>This approximates the original data in the higher-dimensional space but aligned along the principal components.</p>
            <h3 id="choosing-the-number-of-principal-components">Choosing the Number of Principal Components</h3>
            <p>The number of principal components ($k$) is a crucial choice in PCA. The objective is to minimize the average squared projection error while retaining most of the variance in the data:</p>
            <ul>
                <li><strong>Average Squared Projection Error</strong>:</li>
            </ul>
            <p>$$\frac{1}{m} \sum_{i=1}^{m} ||x^{(i)} - x_{\text{approx}}^{(i)}||^2$$</p>
            <ul>
                <li><strong>Total Data Variation</strong>:</li>
            </ul>
            <p>$$\frac{1}{m} \sum_{i=1}^{m} ||x^{(i)}||^2$$</p>
            <ul>
                <li><strong>Choosing $k$</strong>:
                    The fraction of variance retained is often set as a threshold (e.g., 99%):</li>
            </ul>
            <p>$$
                \frac{\frac{1}{m} \sum_{i=1}^{m} ||x^{(i)} - x_{\text{approx}}^{(i)}||^2}
                {\frac{1}{m} \sum_{i=1}^{m} ||x^{(i)}||^2}
                \leq 0.01
                $$</p>
            <h3 id="applications-of-pca">Applications of PCA</h3>
            <ol>
                <li><strong>Compression</strong>: Reducing data size for storage or faster processing.</li>
                <li><strong>Visualization</strong>: With $k=2$ or $k=3$, data can be visualized in 2D or 3D space.</li>
                <li><strong>Limitation</strong>: PCA should not be used indiscriminately to prevent overfitting. It removes data dimensions without understanding their importance.</li>
                <li><strong>Usage Advice</strong>: It's recommended to try understanding the data without PCA first and apply PCA if it is believed to aid in achieving specific objectives.</li>
            </ol>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol><a href="#dimensionality-reduction-with-principal-component-analysis-pca-">Dimensionality Reduction with Principal Component Analysis (PCA)</a>
                <ol>
                    <li><a href="#understanding-pca">Understanding PCA</a></li>
                    <li><a href="#compression-with-pca">Compression with PCA</a></li>
                    <li><a href="#visualization-through-pca">Visualization through PCA</a></li>
                    <li><a href="#pca-problem-formulation">PCA Problem Formulation</a></li>
                    <li><a href="#selecting-principal-components">Selecting Principal Components</a></li>
                    <li><a href="#principal-component-analysis-pca-algorithm">Principal Component Analysis (PCA) Algorithm</a></li>
                    <li><a href="#reconstruction-from-compressed-representation">Reconstruction from Compressed Representation</a></li>
                    <li><a href="#choosing-the-number-of-principal-components">Choosing the Number of Principal Components</a></li>
                    <li><a href="#applications-of-pca">Applications of PCA</a></li>
                </ol>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_01_introduction_to_machine_learning.html">Week 01 Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_02_linear_regression.html">Week 02 Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_03_review_of_linear_algebra.html">Week 03 Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_04_linear_regression_multiple_variables.html">Week 04 Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_06_logistic_regression.html">Week 06 Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_07_regularization.html">Week 07 Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_08_neural_networks_representation.html">Week 08 Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_09_neural_networks_learning.html">Week 09 Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_10_applying_machine_learning_advice.html">Week 10 Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_11_machine_learning_system_design.html">Week 11 Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_12_support_vector_machines.html">Week 12 Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_13_clustering.html">Week 13 Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_14_dimensionality_reduction.html">Week 14 Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_15_anomaly_detection.html">Week 15 Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_16_recommendation_systems.html">Week 16 Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_17_large_scale_machine_learning.html">Week 17 Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_18_photo_ocr.html">Week 18 Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
</body>

</html>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>