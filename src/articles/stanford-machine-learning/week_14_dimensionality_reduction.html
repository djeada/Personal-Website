<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <title>Dimensionality Reduction</title>
    <meta charset="utf-8" />
    <meta content="Principle Component Analysis (PCA) is a technique used in machine learning to reduce the dimensionality of data and improve the performance of algorithms." name="description" />
    <meta content="Adam Djellouli" name="keywords" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" type="text/css" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <link href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="ie-edge" http-equiv="X-UA-Compatible" />
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="../index.html">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul role="menu">
            <li role="menuitem"> <a href="../../index.html" title="Go to Home Page"> Home </a> </li>
            <li role="menuitem"> <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a> </li>
            <li role="menuitem"> <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a> </li>
            <li role="menuitem"> <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a> </li>
            <li role="menuitem"> <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a> </li>
            <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
        </ul>
    </nav>
    <section id="article-body"></section>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="dimensionality-reduction">Dimensionality Reduction</h2>
            <p>Principle Component Analysis (PCA) is a technique used in machine learning to reduce the dimensionality of data and improve the performance of algorithms. It works by finding a lower dimensional surface that minimizes the projection error, or the distance between each point and the projected version of the point. PCA can be used for tasks such as compression, visualization, and noise reduction. It is also useful for feature selection and can be used to improve the performance of machine learning algorithms. To use PCA, you need to compute the covariance matrix and find the eigenvectors of this matrix, then choose the first k eigenvectors and use them to calculate a new feature representation. You can choose the number of principle components by comparing the projection error to the total data variation.</p>
            <h2 id="compression">Compression</h2>
            <ul>
                <li>Speeds up algorithms.</li>
                <li>Saves space.</li>
                <li>Dimension reduction: not all features are needed.</li>
                <li>Example: different units for same attribute.</li>
            </ul>
            <p><img alt="compression_units" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/compression_units.png" /></p>
            <p>Now we can represent x1 as a 1D number (Z dimension).</p>
            <h2 id="visualization">Visualization</h2>
            <ul>
                <li>It is difficult to visualize higher dimensional data.</li>
                <li>Dimensionality reduction can help us show information in a more readable fashion for human consumption.</li>
                <li>Collect a huge data set including numerous facts about a country from around the world.</li>
            </ul>
            <p><img alt="table" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/table.png" /></p>
            <ul>
                <li>Assume each country has 50 characteristics.</li>
                <li>How can we better comprehend this data?</li>
                <li>Plotting 50-dimensional data is quite difficult.</li>
                <li>Create a new feature representation (2 z values) that summarizes these features.</li>
                <li>Reduce $50D\ -&gt;\ 2D$ (now possible to plot).</li>
            </ul>
            <h2 id="principle-component-analysis-pca-problem-formulation">Principle Component Analysis (PCA): problem formulation</h2>
            <ul>
                <li>Assume we have a 2D data collection that we want to reduce to 1D.</li>
                <li>How can we choose a single line that best fits our data?</li>
                <li>The distance between each point and the projected version should be as little as possible (blue lines below are short).</li>
                <li>PCA tries to find a lower dimensional surface so the sum of squares onto that surface is minimized.</li>
                <li>PCA tries to find the surface (a straight line in this case) which has the minimum projection error.</li>
            </ul>
            <p><img alt="pca" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/pca.png" /></p>
            <ul>
                <li>PCA is not linear regression.</li>
                <li>For linear regression, fitting a straight line to minimize the straight line between a point and a squared line. VERTICAL distance between point.</li>
                <li>For PCA minimizing the magnitude of the shortest orthogonal distance.</li>
                <li>With PCA there is no $y$ - instead we have a list of features and all features are treated equally.</li>
            </ul>
            <h2 id="pca-algorithm">PCA algorithm</h2>
            <ul>
                <li>Compute the covariance matrix.</li>
            </ul>
            <p>$$\Sigma = \frac{1}{m} \sum_{i=1}^{n} (x^{(i)})(x^{(i)})^T$$</p>
            <ul>
                <li>This is an $[n x n]$ matrix (Remember than $x^i$ is a $[n \times 1]$ matrix).</li>
                <li>Next, compute eigenvectors of matrix $\Sigma$.</li>
                <li>[U,S,V] = svd(sigma)</li>
                <li>$U$ matrix is also an $[n \times n]$ matrix. Turns out the columns of $U$ are the u vectors we want!</li>
                <li>Just take the first k-vectors from U.</li>
                <li>Next, calculate $z$. $$z = (U_{reduce})^T \cdot x$$</li>
            </ul>
            <h2 id="reconstruction-from-compressed-representation">Reconstruction from compressed representation</h2>
            <ul>
                <li>Is it possible to decompress data from a low dimensionality format to a higher dimensionality format?</li>
            </ul>
            <p>$$x_{approx} = U_{reduce} \cdot z$$</p>
            <ul>
                <li>We lose some information (everything is now precisely aligned on that line), but it is now projected into 2D space.</li>
            </ul>
            <h2 id="choosing-the-number-of-principle-components">Choosing the number of principle components</h2>
            <ul>
                <li>PCA attempts to minimize the averaged squared projection error.</li>
            </ul>
            <p>$$\frac{1}{m} \sum_{i=1}^{m} ||x^{(i)} - x_{approx}^{(i)}||^2$$</p>
            <ul>
                <li>Total data variation may be defined as the average over data indicating how distant the training instances are from the origin.</li>
            </ul>
            <p>$$\frac{1}{m} \sum_{i=1}^{m} ||x^{(i)}||^2$$</p>
            <ul>
                <li>To determine k, we may use the following formula:</li>
            </ul>
            <p>$$
                \frac{\frac{1}{m} \sum_{i=1}^{m} ||x^{(i)} - x_{approx}^{(i)}||^2}
                {\frac{1}{m} \sum_{i=1}^{m} ||x^{(i)}||^2}
                \leq 0.01
                $$</p>
            <h2 id="applications-of-pca">Applications of PCA</h2>
            <ul>
                <li>Compression: Reduce the amount of memory/disk space required to hold data.</li>
                <li>Visualization: k=2 or k=3 for plotting.</li>
                <li>A poor application of PCA is to avoid over-fitting. PCA discards certain data without understanding what values it is discarding.</li>
                <li>Examine how a system works without PCA first, and then apply PCA only if you have reason to believe it will help.</li>
            </ul>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#dimensionality-reduction">Dimensionality Reduction</a></li>
                <li><a href="#compression">Compression</a></li>
                <li><a href="#visualization">Visualization</a></li>
                <li><a href="#principle-component-analysis-pca-problem-formulation">Principle Component Analysis (PCA): problem formulation</a></li>
                <li><a href="#pca-algorithm">PCA algorithm</a></li>
                <li><a href="#reconstruction-from-compressed-representation">Reconstruction from compressed representation</a></li>
                <li><a href="#choosing-the-number-of-principle-components">Choosing the number of principle components</a></li>
                <li><a href="#applications-of-pca">Applications of PCA</a></li>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_01_introduction_to_machine_learning.html">Week 01 Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_02_linear_regression.html">Week 02 Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_03_review_of_linear_algebra.html">Week 03 Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_04_linear_regression_multiple_variables.html">Week 04 Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_06_logistic_regression.html">Week 06 Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_07_regularization.html">Week 07 Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_08_neural_networks_representation.html">Week 08 Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_09_neural_networks_learning.html">Week 09 Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_10_applying_machine_learning_advice.html">Week 10 Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_11_machine_learning_system_design.html">Week 11 Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_12_support_vector_machines.html">Week 12 Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_13_clustering.html">Week 13 Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_14_dimensionality_reduction.html">Week 14 Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_15_anomaly_detection.html">Week 15 Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_16_recommendation_systems.html">Week 16 Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_17_large_scale_machine_learning.html">Week 17 Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_18_photo_ocr.html">Week 18 Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
</body>

</html>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>