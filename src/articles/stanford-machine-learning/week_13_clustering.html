<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <title>Clustering in Unsupervised Learning</title>
    <meta charset="utf-8" />
    <meta content="Unsupervised learning, a core component of machine learning, focuses on discerning the inherent structure of data without any labeled examples." name="description" />
    <meta content="Adam Djellouli" name="keywords" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" type="text/css" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <link href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="ie-edge" http-equiv="X-UA-Compatible" />
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul role="menu">
            <li role="menuitem"> <a href="../../index.html" title="Go to Home Page"> Home </a> </li>
            <li role="menuitem"> <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a> </li>
            <li role="menuitem"> <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a> </li>
            <li role="menuitem"> <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a> </li>
            <li role="menuitem"> <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a> </li>
            <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
        </ul>
    </nav>
    <section id="article-body"></section>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="clustering-in-unsupervised-learning">Clustering in Unsupervised Learning</h2>
            <p>Unsupervised learning, a core component of machine learning, focuses on discerning the inherent structure of data without any labeled examples. Clustering, a pivotal task in unsupervised learning, aims to organize data into meaningful groups or clusters. A quintessential algorithm for clustering is the K-means algorithm.</p>
            <h3 id="overview-of-clustering">Overview of Clustering</h3>
            <ol>
                <li><strong>Objective</strong>: To uncover hidden patterns and structures in unlabeled data.</li>
                <li><strong>Applications</strong>:</li>
                <li>Market segmentation, categorizing clients based on varying characteristics.</li>
                <li>Social network analysis to understand relationship patterns.</li>
                <li>Organizing computer clusters and data centers based on network structure and location.</li>
                <li>Analyzing astronomical data for insights into galaxy formation.</li>
            </ol>
            <h3 id="the-k-means-algorithm">The K-means Algorithm</h3>
            <p>K-means is a widely-used algorithm for clustering, celebrated for its simplicity and efficacy. It works as follows:</p>
            <ol>
                <li><strong>Initialization</strong>:</li>
                <li>
                    <p>Randomly select <code>k</code> points as initial centroids.</p>
                </li>
                <li>
                    <p><strong>Assignment Step</strong>:</p>
                </li>
                <li>
                    <p>Assign each data point to the nearest centroid, forming <code>k</code> clusters.</p>
                    <p><img alt="Initial Clustering Step" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/kclusters_1.png" /></p>
                </li>
                <li>
                    <p><strong>Update Step</strong>:</p>
                </li>
                <li>
                    <p>Update each centroid to the average position of all points assigned to it.</p>
                    <p><img alt="Updating Centroids" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/kclusters_2.png" /></p>
                </li>
                <li>
                    <p><strong>Iteration</strong>:</p>
                </li>
                <li>Repeat the assignment and update steps until convergence is reached.</li>
            </ol>
            <h3 id="k-means-for-non-separated-clusters">K-means for Non-separated Clusters</h3>
            <p>K-means isn't limited to datasets with well-defined clusters. It's also effective in situations where cluster boundaries are ambiguous:</p>
            <ul>
                <li><strong>Example</strong>: Segmenting t-shirt sizes (Small, Medium, Large) even when there aren't clear separations in a dataset. The algorithm creates clusters based on the distribution of data points.</li>
            </ul>
            <p><img alt="T-shirt Size Clustering" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/t_shirt.png" /></p>
            <ul>
                <li><strong>Market Segmentation</strong>: This approach can be likened to market segmentation, where the goal is to tailor products to suit different subpopulations, even if clear divisions among these groups are not initially apparent.</li>
            </ul>
            <h3 id="optimization-objective">Optimization Objective</h3>
            <p>The goal of the K-means algorithm is to minimize the total sum of squared distances between data points and their respective cluster centroids. This objective leads to the formation of clusters that are as compact and distinct as possible given the chosen value of <code>k</code>.</p>
            <h3 id="tracking-variables-in-k-means">Tracking Variables in K-means</h3>
            <p>During the K-means process, two sets of variables are pivotal:</p>
            <ol>
                <li><strong>Cluster Assignment ($c^i$)</strong>: This denotes the cluster index (ranging from 1 to K) to which the data point $x^i$ is currently assigned.</li>
                <li><strong>Centroid Location ($\mu_k$)</strong>: This represents the location of centroid $k$.</li>
                <li><strong>Centroid of Assigned Cluster ($\mu_{c^{(i)}}$)</strong>: This is the centroid of the cluster to which the data point $x^i$ has been assigned.</li>
            </ol>
            <h3 id="the-optimization-objective">The Optimization Objective</h3>
            <p>The goal of K-means is formulated as minimizing the following cost function:</p>
            <p>$$J(c^{(1)}, ..., c^{(m)}, \mu_1, ..., \mu_K) = \frac{1}{m} \sum_{i=1}^{m} ||x^{(i)} - \mu_{c^{(i)}}||^2$$</p>
            <p>This function calculates the average of the squared distances from each data point to the centroid of its assigned cluster.</p>
            <p><img alt="Cost Function Visualization" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/cost_cluster.png" /></p>
            <h3 id="algorithm-steps-and-optimization">Algorithm Steps and Optimization</h3>
            <p>K-means consists of two steps, each minimizing the cost function in different respects:</p>
            <ol>
                <li><strong>Cluster Assignment</strong>: Minimizes $J(...)$ with respect to $c_1, c_2, ..., c_i$. This step finds the nearest centroid for each example, without altering the centroids.</li>
                <li><strong>Move Centroid</strong>: Minimizes $J(...)$ with respect to $\mu_k$. This step updates each centroid to the average position of all points assigned to it.</li>
            </ol>
            <p>These steps are iterated until the algorithm converges, ensuring both parts of the algorithm work together to minimize the cost function.</p>
            <h3 id="random-initialization">Random Initialization</h3>
            <p>K-means can converge to different solutions based on the initial placement of centroids.</p>
            <p><img alt="Different Convergence Outcomes" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/optimum_cluster.png" /></p>
            <ul>
                <li>The algorithm is typically initialized randomly.</li>
                <li>It is run multiple times (say, 100 times) with different initializations.</li>
                <li>The clustering configuration with the lowest distortion (cost function value) at convergence is selected.</li>
            </ul>
            <h3 id="the-elbow-method">The Elbow Method</h3>
            <p>Choosing the optimal number of clusters (K) is a challenge in K-means. The Elbow Method is a heuristic used to determine this:</p>
            <ul>
                <li>The cost function $J(...)$ is computed for a range of values of K.</li>
                <li>The cost typically decreases as K increases. The objective is to find a balance between the number of clusters and the minimization of the cost function.</li>
                <li>The "elbow" point in the plot of K vs $J(...)$ is considered a good choice for K. This is where the rate of decrease sharply changes.</li>
            </ul>
            <p><img alt="Elbow Method Graph" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/elbow.png" /></p>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol><a href="#clustering-in-unsupervised-learning">Clustering in Unsupervised Learning</a>
                <ol>
                    <li><a href="#overview-of-clustering">Overview of Clustering</a></li>
                    <li><a href="#the-k-means-algorithm">The K-means Algorithm</a></li>
                    <li><a href="#k-means-for-non-separated-clusters">K-means for Non-separated Clusters</a></li>
                    <li><a href="#optimization-objective">Optimization Objective</a></li>
                    <li><a href="#tracking-variables-in-k-means">Tracking Variables in K-means</a></li>
                    <li><a href="#the-optimization-objective">The Optimization Objective</a></li>
                    <li><a href="#algorithm-steps-and-optimization">Algorithm Steps and Optimization</a></li>
                    <li><a href="#random-initialization">Random Initialization</a></li>
                    <li><a href="#the-elbow-method">The Elbow Method</a></li>
                </ol>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_01_introduction_to_machine_learning.html">Week 01 Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_02_linear_regression.html">Week 02 Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_03_review_of_linear_algebra.html">Week 03 Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_04_linear_regression_multiple_variables.html">Week 04 Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_06_logistic_regression.html">Week 06 Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_07_regularization.html">Week 07 Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_08_neural_networks_representation.html">Week 08 Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_09_neural_networks_learning.html">Week 09 Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_10_applying_machine_learning_advice.html">Week 10 Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_11_machine_learning_system_design.html">Week 11 Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_12_support_vector_machines.html">Week 12 Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_13_clustering.html">Week 13 Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_14_dimensionality_reduction.html">Week 14 Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_15_anomaly_detection.html">Week 15 Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_16_recommendation_systems.html">Week 16 Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_17_large_scale_machine_learning.html">Week 17 Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_18_photo_ocr.html">Week 18 Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
</body>

</html>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>