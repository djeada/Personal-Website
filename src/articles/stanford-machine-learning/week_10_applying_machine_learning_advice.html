<!DOCTYPE html>

<html lang="en">

<head>
    <meta content="Advice for applying machine learning techniques" name="description" />
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <title>Advice for applying machine learning techniques</title>
    <meta charset="utf-8" />
    <meta content="XXX" name="description" />
    <meta content="Adam Djellouli" name="keywords" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" type="text/css" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <link href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="ie-edge" http-equiv="X-UA-Compatible" />
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="../index.html">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul role="menu">
            <li role="menuitem"> <a href="../../index.html" title="Go to Home Page"> Home </a> </li>
            <li role="menuitem"> <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a> </li>
            <li role="menuitem"> <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a> </li>
            <li role="menuitem"> <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a> </li>
            <li role="menuitem"> <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a> </li>
            <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
        </ul>
    </nav>
    <section id="article-body"></section>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="advice-for-applying-machine-learning-techniques">Advice for applying machine learning techniques</h2>
            <p>If you're having trouble with your machine learning model producing high errors when tested on new data, there are several steps you can take to troubleshoot the problem. You can try adding more training data or features, or you can try changing the value of the regularization parameter. You can also split your data into a training set and a test set to evaluate the model's performance. Another option is to use a technique called "model selection" and create a training, validation, and test set to identify the best performing model. If your model is underperforming, it may be due to either high bias (underfitting) or high variance (overfitting). You can diagnose the issue by plotting the error for both the training and validation set as a function of the polynomial degree. If all else fails, you can try using an advanced optimization algorithm to minimize the cost function and improve the performance of your model.</p>
            <h2 id="debugging-a-learning-algorithm">Debugging a learning algorithm</h2>
            <p>Imagine you've used regularized linear regression to forecast home prices:</p>
            <p>$$J(\theta) = \frac{1}{2m} [ \sum_{i=1}^{m}(h_{\theta}(x^{(i)} + y^{(i)})^2 + \lambda \sum_{j=1}^{m} \theta_j^2] $$</p>
            <ul>
                <li>Trained it.</li>
                <li>However, when tested on new data, it produces unacceptably high errors in its predictions.</li>
                <li>What should your next step be? <ul>
                        <li>Obtain additional training data.</li>
                        <li>Try a smaller set of features.</li>
                        <li>Consider getting more features.</li>
                        <li>Add polynomial features.</li>
                        <li>Change the value of $\lambda$.</li>
                    </ul>
                </li>
            </ul>
            <h2 id="evaluating-the-hypothesis">Evaluating the hypothesis</h2>
            <ul>
                <li>Split data into two portions: training set and test set.</li>
                <li>Learn parameters $\theta$ from training data, minimizing $J(\theta)$ using 70\% of the training data.</li>
                <li>Compute the test error.</li>
            </ul>
            <p>$$J_{test}(\theta) = \frac{1}{2m_{test}} \sum_{i=1}^{m_{test}}(h_{\theta}(x^{(i)}<em>{test} + y^{(i)}</em>{test})^2$$</p>
            <h2 id="model-selection-and-training-validation-test-sets">Model selection and training validation test sets</h2>
            <ul>
                <li>How should a regularization parameter or polynomial degree be chosen?</li>
                <li>We've previously discussed the issue of overfitting.</li>
                <li>This is why, in general, training set error is a poor predictor of hypothesis accuracy for new data (generalization).</li>
                <li>
                    <p>Try to determine the degree of polynomial that will fit data.</p>
                    <ol>
                        <li>
                            <p>$h_{\theta}(x) = \theta_0 + \theta_1x$</p>
                        </li>
                        <li>
                            <p>$h_{\theta}(x) = \theta_0 + \theta_1x + \theta_2x^2$</p>
                        </li>
                        <li>
                            <p>$h_{\theta}(x) = \theta_0 + ... + \theta_3x^3$</p>
                        </li>
                    </ol>
                    <p>$$\vdots$$</p>
                    <ol>
                        <li>$h_{\theta}(x) = \theta_0 + ... + \theta_{10}x^{10}$</li>
                    </ol>
                </li>
                <li>
                    <p>Introduce a new parameter d, which represents the degree of polynomial you want to use.</p>
                </li>
                <li>Model 1 is minimized using training data, resulting in a parameter vector $\theta^1$ (where d =1).</li>
                <li>Same goes for other models up to $n$.</li>
                <li>Using the previous formula, examine the test set error for each computed parameter $J_{test}(\theta^k)$.</li>
                <li>Minimize cost function for each of the models as before.</li>
                <li>Test these hypothesis on the cross validation set to generate the cross validation error.</li>
                <li>Pick the hypothesis with the lowest cross validation error.</li>
            </ul>
            <p>Training error:</p>
            <p>$$J_{train}(\theta) = \frac{1}{2m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)} + y^{(i)})^2$$</p>
            <p>Cross Validation error:</p>
            <p>$$J_{cv}(\theta) = \frac{1}{2m_{cv}} \sum_{i=1}^{m_{cv}}(h_{\theta}(x^{(i)}<em>{cv} + y^{(i)}</em>{cv})^2$$</p>
            <p>Test error:</p>
            <p>$$J_{test}(\theta) = \frac{1}{2m_{test}} \sum_{i=1}^{m_{test}}(h_{\theta}(x^{(i)}<em>{test} + y^{(i)}</em>{test})^2$$</p>
            <h2 id="model-selection-and-training-validation-test-sets">Model selection and training validation test sets</h2>
            <p>Bad results are generally the consequence of one of the following:</p>
            <ul>
                <li>High bias - under fitting problem.</li>
                <li>High variance - over fitting problem.</li>
            </ul>
            <p><img alt="diagnosis" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/diagnosis.png" /></p>
            <p>Now plot</p>
            <ul>
                <li>$x$ = degree of polynomial d</li>
                <li>$y$ = error for both training and cross validation (two lines)</li>
            </ul>
            <p><img alt="error_vs_d" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/error_vs_d.png" /></p>
            <ul>
                <li>For the high bias case, we find both cross validation and training error are high</li>
                <li>For high variance, we find the cross validation error is high but training error is low</li>
            </ul>
            <h2 id="regularization-and-bias-variance">Regularization and bias/variance</h2>
            <p>Linear regression with regularization:</p>
            <p>$$h_{\theta}(x) = \theta_0 + \theta_1x + \theta_2x^2 + \theta_3x^3 + \theta_4x^4$$</p>
            <p>$$J(\theta) = \frac{1}{2m} [ \sum_{i=1}^{m}(h_{\theta}(x^{(i)} + y^{(i)})^2 + \lambda \sum_{j=1}^{m} \theta_j^2]$$</p>
            <p>The above equation describes the fitting of a high order polynomial with regularization (used to keep parameter values small).</p>
            <ul>
                <li>$\lambda$ is large (high bias $-&gt;$ under fitting data)</li>
                <li>$\lambda$ is intermediate (good)</li>
                <li>$\lambda$ is small (high variance $-&gt;$ overfitting)</li>
            </ul>
            <p><img alt="lambda" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/lambda.png" /></p>
            <ul>
                <li>Have a set or range of values to use (for example from 0 to 15).</li>
                <li>For each $\lambda_i$ minimize the cost function. Result is $\theta^{(i)}$.</li>
                <li>For each $\theta^{(i)}$ measure average squared error on cross validation set.</li>
                <li>Pick the model which gives the lowest error.</li>
            </ul>
            <h2 id="learning-curves">Learning curves</h2>
            <p>Plot $J_{train}$ (average squared error on training set) and $J_{cv}$ (average squared error on cross validation set) against m (number of training examples).</p>
            <ul>
                <li>$J_{train}$ on smaller sample sizes is smaller (as less variance to accommodate).</li>
                <li>As training set grows your hypothesis generalize better and $J_{cv}$ gets smaller.</li>
            </ul>
            <p><img alt="learning_curve" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/learning_curve.png" /></p>
            <ul>
                <li>A small gap between training error and cross validation error might indicate high bias. Here, more data will not help.</li>
                <li>A large gap between training error and cross validation error might indicate high variance. Here, more data will probably help.</li>
            </ul>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#advice-for-applying-machine-learning-techniques">Advice for applying machine learning techniques</a></li>
                <li><a href="#debugging-a-learning-algorithm">Debugging a learning algorithm</a></li>
                <li><a href="#evaluating-the-hypothesis">Evaluating the hypothesis</a></li>
                <li><a href="#model-selection-and-training-validation-test-sets">Model selection and training validation test sets</a></li>
                <li><a href="#model-selection-and-training-validation-test-sets">Model selection and training validation test sets</a></li>
                <li><a href="#regularization-and-bias-variance">Regularization and bias/variance</a></li>
                <li><a href="#learning-curves">Learning curves</a></li>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_01_introduction_to_machine_learning.html">Week 01 Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_02_linear_regression.html">Week 02 Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_03_review_of_linear_algebra.html">Week 03 Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_04_linear_regression_multiple_variables.html">Week 04 Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_06_logistic_regression.html">Week 06 Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_07_regularization.html">Week 07 Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_08_neural_networks_representation.html">Week 08 Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_09_neural_networks_learning.html">Week 09 Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_10_applying_machine_learning_advice.html">Week 10 Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_11_machine_learning_system_design.html">Week 11 Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_12_support_vector_machines.html">Week 12 Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_13_clustering.html">Week 13 Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_14_dimensionality_reduction.html">Week 14 Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_15_anomaly_detection.html">Week 15 Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_16_recommendation_systems.html">Week 16 Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_17_large_scale_machine_learning.html">Week 17 Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_18_photo_ocr.html">Week 18 Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/addjellouli/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
</body>

</html>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>