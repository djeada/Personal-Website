<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <title>Support Vector Machines</title>
    <meta charset="utf-8" />
    <meta content="In this article, we explore how to use logistic regression cost functions to create support vector machine (SVM) cost functions." name="description" />
    <meta content="Adam Djellouli" name="keywords" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" type="text/css" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <link href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="ie-edge" http-equiv="X-UA-Compatible" />
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="../index.html">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul role="menu">
            <li role="menuitem"> <a href="../../index.html" title="Go to Home Page"> Home </a> </li>
            <li role="menuitem"> <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a> </li>
            <li role="menuitem"> <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a> </li>
            <li role="menuitem"> <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a> </li>
            <li role="menuitem"> <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a> </li>
            <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
        </ul>
    </nav>
    <section id="article-body"></section>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="support-vector-machines">Support Vector Machines</h2>
            <p>In this article, we explore how to use logistic regression cost functions to create support vector machine (SVM) cost functions. We start by introducing logistic regression cost functions and how they can be used to make predictions about data. Then, we show how the SVM cost functions are derived from the logistic regression cost functions by replacing the logistic regression terms with $cost_1(\theta^Tx)$ and $cost_0(\theta^Tx)$. We also explain how adjusting the value of C in the SVM cost function can impact the bias and variance of the resulting hypothesis, with larger values of C leading to higher variance and lower bias, and smaller values leading to lower variance and higher bias. Finally, we discuss the concept of large margin classification, which involves trying to find a hypothesis that separates the classes as widely as possible.</p>
            <h2 id="an-alternative-view-of-logistic-regression">An alternative view of logistic regression</h2>
            <p>As previously stated, the logistic regression hypothesis is as follows:</p>
            <p>$$h_{\theta}(x) = \frac{1}{1+e^{-\theta^Tx}}$$</p>
            <p>We have an example in which $y = 1$. We expect that $h_{\theta}(x)$ is close to 1.</p>
            <p><img alt="sigmoid" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/sigmoid2.png" /></p>
            <p>When you look at the cost function, you'll see that each example contributes a term like the one below to the total cost function.</p>
            <p>$$-(ylogh_{\theta}(x)+(1-y)log(1-h_{\theta}(x)))$$</p>
            <p>After plugging in the hypothesis function $h_{\theta}(x)$, you obtain an enlarged cost function equation:</p>
            <p>$$-ylog\frac{1}{1+e^{-\theta^Tx}}-(1-y)log(1-\frac{1}{1+e^{-\theta^Tx}})$$</p>
            <p><img alt="log_function" src="https://user-images.githubusercontent.com/37275728/201519577-c93854b4-1270-4082-9d9b-da0d543b0375.png" /></p>
            <ul>
                <li>As a result, if z is large, the cost is small.</li>
                <li>If z is 0 or negative, however, the cost contribution is large..</li>
                <li>This is why, when logistic regression encounters a positive case, it attempts to make $\theta^Tx$ a very big term.</li>
            </ul>
            <h2 id="svm-cost-functions-from-logistic-regression-cost-functions">SVM cost functions from logistic regression cost functions</h2>
            <ul>
                <li>Instead of a curved line, draw two straight lines (magenta) to approximate the logistic regression y = 1 function.</li>
                <li>Flat when cost is 0.</li>
                <li>Straight growing line after 1.</li>
                <li>So this is the new y=1 cost function, which provides the SVM with a computational advantage and makes optimization easier.</li>
            </ul>
            <p><img alt="svm_cost" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/svm_cost.png" /></p>
            <p>Logistic regression cost function:</p>
            <p>$$J(\theta) = -\frac{1}{m} \sum_{i=1}^{m}[ y^{(i)} log h_{\theta}(x^{(i)}) + (1- y^{(i)})log(1 - h_{\theta}(x^{(i)}))] + \frac{\lambda}{2m} \sum_{j=1}^{m} \theta_j^2$$</p>
            <p>For the SVM we take our two logistic regression $y=1$ and $y=0$ terms described previously and replace with $cost_1(\theta^Tx)$ and $cost_0(\theta^Tx)$.</p>
            <p>$$J(\theta) = -\frac{1}{m} \sum_{i=1}^{m}[ y^{(i)} cost_1(\theta^Tx^{(i)}) + (1- y^{(i)}) cost_0(\theta^Tx^{(i)})] + \frac{\lambda}{2m} \sum_{j=1}^{m} \theta_j^2$$</p>
            <p>Which can be rewritten as:</p>
            <p>$$J(\theta) = C \sum_{i=1}^{m}[ y^{(i)} cost_1(\theta^Tx^{(i)}) + (1- y^{(i)}) cost_0(\theta^Tx^{(i)})] + \frac{1}{2} \sum_{j=1}^{m} \theta_j^2$$</p>
            <ul>
                <li>Large C gives a hypothesis of low bias high variance $-&gt;$ overfitting</li>
                <li>Small C gives a hypothesis of high bias low variance $-&gt;$ underfitting</li>
            </ul>
            <h2 id="large-margin-intuition">Large margin intuition</h2>
            <ul>
                <li>So, given that we're aiming to minimize CA + B.</li>
                <li>Consider the following scenario: we set C to be really large.</li>
                <li>If C is large, we will choose an A value such that A equals zero.</li>
                <li>If y = 1, then we must find a value of $\theta$ so that $\theta^Tx$ is larger than or equal to 1 in order to make our "A" term 0.</li>
                <li>If y = 0, then we must find a value of $\theta$ so that $\theta^Tx$ is equal to or less than -1 in order to make our "A" term 0.</li>
                <li>So we're minimizing B, under the constraints shown below:</li>
            </ul>
            <p>$$min\ \frac{1}{2} \sum_{j=1}^{m} \theta_j^2$$</p>
            <p>$$\theta^Tx^{(i)} \geq 1 \quad if\ y^{(i)}=1$$
                $$\theta^Tx^{(i)} \leq 1 \quad if\ y^{(i)}=0$$</p>
            <p><img alt="large_dist" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/large_dist.png" /></p>
            <ul>
                <li>The green and magenta lines represent functional decision limits that might be selected using logistic regression. However, they are unlikely to generalize effectively.</li>
                <li>The black line, on the other hand, is the one picked by the SVM as a result of the optimization graph's safety net. Stronger separator.</li>
                <li>That black line has a greater minimum distance (margin) than any of the training samples.</li>
            </ul>
            <h2 id="svm-decision-boundary">SVM decision boundary</h2>
            <p>Assume we only have two features and $\theta_0=0$. Then we can rewrite th expression for minimizing B as follows:</p>
            <p>$$\frac{1}{2}(\theta_1^2 + \theta_2^2) =\frac{1}{2}(\sqrt{\theta_1^2 + \theta_2^2})^2 = \frac{1}{2}||\theta||^2$$</p>
            <ul>
                <li>Given this, what are $\theta^Tx$ parameters doing?</li>
                <li>Assume we have just one positive training example (red cross below).</li>
                <li>Assume we have our parameter vector and plot it on the same axis.</li>
                <li>The following question asks what the inner product of these two vectors is.</li>
            </ul>
            <p><img alt="svm_vectors" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/svm_vectors.png" /></p>
            <p>$p$, is in fact $p^i$, because it's the length of $p$ for example $i$.</p>
            <p>$$\theta^Tx^{(i)} = p^i \cdot ||\theta||$$</p>
            <p>$$min\ \frac{1}{2} \sum_{j=1}^{m} \theta_j^2 = \frac{1}{2} ||\theta||^2$$</p>
            <p>$$p^{(i)} \cdot ||\theta|| \geq 1 \quad if\ y^{(i)}=1$$
                $$p^{(i)} \cdot ||\theta|| \leq 1 \quad if\ y^{(i)}=0$$</p>
            <h2 id="adapting-svm-to-non-linear-classifiers">Adapting SVM to non-linear classifiers</h2>
            <ul>
                <li>We have a training set.</li>
                <li>We want to find a non-linear boundary.</li>
            </ul>
            <p><img alt="non_linear_boundary" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/non_linear_boundary.png" /></p>
            <ul>
                <li>Define three features in this example (ignore $x_0$).</li>
                <li>Have a graph of $x_1$ vs. $x_2$ (don't plot the values, just define the space).</li>
                <li>Pick three points.</li>
            </ul>
            <p><img alt="landmarks" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/landmarks.png" /></p>
            <ul>
                <li>These points $l^1$, $l^2$, and $l^3$, were chosen manually and are called landmarks.</li>
                <li>Kernel is the name given to the similarity function between $(x, l^i)$.</li>
            </ul>
            <p>$$f_1 = k(X, l^1) = exp(- \frac{||x-l^{(1)}||^2}{2\sigma^2})$$</p>
            <ul>
                <li>Large $\sigma^2$ - $f$ features vary more smoothly - higher bias, lower variance.</li>
                <li>Small $\sigma^2$ - $f$ features vary abruptly - low bias, high variance.</li>
                <li>With training examples x we predict "1" when: $\theta_0+\theta_1f_1+\theta_2f_2+\theta_3f_3 \geq 0$</li>
                <li>Let's say that: $\theta_0 = -0.5,\ \theta_1=1,\ \theta_2=1,\ \theta_3=0$</li>
                <li>Given our placement of three examples, what happens if we evaluate an example at the magenta dot below?</li>
            </ul>
            <p><img alt="landmarks_magneta" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/landmarks_magneta.png" /></p>
            <ul>
                <li>We can see from our formula that f1 will be close to 1, whereas f2 and f3 will be close to 0.</li>
                <li>We have: $-0.5+1\cdot1+0\cdot1+0\cdot0 \geq 0$.</li>
                <li>The inequality holds. We predict 1.</li>
                <li>If we had another point far away from all three. The inequality wouldn't hold. As a result, we would predict 0.</li>
            </ul>
            <h2 id="choosing-the-landmarks">Choosing the landmarks</h2>
            <ul>
                <li>Take the training data. Vectors X and Y, both with m elements.</li>
                <li>As a result, you'll wind up having m landmarks. Each training example has one landmark per location.</li>
                <li>So we just cycle over each landmark, determining how close $x^i$ is to that landmark. Here we are using the kernel function.</li>
                <li>Take these m features $(f_1, f_2 ... f_m)$ group them into an $[m +1 \times 1]$ dimensional vector called $f$.</li>
            </ul>
            <h2 id="kernels">Kernels</h2>
            <ul>
                <li>Linear kernel: no kernel, no $f$ vector. Predict $y=1$ if $(\theta^Tx) \geq 0$.</li>
                <li>Not all similarity functions you develop are valid kernels. Must satisfy Merecer's Theorem.</li>
                <li>Polynomial kernel.</li>
                <li>String kernel.</li>
                <li>Chi-squared kernel.</li>
                <li>Histogram intersection kernel.</li>
            </ul>
            <h2 id="logistic-regression-vs-svm">Logistic regression vs. SVM</h2>
            <ul>
                <li>Use logistic regression or SVM with a linear kernel if n (features) is much greater than m (training set).</li>
                <li>If n is small and m is intermediate, the Gaussian kernel is suitable.</li>
                <li>With a Gaussian kernel, SVM will be sluggish if n is small and m is large. Use logistic regression or SVM with a linear kernel.</li>
                <li>A lot of SVM's power is using diferent kernels to learn complex non-linear functions.</li>
                <li>Because SVM is a convex optimization problem, it gives a global minimum.</li>
            </ul>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#support-vector-machines">Support Vector Machines</a></li>
                <li><a href="#an-alternative-view-of-logistic-regression">An alternative view of logistic regression</a></li>
                <li><a href="#svm-cost-functions-from-logistic-regression-cost-functions">SVM cost functions from logistic regression cost functions</a></li>
                <li><a href="#large-margin-intuition">Large margin intuition</a></li>
                <li><a href="#svm-decision-boundary">SVM decision boundary</a></li>
                <li><a href="#adapting-svm-to-non-linear-classifiers">Adapting SVM to non-linear classifiers</a></li>
                <li><a href="#choosing-the-landmarks">Choosing the landmarks</a></li>
                <li><a href="#kernels">Kernels</a></li>
                <li><a href="#logistic-regression-vs-svm">Logistic regression vs. SVM</a></li>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_01_introduction_to_machine_learning.html">Week 01 Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_02_linear_regression.html">Week 02 Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_03_review_of_linear_algebra.html">Week 03 Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_04_linear_regression_multiple_variables.html">Week 04 Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_06_logistic_regression.html">Week 06 Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_07_regularization.html">Week 07 Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_08_neural_networks_representation.html">Week 08 Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_09_neural_networks_learning.html">Week 09 Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_10_applying_machine_learning_advice.html">Week 10 Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_11_machine_learning_system_design.html">Week 11 Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_12_support_vector_machines.html">Week 12 Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_13_clustering.html">Week 13 Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_14_dimensionality_reduction.html">Week 14 Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_15_anomaly_detection.html">Week 15 Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_16_recommendation_systems.html">Week 16 Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_17_large_scale_machine_learning.html">Week 17 Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_18_photo_ocr.html">Week 18 Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
</body>

</html>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>