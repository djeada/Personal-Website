<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <title>Recommendation Systems</title>
    <meta charset="utf-8" />
    <meta content="Recommendation systems are a fundamental component in the interface between users and large-scale content providers like Amazon, eBay, and iTunes." name="description" />
    <meta content="Adam Djellouli" name="keywords" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../resources/style.css" rel="stylesheet" type="text/css" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <link href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="ie-edge" http-equiv="X-UA-Compatible" />
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul role="menu">
            <li role="menuitem"> <a href="../../index.html" title="Go to Home Page"> Home </a> </li>
            <li role="menuitem"> <a class="active" href="../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a> </li>
            <li role="menuitem"> <a href="../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a> </li>
            <li role="menuitem"> <a href="../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a> </li>
            <li role="menuitem"> <a href="../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a> </li>
            <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
        </ul>
    </nav>
    <section id="article-body"></section>
    <div id="article-wrapper">
        <section id="article-body">
            <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
            <h2 id="recommendation-systems">Recommendation Systems</h2>
            <p>Recommendation systems are a fundamental component in the interface between users and large-scale content providers like Amazon, eBay, and iTunes. These systems personalize user experiences by suggesting products, movies, or content based on past interactions and preferences.</p>
            <h3 id="concept-of-recommender-systems">Concept of Recommender Systems</h3>
            <ul>
                <li><strong>Purpose</strong>: To recommend new content or products to users based on their historical interactions or preferences.</li>
                <li><strong>Nature</strong>: Not a single algorithm, but a category of methods tailored to predicting user preferences.</li>
            </ul>
            <h3 id="example-scenario-movie-rating-prediction">Example Scenario: Movie Rating Prediction</h3>
            <p>Imagine a company that sells movies and allows users to rate them. Consider the following setup:</p>
            <ul>
                <li><strong>Movies</strong>: A catalog of five movies.</li>
                <li><strong>Users</strong>: A database of four users.</li>
                <li><strong>User Ratings</strong>: Users rate movies on a scale of 1 to 5.</li>
            </ul>
            <p><img alt="Movie Recommendation Example" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/movie_recommendation.png" /></p>
            <p>Notations:</p>
            <ul>
                <li>$n_u$: Number of users.</li>
                <li>$n_m$: Number of movies.</li>
                <li>$r(i, j)$: Indicator (1 or 0) if user $j$ has rated movie $i$.</li>
                <li>$y^{(i, j)}$: Rating given by user $j$ to movie $i$.</li>
            </ul>
            <h3 id="feature-vectors-and-parameter-vectors">Feature Vectors and Parameter Vectors</h3>
            <ul>
                <li><strong>Movie Features</strong>: Each movie can have a feature vector representing various attributes or genres.</li>
                <li><strong>Additional Feature (x0 = 1)</strong>: For computational convenience, an additional feature is added to each movie's feature vector.</li>
            </ul>
            <p>Example for "Cute Puppies of Love":</p>
            <p>$$
                x^{(3)} = \begin{bmatrix}
                1 \\
                0.99 \\
                0
                \end{bmatrix}
                $$</p>
            <ul>
                <li><strong>User Parameters</strong>: Each user has a parameter vector representing their preferences.</li>
            </ul>
            <p>Example for user 1 (Alice) and her preference for "Cute Puppies of Love":</p>
            <p>$$
                \theta^{(1)} = \begin{bmatrix}
                0 \\
                5 \\
                0
                \end{bmatrix}
                $$</p>
            <h3 id="making-predictions">Making Predictions</h3>
            <p>To predict how much Alice might like "Cute Puppies of Love", we compute:</p>
            <p>$$
                (\theta^{(1)})^T x^{(3)} = (0 \cdot 1) + (5 \cdot 0.99) + (0 \cdot 0) = 4.95
                $$</p>
            <p>This predicts a high rating of 4.95 out of 5 for Alice for this movie, based on her preference parameters and the movie's features.</p>
            <h3 id="collaborative-filtering">Collaborative Filtering</h3>
            <ul>
                <li><strong>Method</strong>: Collaborative filtering is often used in recommender systems. It involves learning either user preferences or item features, depending on the data available.</li>
                <li><strong>Learning</strong>: This can be achieved using algorithms like gradient descent.</li>
                <li><strong>Advantage</strong>: The system can learn to make recommendations on its own without explicit programming of the features or preferences.</li>
            </ul>
            <h2 id="learning-user-preferences-in-recommendation-systems">Learning User Preferences in Recommendation Systems</h2>
            <p>In recommendation systems, learning user preferences $(\theta^j)$ is a key component. This process is akin to linear regression, but with a unique approach tailored for recommendation contexts.</p>
            <h3 id="minimizing-cost-function-for-user-preferences">Minimizing Cost Function for User Preferences</h3>
            <ul>
                <li>The objective is to minimize the following cost function for each user $j$:</li>
            </ul>
            <p>$$
                \min_{\theta^j} = \frac{1}{2m^{(j)}} \sum_{i:r(i,j)=1} \left((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)}\right)^2 + \frac{\lambda}{2m^{(j)}} \sum_{k=1}^{n} (\theta_k^{(j)})^2
                $$</p>
            <ul>
                <li>Here, $m^{(j)}$ is the number of movies rated by user $j$, $\lambda$ is the regularization parameter, and $r(i, j)$ indicates whether user $j$ has rated movie $i$.</li>
            </ul>
            <h3 id="gradient-descent-for-optimization">Gradient Descent for Optimization</h3>
            <p>Update Rule for $\theta_k^{(j)}$:</p>
            <ul>
                <li>For $k = 0$ (bias term):</li>
            </ul>
            <p>$$
                \theta_k^{(j)} := \theta_k^{(j)} - \alpha \sum_{i:r(i,j)=1} \left((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)}\right) x_k^{(i)}
                $$</p>
            <ul>
                <li>For $k \neq 0$:</li>
            </ul>
            <p>$$
                \theta_k^{(j)} := \theta_k^{(j)} - \alpha \left(\sum_{i:r(i,j)=1} \left((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)}\right) x_k^{(i)} + \lambda \theta_k^{(j)}\right)
                $$</p>
            <h3 id="collaborative-filtering-algorithm">Collaborative Filtering Algorithm</h3>
            <p>Collaborative filtering leverages the interplay between user preferences and item (movie) features:</p>
            <ul>
                <li><strong>Learning Features from Preferences</strong>: Given user preferences $(\theta^{(1)}, ..., \theta^{(n_u)})$, the algorithm can learn movie features $(x^{(1)}, ..., x^{(n_m)})$.</li>
                <li><strong>Cost Function for Movies</strong>:</li>
            </ul>
            <p>$$
                \min_{x^{(1)}, ..., x^{(n_m)}} \frac{1}{2} \sum_{i=1}^{n_m} \sum_{i:r(i,j)=1} \left((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)}\right)^2 + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2
                $$</p>
            <ul>
                <li><strong>Iterative Process</strong>: The algorithm typically involves an iterative process, alternating between optimizing for movie features and user preferences.</li>
            </ul>
            <h3 id="vectorization-low-rank-matrix-factorization">Vectorization: Low Rank Matrix Factorization</h3>
            <ul>
                <li><strong>Matrix Y</strong>: Organize all user ratings into a matrix Y, which represents the interactions between users and movies.</li>
            </ul>
            <p>Example $[5 \times 4]$ Matrix Y for 5 movies and 4 users:</p>
            <p>$$
                Y = \begin{pmatrix}
                5 &amp; 5 &amp; 0 &amp; 0 \\
                5 &amp; ? &amp; ? &amp; 0 \\
                ? &amp; 4 &amp; 0 &amp; ? \\
                0 &amp; 0 &amp; 5 &amp; 4 \\
                0 &amp; 0 &amp; 5 &amp; 0
                \end{pmatrix}
                $$</p>
            <ul>
                <li><strong>Predicted Ratings Matrix</strong>: The predicted ratings can be expressed as the product of matrices $X$ and $\Theta^T$.</li>
            </ul>
            <p>$$
                X \Theta^T = \begin{pmatrix}
                (\theta^{(1)})^T(x^{(1)}) &amp; \dots &amp; (\theta^{(n_u)})^T(x^{(1)}) \\
                \vdots &amp; \ddots &amp; \vdots \\
                (\theta^{(1)})^T(x^{(n_m)}) &amp; \dots &amp; (\theta^{(n_u)})^T(x^{(n_m)})
                \end{pmatrix}
                $$</p>
            <ul>
                <li><strong>Matrix X</strong>: Contains the features for each movie, stacked in rows.</li>
                <li><strong>Matrix $\Theta$</strong>: Contains the user preferences, also stacked in rows.</li>
            </ul>
            <h3 id="scenario-a-user-with-no-ratings">Scenario: A User with No Ratings</h3>
            <p>Imagine a user in a movie recommendation system who hasn't rated any movies. This scenario presents a challenge for typical collaborative filtering algorithms.</p>
            <p><img alt="User with No Ratings" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/no_ratings.png" /></p>
            <ul>
                <li>For such a user, there are no movies for which $r(i, j) = 1$.</li>
                <li>As a result, the algorithm ends up minimizing only the regularization term for this user, which doesn't provide meaningful insight for recommendations.</li>
            </ul>
            <h3 id="mean-normalization-approach">Mean Normalization Approach</h3>
            <ol>
                <li><strong>Compute the Mean Rating</strong>: Calculate the average rating for each movie and store these in a vector $\mu$.</li>
            </ol>
            <p>Example $\mu$ vector for a system with 5 movies:</p>
            <p>$$
                \mu = \begin{bmatrix}
                2.5 \\
                2.5 \\
                2 \\
                2.25 \\
                1.25
                \end{bmatrix}
                $$</p>
            <ol>
                <li><strong>Normalize the Ratings Matrix</strong>: Subtract the mean rating for each movie from all its ratings in the matrix $Y$.</li>
            </ol>
            <p>Normalized Ratings Matrix $Y$:</p>
            <p>$$
                Y = \begin{pmatrix}
                2.5 &amp; 2.5 &amp; -2.5 &amp; -2.5 &amp; ? \\
                2.5 &amp; ? &amp; ? &amp; -2.5 &amp; ? \\
                ? &amp; 2 &amp; -2 &amp; ? &amp; ? \\
                -2.25 &amp; -2.25 &amp; 2.75 &amp; 1.75 &amp; ? \\
                -1.25 &amp; -1.25 &amp; 3.75 &amp; -1.25 &amp; ?
                \end{pmatrix}
                $$</p>
            <ol>
                <li><strong>Adjust for Users with No Ratings</strong>: For a new user with no ratings, their predicted rating for each movie can be initialized to the mean rating of that movie. This provides a baseline from which personalized recommendations can evolve as the user starts rating movies.</li>
            </ol>
            <h3 id="benefits-of-mean-normalization">Benefits of Mean Normalization</h3>
            <ul>
                <li><strong>Handling New Users</strong>: Provides a starting point for recommendations for users who have not yet provided any ratings.</li>
                <li><strong>Rating Sparsity</strong>: Mitigates issues arising from sparse data, which is common in many real-world recommendation systems.</li>
                <li><strong>Balancing the Dataset</strong>: Normalization helps in balancing the dataset, especially when there are variations in the number of ratings per movie.</li>
            </ul>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#recommendation-systems">Recommendation Systems</a>
                    <ol>
                        <li><a href="#concept-of-recommender-systems">Concept of Recommender Systems</a></li>
                        <li><a href="#example-scenario-movie-rating-prediction">Example Scenario: Movie Rating Prediction</a></li>
                        <li><a href="#feature-vectors-and-parameter-vectors">Feature Vectors and Parameter Vectors</a></li>
                        <li><a href="#making-predictions">Making Predictions</a></li>
                        <li><a href="#collaborative-filtering">Collaborative Filtering</a></li>
                    </ol>
                </li>
                <li><a href="#learning-user-preferences-in-recommendation-systems">Learning User Preferences in Recommendation Systems</a>
                    <ol>
                        <li><a href="#minimizing-cost-function-for-user-preferences">Minimizing Cost Function for User Preferences</a></li>
                        <li><a href="#gradient-descent-for-optimization">Gradient Descent for Optimization</a></li>
                        <li><a href="#collaborative-filtering-algorithm">Collaborative Filtering Algorithm</a></li>
                        <li><a href="#vectorization-low-rank-matrix-factorization">Vectorization: Low Rank Matrix Factorization</a></li>
                        <li><a href="#scenario-a-user-with-no-ratings">Scenario: A User with No Ratings</a></li>
                        <li><a href="#mean-normalization-approach">Mean Normalization Approach</a></li>
                        <li><a href="#benefits-of-mean-normalization">Benefits of Mean Normalization</a></li>
                    </ol>
                </li>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_01_introduction_to_machine_learning.html">Week 01 Introduction to Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_02_linear_regression.html">Week 02 Linear Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_03_review_of_linear_algebra.html">Week 03 Review of Linear Algebra</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_04_linear_regression_multiple_variables.html">Week 04 Linear Regression Multiple Variables</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_06_logistic_regression.html">Week 06 Logistic Regression</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_07_regularization.html">Week 07 Regularization</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_08_neural_networks_representation.html">Week 08 Neural Networks Representation</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_09_neural_networks_learning.html">Week 09 Neural Networks Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_10_applying_machine_learning_advice.html">Week 10 Applying Machine Learning Advice</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_11_machine_learning_system_design.html">Week 11 Machine Learning System Design</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_12_support_vector_machines.html">Week 12 Support Vector Machines</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_13_clustering.html">Week 13 Clustering</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_14_dimensionality_reduction.html">Week 14 Dimensionality Reduction</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_15_anomaly_detection.html">Week 15 Anomaly Detection</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_16_recommendation_systems.html">Week 16 Recommendation Systems</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_17_large_scale_machine_learning.html">Week 17 Large Scale Machine Learning</a></li>
                    <li><a href="https://adamdjellouli.com/articles/stanford-machine-learning/week_18_photo_ocr.html">Week 18 Photo Ocr</a></li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../app.js"></script>
    </footer>
</body>

</html>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>