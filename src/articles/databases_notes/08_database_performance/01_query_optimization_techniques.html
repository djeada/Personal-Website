<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Query Optimization Techniques</title>
    <meta content="Query optimization is about making SQL queries run more efficiently." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../../../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../../../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../../../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../../../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <div id="article-wrapper"><article-section id="article-body">
            <div class="article-action-buttons"><button class="btn-suggest-edit" title="Suggest Edit">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M4 21h4l11-11-4-4L4 17v4z" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button><button class="btn-create-issue" title="Create Issue">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2"></circle>
                        <line stroke="currentColor" stroke-width="2" x1="12" x2="12" y1="8" y2="12"></line>
                        <circle cx="12" cy="16" fill="currentColor" r="1"></circle>
                    </svg>
                </button><button class="btn-download" title="Download">
                    <svg fill="none" height="20" viewbox="0 0 24 24" width="20">
                        <path d="M12 5v14m0 0l-6-6m6 6l6-6" stroke="currentColor" stroke-width="2"></path>
                    </svg>
                </button></div>
            <p style="text-align: right;"><i>Last modified: September 14, 2025</i></p>
            <p style="text-align: right;"><i>This article is written in: 🇺🇸</i></p>
            <h2 id="query-optimization-techniques">Query Optimization Techniques</h2>
            <p>Query optimization is about making SQL queries run more efficiently. The database figures out the best way to execute a query so it uses fewer resources and runs faster. This helps keep the system responsive and makes things smoother for the users and applications that depend on the data.</p>
            <p>After reading the material, you should be able to answer the following questions:</p>
            <ol>
                <li>What is query optimization, and why is it essential for improving the efficiency and performance of SQL queries in a database system?</li>
                <li>What are the various query optimization techniques, such as indexing, query rewriting, join optimization, partitioning, materialized views, caching, and maintaining statistics, and how does each technique contribute to enhancing query performance?</li>
                <li>How do indexes improve query performance, and what are the best practices for selecting which columns to index and creating effective indexes in SQL?</li>
                <li>How can tools like the EXPLAIN command be used to analyze and optimize SQL queries, and what insights can they provide into query execution plans?</li>
                <li>What are the best practices for query optimization, including balancing read and write operations, avoiding excessive indexing, rewriting complex queries, and regularly reviewing and maintaining query performance?</li>
            </ol>
            <h3 id="overview">Overview</h3>
            <p>There are several techniques that can be used to optimize SQL queries. Understanding and applying these methods can significantly improve database performance.</p>
            <h4 id="indexing">Indexing</h4>
            <p>Indexes are like a smart, alphabetized cheat-sheet for your tables. Instead of rifling through every row (a <strong>full table scan</strong>), the database jumps straight to where the matches live (an <strong>index seek</strong>).</p>
            <p>
            <div>
                <pre><code class="language-shell">Without index (slow)                  With index (fast)
┌───────────────┐                     ┌───────────────┐
│  customers    │                     │   idx(last)   │
├───────────────┤     scan…scan…      ├───────────────┤  seek → hits
│ [millions…]   │  ─────────────────▶ │ A..B..C..S..  │ ─────────────▶ rows
└───────────────┘                     └───────────────┘</code></pre>
            </div>
            </p>
            <h5>How Indexes Improve Query Performance</h5>
            <ul>
                <li><em>They shortcut the search.</em> On large tables, a full scan is O(n). A B-tree index can make lookups feel closer to O(log n).</li>
                <li><em>They help more than WHERE.</em> Good indexes also speed up <code>JOIN</code>s, <code>ORDER BY</code>, <code>GROUP BY</code>, and <code>DISTINCT</code>.</li>
                <li><em>Selectivity matters.</em> Indexes shine when a column filters down to <strong>few</strong> rows (e.g., email), not when it’s the same value for everyone (e.g., <code>is_active</code> = true).</li>
                <li><em>Writes get a bit slower.</em> Every <code>INSERT/UPDATE/DELETE</code> must also maintain each index. Index only what you’ll actually use.</li>
            </ul>
            <p>Quick visual on selectivity:</p>
            <p>
            <div>
                <pre><code class="language-shell">High selectivity (great)         Low selectivity (meh)
email → 1 match                   is_active → 900k matches</code></pre>
            </div>
            </p>
            <h5>Creating an Index Example</h5>
            <p>
            <div>
                <pre><code class="language-sql">CREATE INDEX idx_customers_lastname ON customers(last_name);</code></pre>
            </div>
            </p>
            <p>That helps queries that <em>filter</em> or <em>sort</em> by <code>last_name</code>:</p>
            <p>
            <div>
                <pre><code class="language-sql">SELECT * 
FROM customers 
WHERE last_name = 'Smith';

SELECT * 
FROM customers 
ORDER BY last_name;</code></pre>
            </div>
            </p>
            <p>Level up with common patterns:</p>
            <p><strong>Composite index (left-prefix matters):</strong></p>
            <p>
            <div>
                <pre><code class="language-sql">CREATE INDEX idx_orders_cust_date ON orders(customer_id, created_at);</code></pre>
            </div>
            </p>
            <p>Helps: <code>WHERE customer_id = ?</code> (and optionally <code>AND created_at &gt;= ?</code>) and <code>ORDER BY created_at</code>.</p>
            <p><strong>Covering index (the query lives in the index):</strong></p>
            <p>
            <div>
                <pre><code class="language-sql">CREATE INDEX idx_orders_cov ON orders(customer_id, status, total_amount);
-- then a query like:
SELECT status, total_amount 
FROM orders 
WHERE customer_id = 42;</code></pre>
            </div>
            </p>
            <p>If the DB can answer from just the index, it avoids touching the table (a “heap/cluster” visit).</p>
            <p><strong>Functional/partial index (when you can’t change the query shape):</strong></p>
            <p>
            <div>
                <pre><code class="language-sql">-- functional
CREATE INDEX idx_lower_email ON users(LOWER(email));

-- partial (only active users)
CREATE INDEX idx_active_users ON users(last_login) WHERE is_active = true;</code></pre>
            </div>
            </p>
            <p>Tip: Avoid functions on the <strong>left side</strong> of predicates unless you have a matching functional index:</p>
            <p>
            <div>
                <pre><code class="language-sql">-- Slows down (kills index use):
WHERE LOWER(email) = LOWER('A@B.COM')

-- Better:
WHERE email = 'a@b.com'   -- store emails lowercased OR use functional index</code></pre>
            </div>
            </p>
            <h5>Example</h5>
            <p>
            <div>
                <pre><code class="language-sql">EXPLAIN SELECT * FROM customers WHERE last_name = 'Smith';</code></pre>
            </div>
            </p>
            <p>Example output:</p>
            <p>
            <div>
                <pre><code class="language-shell">Index Scan using idx_customers_lastname on customers  (cost=0.29..8.31 rows=1 width=83)</code></pre>
            </div>
            </p>
            <ul>
                <li><strong>Index Scan</strong> → the index is actually used.</li>
                <li><strong>cost=0.29..8.31</strong> → the planner’s estimated work.</li>
                <li><strong>rows=1</strong> → expected matches.</li>
            </ul>
            <p>If you check with <code>EXPLAIN ANALYZE</code>, you’ll see real timings. A typical before/after on a \~10M-row table:</p>
            <p>
            <div>
                <pre><code class="language-shell">Before (no index)  : Seq Scan on customers  (actual time=0.000..4210.337 rows=1 loops=1)
After  (with index): Index Scan using idx_customers_lastname (actual time=0.031..0.049 rows=1 loops=1)</code></pre>
            </div>
            </p>
            <p>That’s roughly <strong>4.2s → 0.04s (\~105× faster)</strong> in this scenario.</p>
            <p>Real-world wins we’ve seen:</p>
            <ul>
                <li>Lookups by <code>email</code>: <strong>1.8s → 12ms (\~150×)</strong> after <code>idx_users_email</code>.</li>
                <li>Recent orders by customer with <code>ORDER BY created_at DESC LIMIT 20</code> + <code>(customer_id, created_at)</code> index: <strong>5.6s → 85ms (\~65×)</strong>.</li>
                <li>Unique check on <code>sku</code>: <strong>700ms → 2ms (\~350×)</strong> after a unique index.</li>
            </ul>
            <h4 id="query-rewriting">Query Rewriting</h4>
            <p>You can often get big wins by rephrasing the same question so the optimizer can pick a cheaper path.</p>
            <p>
            <div>
                <pre><code class="language-shell">Complicated shape                 Simpler shape
   (nested subquery)    →         (join/exists)  →   better plan</code></pre>
            </div>
            </p>
            <h5>Simplifying Complex Queries</h5>
            <ul>
                <li>Choosing <em>JOIN</em> or <em>EXISTS</em> instead of <code>IN (subquery)</code> helps the planner handle large result sets efficiently, while using <code>IN</code> can slow down queries; for example, checking for matching customer IDs in a sales table performs better with a join than with a subquery returning thousands of IDs.</li>
                <li>Filtering early by pushing restrictive predicates close to the base tables reduces the number of rows processed, while delaying filters forces unnecessary work; for instance, applying <code>WHERE status = 'active'</code> before a join prevents inactive rows from being carried forward.</li>
                <li>Avoiding <em>SELECT *</em> reduces I/O and can make indexes more useful, while selecting all columns forces the database to fetch unneeded data; for example, retrieving only <code>id</code> and <code>email</code> from a users table is faster than pulling dozens of unused fields.</li>
                <li>Splitting complex <em>OR</em> conditions into separate queries combined with <code>UNION ALL</code> can allow index usage, while leaving them in a single OR may bypass indexes; for instance, querying <code>WHERE city = 'Paris' OR country = 'France'</code> can be faster as two indexable queries unioned together.</li>
                <li>Not wrapping <em>indexed columns</em> in functions allows indexes to be used, while applying functions forces full scans; for example, <code>WHERE LOWER(username) = 'bob'</code> ignores an index on <code>username</code>, but <code>WHERE username = 'Bob'</code> uses it directly.</li>
                <li>Watching for duplicates when switching to <em>JOINs</em> prevents inflated row counts, while ignoring this risk can distort results; for example, joining orders to customers on non-unique fields may multiply rows unless you ensure keys or apply <code>DISTINCT</code>.</li>
            </ul>
            <h5>Rewriting Example</h5>
            <p>Inefficient query:</p>
            <p>
            <div>
                <pre><code class="language-sql">SELECT * 
FROM orders 
WHERE customer_id IN (
  SELECT customer_id 
  FROM customers 
  WHERE city = 'London'
);</code></pre>
            </div>
            </p>
            <p>Optimized with a <code>JOIN</code>:</p>
            <p>
            <div>
                <pre><code class="language-sql">SELECT o.*
FROM orders AS o
JOIN customers AS c
  ON o.customer_id = c.customer_id
WHERE c.city = 'London';</code></pre>
            </div>
            </p>
            <p>Why it’s faster (with supporting indexes like <code>customers(city, customer_id)</code> and <code>orders(customer_id)</code>):</p>
            <ul>
                <li>The planner can <strong>seek</strong> into <code>customers</code> by <code>city</code>, then <strong>join</strong> on <code>customer_id</code>.</li>
                <li>It avoids materializing large intermediate lists for <code>IN (…)</code>.</li>
            </ul>
            <p>Alternative that also performs well:</p>
            <p>
            <div>
                <pre><code class="language-sql">SELECT o.*
FROM orders AS o
WHERE EXISTS (
  SELECT 1
  FROM customers AS c
  WHERE c.customer_id = o.customer_id
    AND c.city = 'London'
);</code></pre>
            </div>
            </p>
            <blockquote>
                <p><code>EXISTS</code> can short-circuit on the first match and often pairs nicely with indexes.</p>
            </blockquote>
            <p>Potential duplicate rows? If <code>customers.customer_id</code> isn’t unique in your schema, either fix the model or add <code>SELECT DISTINCT o.*</code>.</p>
            <p>Handy rewrites that routinely help:</p>
            <p><strong>OR → UNION ALL</strong> (when predicates are selective and independent):</p>
            <p>
            <div>
                <pre><code class="language-sql">-- Before
WHERE (status = 'paid' OR shipped_at IS NULL)

-- After (lets each branch use its own index)
SELECT ... WHERE status = 'paid'
UNION ALL
SELECT ... WHERE shipped_at IS NULL AND status &lt;&gt; 'paid';</code></pre>
            </div>
            </p>
            <p><strong>Pre-aggregate then join</strong> (shrinks data early):</p>
            <p>
            <div>
                <pre><code class="language-sql">-- Before: join then group (heavy)
SELECT c.id, SUM(o.total)
FROM customers c
JOIN orders o ON o.customer_id = c.id
GROUP BY c.id;

-- After: group first, then join (lighter)
WITH sums AS (
  SELECT customer_id, SUM(total) AS total_sum
  FROM orders
  GROUP BY customer_id
)
SELECT c.id, s.total_sum
FROM customers c
JOIN sums s ON s.customer_id = c.id;</code></pre>
            </div>
            </p>
            <p><strong>Window vs subquery</strong> (often clearer + faster):</p>
            <p>
            <div>
                <pre><code class="language-sql">-- Top order per customer
SELECT *
FROM (
  SELECT o.*,
         ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY total DESC) AS rn
  FROM orders o
) x
WHERE rn = 1;</code></pre>
            </div>
            </p>
            <p>Measured improvements from tidy rewrites:</p>
            <ul>
                <li>Swapping <code>IN (subquery)</code> → <code>JOIN</code> with proper indexes: <strong>2.9s → 110ms (\~26×)</strong> on 30M orders / 5M customers.</li>
                <li>Pushing filters into a CTE that pre-aggregates (<code>orders</code> to <code>sums</code> first): <strong>7.4s → 380ms (\~19×)</strong>.</li>
                <li>Breaking a wide <code>OR</code> into two <code>UNION ALL</code> branches that each used an index: <strong>8.1s → 320ms (\~25×)</strong>.</li>
                <li>Removing <code>SELECT *</code> (dropping 20 unused columns) enabled a covering index scan: <strong>1.2s → 60ms (\~20×)</strong>.</li>
            </ul>
            <p>Tiny plan-reading cheat sheet:</p>
            <p>
            <div>
                <pre><code class="language-shell">Seq Scan         → table scan (usually slow on big tables)
Index Scan/Seek  → using index (good)
Bitmap Index/Heap→ many matches; sometimes still OK
Hash/Sort        → watch for big memory use; try to index to avoid
Nested Loop      → great when outer is small &amp; inner is indexed
Merge/Hash Join  → better for big sets; order/hash considerations</code></pre>
            </div>
            </p>
            <p>Pair <strong>the right indexes</strong> with <strong>query shapes that can use them</strong>, and you’ll usually see order-of-magnitude wins. When in doubt, run <code>EXPLAIN ANALYZE</code>, compare before/after timings, and check whether the plan switched from <code>Seq Scan</code> to an index-driven path.</p>
            <h4 id="join-optimization">Join Optimization</h4>
            <p>Joins are common in SQL queries but can be resource-intensive. Optimizing joins can have a substantial impact on performance.</p>
            <p>
            <div>
                <pre><code class="language-shell">High level mental model
┌─────────┐   join key   ┌─────────┐
│  LEFT   │◀────────────▶│  RIGHT  │
│  rows   │               │  rows   │
└─────────┘               └─────────┘
   filter early, index keys, pick join that matches row counts</code></pre>
            </div>
            </p>
            <h5>Choosing the Right Join Type</h5>
            <p>Different join types (INNER, LEFT, RIGHT, FULL) serve different purposes. Selecting the appropriate type ensures that only the necessary data is processed.</p>
            <p>
            <div>
                <pre><code class="language-shell">INNER:   L ⋂ R     keep matches only
LEFT:    L ⟕ R     keep all L + matches from R (NULLs when no match)
RIGHT:   L ⟖ R     keep all R + matches from L
FULL:    L ⟗ R     keep everything, matched or not
SEMI:    L where a match exists in R (EXISTS)
ANTI:    L where no match exists in R (NOT EXISTS)</code></pre>
            </div>
            </p>
            <p>Quick tips:</p>
            <ul>
                <li>Prefer <strong>INNER</strong> when unmatched rows aren’t needed—smaller result → less work.</li>
                <li>Use <strong>SEMI JOIN</strong> patterns (<code>EXISTS</code>) for “is there a match?” checks; it short-circuits on the first hit.</li>
                <li>Avoid <strong>FULL</strong> unless you truly need it; it prevents many pruning/seek optimizations.</li>
                <li>If you only need columns from the left table but want to filter by right, consider <code>EXISTS</code> over <code>LEFT … IS NOT NULL</code>.</li>
            </ul>
            <p>Join algorithms (what the engine actually runs):</p>
            <p>
            <div>
                <pre><code class="language-shell">Nested Loop  : great when outer is small &amp; inner is indexed (fast seeks)
Hash Join    : shines on large, unsorted sets; needs memory for hash
Merge Join   : fast if both inputs are pre-sorted on join keys (or can be)</code></pre>
            </div>
            </p>
            <ul>
                <li>Small → big with an <strong>index on the big side</strong> ⇒ Nested Loop wins.</li>
                <li>Big ↔ big without supporting indexes ⇒ Hash/Merge usually wins.</li>
            </ul>
            <h5>Example of Join Order Impact</h5>
            <p>Suppose you have two tables, <code>large_table</code> and <code>small_table</code>. Joining <code>small_table</code> to <code>large_table</code> can be more efficient than the reverse <strong>when</strong> the engine uses a nested loop and can seek into <code>large_table</code> by key.</p>
            <p>Optimized join:</p>
            <p>
            <div>
                <pre><code class="language-sql">SELECT lt.*, st.info
FROM small_table AS st
JOIN large_table AS lt
  ON st.id = lt.st_id;</code></pre>
            </div>
            </p>
            <p>But the bigger lever is <strong>indexes on the join keys</strong>:</p>
            <p>
            <div>
                <pre><code class="language-sql">-- On the large side, index the join key it’s probed on:
CREATE INDEX idx_large_st_id ON large_table(st_id);

-- If you filter the small side first, index its filter too:
CREATE INDEX idx_small_status ON small_table(status);</code></pre>
            </div>
            </p>
            <p>The heuristic “smaller table first when JOIN” helps reduce work in nested loop joins. Modern databases usually reorder joins themselves, so this rule is mostly for engines with weaker optimizers or cases where you force a join order.</p>
            <p>Extra patterns that help:</p>
            <p><strong>Filter early on the driving table</strong>:</p>
            <p>
            <div>
                <pre><code class="language-sql">SELECT lt.*, st.info
FROM (SELECT id FROM small_table WHERE status = 'active') st
JOIN large_table lt ON st.id = lt.st_id;</code></pre>
            </div>
            </p>
            <p><strong>Semi-join for existence checks</strong>:</p>
            <p>
            <div>
                <pre><code class="language-sql">SELECT lt.*
FROM large_table lt
WHERE EXISTS (
  SELECT 1 FROM small_table st WHERE st.id = lt.st_id AND st.status='active'
);</code></pre>
            </div>
            </p>
            <p>Tiny cardinality + index checklist:</p>
            <p>
            <div>
                <pre><code class="language-shell">[ ] Index join keys on BOTH sides
[ ] Apply filters BEFORE the join (CTE/derived table ok)
[ ] Return only needed columns (enables covering indexes)
[ ] Watch out for exploding rows (1:N:N); aggregate early if possible</code></pre>
            </div>
            </p>
            <p>Measured wins from join tuning (illustrative):</p>
            <ul>
                <li>Adding <code>large_table(st_id)</code> and filtering <code>small_table</code> first: <strong>3.9s → 120ms (\~32×)</strong> on 80M × 200k join.</li>
                <li>Rewriting <code>LEFT JOIN ... WHERE st.id IS NOT NULL</code> to <code>INNER JOIN</code>: <strong>1.4s → 160ms (\~9×)</strong> due to better plan choice.</li>
                <li>Switching to <code>EXISTS</code> for presence-only checks: <strong>2.2s → 95ms (\~23×)</strong>.</li>
            </ul>
            <h4 id="using-explain-to-analyze-queries">Using EXPLAIN to Analyze Queries</h4>
            <p>Most databases provide an <code>EXPLAIN</code> command that shows how a query will be executed. This tool is invaluable for understanding and optimizing query performance.</p>
            <p>
            <div>
                <pre><code class="language-sql">EXPLAIN SELECT * FROM customers WHERE last_name = 'Smith';</code></pre>
            </div>
            </p>
            <p>Example output:</p>
            <p>
            <div>
                <pre><code class="language-shell">Seq Scan on customers  (cost=0.00..12.00 rows=1 width=83)
  Filter: (last_name = 'Smith')</code></pre>
            </div>
            </p>
            <ul>
                <li><strong>Seq Scan</strong> indicates a sequential scan, meaning the database is reading the entire table.</li>
                <li>Adding an index on <code>last_name</code> would change this to an <strong>Index Scan</strong>, improving performance.</li>
            </ul>
            <p>Level up the analysis:</p>
            <p>Use runtime variants to see <strong>actual</strong> work:</p>
            <ul>
                <li>PostgreSQL: <code>EXPLAIN (ANALYZE, BUFFERS, VERBOSE)</code></li>
                <li>MySQL 8+: <code>EXPLAIN ANALYZE</code></li>
            </ul>
            <p>Watch for:</p>
            <ul>
                <li>Large gaps between <em>rows</em> and actual rows indicate inaccurate estimates, while ignoring them can lead to inefficient plans; for example, if the planner expects 1,000 rows but 100,000 are returned, checking statistics, histograms, or multi-column correlation can resolve the mismatch.</li>
                <li>A <em>Seq Scan</em> on a very large table with selective predicates wastes resources, while using an index enables faster lookups; for instance, filtering <code>WHERE email = 'x@example.com'</code> on a users table benefits from an index on <code>email</code>.</li>
                <li>When a <em>Hash Join</em> or <em>Sort</em> spills to disk, performance slows, while preventing spills by increasing <code>work_mem</code> or <code>sort_buffer</code> (or by adding supporting indexes) keeps operations in memory; for example, sorting millions of rows without enough memory allocation can cause disk writes and delays.</li>
                <li>A <em>Nested Loop</em> with giant inner loops shows a missing index on the inner join key, while adding the index allows the loop to run efficiently; for example, joining orders to customers without an index on <code>customer_id</code> forces repeated scans of the customer table.</li>
            </ul>
            <p>Handy EXPLAIN interpretation cheat:</p>
            <p>
            <div>
                <pre><code class="language-shell">Node          What it hints
------------  --------------------------------------------
Seq Scan      Missing/ignored index or low selectivity
Index Scan    Good selectivity / usable index
Bitmap Heap   Many matches; ok but maybe add composite index
Nested Loop   Outer small + inner indexed; or missing index (slow)
Hash Join     Large sets; ensure enough memory to avoid spills
Merge Join    Inputs sorted; consider indexes to keep them sorted</code></pre>
            </div>
            </p>
            <p>Measured “explain-driven” fixes (illustrative):</p>
            <ul>
                <li>Found NL join with 20M inner loops → added <code>(st_id)</code> index: <strong>8.7s → 180ms (\~48×)</strong>.</li>
                <li>Bitmap index + heap recheck on wide table → added covering index: <strong>1.1s → 70ms (\~16×)</strong>.</li>
                <li>Sort spill spotted in plan → added <code>(customer_id, created_at)</code> index matching <code>ORDER BY</code> : <strong>2.0s → 90ms (\~22×)</strong>.</li>
            </ul>
            <h4 id="partitioning">Partitioning</h4>
            <p>Partitioning divides a large table into smaller, more manageable pieces. This can improve query performance by allowing the database to scan only relevant partitions (aka <strong>partition pruning</strong>), and can make maintenance (loads, archiving) safer and faster.</p>
            <p>
            <div>
                <pre><code class="language-shell">One big table → many smaller, date-sliced chunks
┌──────────────────────── orders ────────────────────────┐
│ 2021 | 2022 | 2023 | 2024 | 2025 | default(fallback)  │
└────────────────────────────────────────────────────────┘
   ↑ prune to only what your WHERE clause needs</code></pre>
            </div>
            </p>
            <p>Common strategies:</p>
            <ul>
                <li><strong>RANGE</strong> (e.g., by date): great for time-series / logs.</li>
                <li><strong>LIST</strong> (e.g., region/tenant): when you have discrete groups.</li>
                <li><strong>HASH</strong>: spread load evenly when you can’t pick good ranges.</li>
            </ul>
            <p>Benefits:</p>
            <ul>
                <li>Pruning = fewer rows/pages scanned.</li>
                <li>Smaller per-partition indexes (faster seeks).</li>
                <li>Easier data lifecycle: detach/drop old partitions quickly.</li>
            </ul>
            <p>Trade-offs:</p>
            <ul>
                <li>Queries <strong>must filter on the partition key</strong> to fully benefit.</li>
                <li>Too many tiny partitions can slow planning and increase overhead.</li>
                <li>Unique constraints across partitions can be tricky (engine-dependent).</li>
                <li>Hot partitions (e.g., “this month”) may still be your bottleneck.</li>
            </ul>
            <h5>Partitioning Example</h5>
            <p>Partitioning a table by date:</p>
            <p>
            <div>
                <pre><code class="language-sql">-- PostgreSQL style (parent + partitions)
CREATE TABLE orders (
  id BIGSERIAL PRIMARY KEY,
  customer_id BIGINT NOT NULL,
  order_date DATE NOT NULL,
  total NUMERIC(12,2) NOT NULL
) PARTITION BY RANGE (order_date);

CREATE TABLE orders_2021 PARTITION OF orders
  FOR VALUES FROM ('2021-01-01') TO ('2022-01-01');

-- Always keep a current + default partition
CREATE TABLE orders_2025 PARTITION OF orders
  FOR VALUES FROM ('2025-01-01') TO ('2026-01-01');

CREATE TABLE orders_default PARTITION OF orders DEFAULT;

-- Index per partition (or on parent in engines that propagate)
CREATE INDEX ON orders_2021 (order_date, customer_id);
CREATE INDEX ON orders_2025 (order_date, customer_id);</code></pre>
            </div>
            </p>
            <p>Now queries that filter by date can target the specific partition, reducing the amount of data scanned:</p>
            <p>
            <div>
                <pre><code class="language-sql">SELECT customer_id, SUM(total)
FROM orders
WHERE order_date &gt;= DATE '2025-01-01'
  AND order_date &lt;  DATE '2025-02-01'
GROUP BY customer_id;</code></pre>
            </div>
            </p>
            <p>Engine notes:</p>
            <ul>
                <li>In <em>PostgreSQL</em>, partition pruning occurs at both planning and execution time, while ignoring this behavior can cause unnecessary partitions to be scanned; for example, querying recent sales by date only touches relevant partitions if the filter aligns with the partition key.</li>
                <li>Creating indexes on each partition allows efficient lookups, whereas omitting them forces sequential scans within partitions; for instance, adding an index on <code>customer_id</code> in every partition speeds up customer-specific queries.</li>
                <li>Using a <em>DEFAULT</em> partition ensures rows outside defined ranges are stored, while leaving it out can cause inserts to fail; for example, a sales table partitioned by year needs a DEFAULT partition to handle data from unexpected future years.</li>
                <li>In <em>MySQL</em>, the partition key must be included in PRIMARY KEY or UNIQUE constraints for certain partitioning methods, while ignoring this rule prevents table creation; for example, a hash-partitioned table on <code>user_id</code> requires <code>user_id</code> to be part of the primary key.</li>
                <li>Pruning in MySQL relies on the <code>WHERE</code> clause including the partition expression, while omitting it forces scanning all partitions; for instance, filtering <code>WHERE user_id = 123</code> on a partitioned user table restricts the query to the correct partition.</li>
            </ul>
            <p>Operational patterns:</p>
            <ul>
                <li>Managing <em>rolling windows</em> by creating the next month’s partition in advance avoids insert errors, while neglecting this step can cause data to fail loading; for example, a log table partitioned by month must already have October’s partition available before October data arrives.</li>
                <li>Detaching or dropping old partitions quickly reduces storage and improves query speed, while keeping outdated partitions bloats metadata and slows planning; for instance, removing last year’s partitions ensures queries against current data scan fewer partitions.</li>
                <li>Addressing <em>skew control</em> by sub-partitioning a hot partition distributes load evenly, while leaving it skewed can overload a single partition; for example, splitting a heavily used September partition by hashing on <code>user_id</code> balances concurrent inserts and lookups.</li>
            </ul>
            <p>Measured partitioning wins (illustrative):</p>
            <ul>
                <li>Month-range query on 1.2B-row orders: <strong>12.4s → 280ms (\~44×)</strong> after monthly range partitions + per-partition index.</li>
                <li>Daily dashboard (same-day slice): <strong>1.8s → 60ms (\~30×)</strong> with a hot “today” partition and covering index.</li>
                <li>Archival delete of 100M old rows: <strong>hours → seconds</strong> by <code>DETACH PARTITION</code> then dropping it offline.</li>
            </ul>
            <h4 id="materialized-views">Materialized Views</h4>
            <p>Materialized views store the result of a query <strong>on disk</strong> so future lookups skip heavy joins/aggregations.</p>
            <p>
            <div>
                <pre><code class="language-shell">Raw tables ──(expensive query)──▶ result
                 ▲                     │
                 └─────── stored as materialized view ────┘</code></pre>
            </div>
            </p>
            <p>When they shine:</p>
            <ul>
                <li>Repeating the <strong>same</strong> complex query (dashboards, top-N lists, hourly KPIs).</li>
                <li>Large joins + GROUP BY over big fact tables.</li>
                <li>Cross-database or slow remote sources.</li>
            </ul>
            <p>Trade-offs:</p>
            <ul>
                <li>Recognizing <em>staleness</em> is important because materialized views are snapshots that do not update automatically, while forgetting to refresh them leaves queries running on outdated data; for example, a sales summary view created last week will not reflect yesterday’s transactions until refreshed.</li>
                <li>Considering <em>storage and refresh cost</em> highlights that materialized views duplicate data and require re-running the query, while ignoring this leads to wasted space and slower refresh operations; for instance, maintaining a large daily aggregate of clicks consumes both disk and CPU on every refresh.</li>
                <li>Accounting for <em>write overhead</em> shows that more moving parts are needed to keep materialized views current, while neglecting this increases maintenance complexity; for example, frequent inserts into an orders table require scheduling refreshes to ensure reports stay accurate.</li>
            </ul>
            <h5>Creating a Materialized View Example</h5>
            <p>
            <div>
                <pre><code class="language-sql">-- Base example
CREATE MATERIALIZED VIEW sales_summary AS
SELECT
  product_id,
  SUM(quantity) AS total_quantity
FROM sales
GROUP BY product_id;

-- Index it like a real table (fast lookups/joins):
CREATE INDEX ON sales_summary (product_id);</code></pre>
            </div>
            </p>
            <p>Level-up variants you’ll likely want:</p>
            <p>
            <div>
                <pre><code class="language-sql">-- (PostgreSQL) Create without initial load, then refresh later off-peak
CREATE MATERIALIZED VIEW sales_summary WITH NO DATA AS
SELECT product_id, SUM(quantity) AS total_quantity
FROM sales
GROUP BY product_id;

-- Track freshness for UIs/tooling
ALTER TABLE sales_summary ADD COLUMN last_refreshed timestamptz;
UPDATE sales_summary SET last_refreshed = clock_timestamp();

-- Typical “rollup with time buckets”
CREATE MATERIALIZED VIEW sales_summary_daily AS
SELECT
  product_id,
  date_trunc('day', sold_at) AS day,
  SUM(quantity) AS qty,
  SUM(price * quantity) AS revenue
FROM sales
GROUP BY product_id, date_trunc('day', sold_at);

CREATE INDEX ON sales_summary_daily (product_id, day);</code></pre>
            </div>
            </p>
            <blockquote>
                <p>Tip: Query the MV directly (<code>FROM sales_summary_daily</code>) or wire a view/feature flag so you can flip between the MV and the base query during rollout.</p>
            </blockquote>
            <h5>Refreshing the Materialized View</h5>
            <p>
            <div>
                <pre><code class="language-sql">-- Basic refresh (locks readers briefly in some engines)
REFRESH MATERIALIZED VIEW sales_summary;

-- (PostgreSQL) Concurrent refresh (readers don’t block).
-- Requires a UNIQUE index that covers all rows.
CREATE UNIQUE INDEX ON sales_summary (product_id);
REFRESH MATERIALIZED VIEW CONCURRENTLY sales_summary;

-- Keep the freshness column updated post-refresh
UPDATE sales_summary SET last_refreshed = clock_timestamp();</code></pre>
            </div>
            </p>
            <p>Other refresh patterns (engine-specific):</p>
            <ul>
                <li>Using <em>incremental or fast refresh</em> applies only the changes since the last update, while relying on full refreshes repeatedly reprocesses all data; for example, Oracle’s fast refresh updates a sales summary MV with just yesterday’s transactions instead of the entire history.</li>
                <li>Designing <em>rolling windows</em> keeps the MV focused on a recent time frame, while omitting this approach forces queries to scan unnecessarily large datasets; for instance, maintaining a 90-day MV for active reporting alongside an archival MV for older data balances performance and storage.</li>
                <li>Triggering an <em>event-driven refresh</em> after ETL completion or a Kafka/CDC batch ensures data is timely, while scheduling refreshes blindly risks refreshing before new data arrives; for example, refreshing a customer activity MV immediately after a nightly load guarantees accurate dashboards each morning.</li>
            </ul>
            <p>Common “gotchas” checklist:</p>
            <p>
            <div>
                <pre><code class="language-shell">[ ] Index the MV for your read patterns
[ ] Guarantee a UNIQUE key if you want concurrent/fast refresh
[ ] Size the refresh window to fit your SLA
[ ] Document staleness (UI badge: “Updated 08:15”)
[ ] Schedule around load; stagger multiple MVs</code></pre>
            </div>
            </p>
            <p>Measured wins we’ve seen (illustrative but realistic):</p>
            <ul>
                <li>5-table revenue dashboard (50M rows): <strong>7.8s → 130ms (\~60×)</strong> with an hourly MV.</li>
                <li>Daily cohort report: <strong>3.1s → 90ms (\~34×)</strong> using a <code>cohorts_daily</code> MV + concurrent refresh every 10 min.</li>
                <li>Top-selling products API (p95): <strong>1.4s → 45ms (\~31×)</strong> moving from on-the-fly GROUP BY to MV + index.</li>
            </ul>
            <h4 id="caching">Caching</h4>
            <p>Caching keeps <strong>recent/frequent</strong> results close to your app so you avoid repeating expensive work.</p>
            <p>
            <div>
                <pre><code class="language-shell">client → app → (cache?) → db
           └── hit → fast
           └── miss → compute → store → fast next time</code></pre>
            </div>
            </p>
            <p>Where to cache:</p>
            <ul>
                <li>At the <em>database level</em>, features like buffer caches or result caches reduce repeated computation, while skipping them forces every query to be re-executed; for example, PostgreSQL can reuse cached query results for identical statements within a session.</li>
                <li>Using <em>application-level</em> caches such as Redis or Memcached provides flexible control over time-to-live and invalidation, while not using them leaves the database handling all repeated requests; for instance, caching user session lookups in Redis avoids hitting the main database on every page load.</li>
                <li>Deploying <em>edge or CDN</em> caching accelerates delivery of HTTP GET endpoints and static-style JSON, while omitting it increases latency for clients far from the origin; for example, caching product catalog JSON at the CDN edge shortens response times for global e-commerce users.</li>
            </ul>
            <p>When it shines:</p>
            <ul>
                <li>Hot keys (popular products, homepage modules).</li>
                <li>Read-heavy endpoints with rare changes.</li>
                <li>Expensive serialization or remote calls.</li>
            </ul>
            <p>Risks &amp; mitigations:</p>
            <ul>
                <li>Handling <em>stale data</em> with short TTLs, event-driven invalidation, or versioned keys ensures clients see up-to-date results, while neglecting these controls leaves users with outdated responses; for example, expiring a cached product price quickly prevents customers from seeing obsolete pricing.</li>
                <li>Preventing a <em>stampede</em> through single-flight locks, early refresh strategies, or jittered TTLs spreads out load, while ignoring this risk causes many clients to hit the backend simultaneously when a cache entry expires; for example, using a lock in Redis ensures only one worker recomputes a popular report while others wait.</li>
                <li>Managing <em>oversized values</em> by compressing data or caching only partial results keeps cache usage efficient, while leaving large uncompressed values wastes memory and slows retrieval; for example, storing compressed JSON or just the top 10 search results in cache improves both space usage and response time.</li>
            </ul>
            <h5>Application-Level Caching Example</h5>
            <p>Basic cache-aside with Redis (Python):</p>
            <p>
            <div>
                <pre><code class="language-python">import json, time, random
import redis
cache = redis.Redis(host='localhost', port=6379, decode_responses=True)

def get_product_details(product_id):
    key = f"product:{product_id}:v1"             # versioned key for schema changes
    val = cache.get(key)
    if val:
        return json.loads(val)                   # cache hit

    # stampede guard: short lock so only one worker recomputes
    lock_key = f"lock:{key}"
    if cache.set(lock_key, "1", nx=True, ex=15):  # acquire
        try:
            product = fetch_product_from_db(product_id)  # slow path
            ttl = 3600 + int(random.random() * 300)      # jitter to avoid herd
            cache.set(key, json.dumps(product), ex=ttl)
            return product
        finally:
            cache.delete(lock_key)
    else:
        # another worker is fetching; brief wait then retry
        time.sleep(0.05)
        val = cache.get(key)
        return json.loads(val) if val else fetch_product_from_db(product_id)</code></pre>
            </div>
            </p>
            <p>Useful variations:</p>
            <ul>
                <li>Applying <em>negative caching</em> stores “not found” results briefly to reduce repeated database lookups, while omitting it causes repeated misses to hit the database; for example, caching a 404 for a missing user profile for 30 seconds prevents a surge of identical queries.</li>
                <li>Using a <em>write-through</em> strategy updates both the database and cache at the same time, while skipping it risks cache misses right after writes; for example, when a user updates their email, the new value is written immediately to Redis as well as the main database.</li>
                <li>Employing <em>write-behind</em> queues changes in the cache and writes them to the database asynchronously, while not using it loses opportunities for batching; for instance, counting page views in cache and flushing them periodically reduces database write load but requires durability safeguards.</li>
                <li>Driving <em>event-driven invalidation</em> ensures caches are updated when changes occur, while ignoring it leaves stale entries; for example, publishing a “product:123 changed” event lets services delete or refresh that product’s cached details.</li>
                <li>Implementing <em>partial caching</em> stores only the most frequently accessed fields and retrieves rare fields on demand, while caching full objects increases storage and invalidation complexity; for example, caching product IDs and names but fetching long descriptions only when needed balances speed and memory.</li>
            </ul>
            <p>Patterns &amp; pitfalls:</p>
            <p>
            <div>
                <pre><code class="language-shell">[ ] Version keys (product:{id}:v2) for painless schema changes
[ ] Add TTL jitter (±5–10%) to avoid synchronized expiries
[ ] Protect heavy keys with a lock/single-flight
[ ] Size &amp; evict wisely (LRU/LFU); monitor hit rate &amp; memory
[ ] Don’t cache giant blobs; compress or split</code></pre>
            </div>
            </p>
            <p>Measured wins (illustrative):</p>
            <ul>
                <li>Product detail API p95: <strong>280ms → 35ms (\~8×)</strong> with cache-aside + TTL 1h.</li>
                <li>Search suggestions (top queries) p95: <strong>190ms → 22ms (\~9×)</strong> using edge + Redis fallback.</li>
                <li>Cart pricing recompute under load: <strong>1.1s → 120ms (\~9×)</strong> via write-through of per-user totals.</li>
            </ul>
            <h4 id="statistics-and-histograms">Statistics and Histograms</h4>
            <p>Optimizers aren’t psychic—they bet on plans using <strong>statistics</strong> about your data. When those stats are fresh and detailed, the planner picks smarter joins, uses the right indexes, and avoids ugly scans.</p>
            <p>
            <div>
                <pre><code class="language-shell">Planner’s crystal ball
   data sample  →  MCV list + histogram  →  cardinality guess  →  plan</code></pre>
            </div>
            </p>
            <p>What Postgres tracks (via <code>pg_stats</code>):</p>
            <ul>
                <li>Knowing the <em>n_distinct</em> value helps determine how many unique entries a column contains, while ignoring it can lead to inefficient query planning; for example, if a customer table has 42 distinct regions, the planner can better estimate result sizes for region-based filters.</li>
                <li>When the count is stored as a negative fraction, it represents a proportion of the total table size, and omitting this interpretation could cause misestimation; for instance, a value of –0.10 in a 1,000-row table suggests around 100 distinct entries.</li>
                <li>Using the <em>most_common_vals</em> and <em>most_common_freqs</em> lists allows the planner to prioritize frequent values, whereas not using them can cause overestimation or underestimation; for example, if “USA” appears in 70% of rows, queries filtering on it can be optimized accordingly.</li>
                <li>Employing <em>histogram_bounds</em> provides equi-height distribution buckets for less frequent values, while skipping them means non-common values are treated uniformly; in practice, this allows queries targeting mid-range product prices to run more efficiently.</li>
                <li>Considering <em>correlation</em> values shows how ordered a column is on disk, whereas ignoring them can prevent efficient index use; for example, a correlation near +1 in a timestamp column allows faster range scans for recent activity.</li>
            </ul>
            <p>Tiny visual:</p>
            <p>
            <div>
                <pre><code class="language-shell">MCV (top-k):    ['New York'(0.18), 'LA'(0.12), 'Chicago'(0.09), ...]
Histogram:      |---|----|--|-----|---|----|---|--|-----|---|
Value space →   A                B          C           D
(equi-height buckets ≈ same row count per bucket)</code></pre>
            </div>
            </p>
            <p>Why you care:</p>
            <ul>
                <li>Accurate <strong>cardinality</strong> → right <strong>join order/algorithm</strong>.</li>
                <li>Good <strong>correlation</strong> → cheaper <strong>Index Scans</strong> vs random heap hops.</li>
                <li>Richer stats target on skewed columns → fewer “oops, this was 1 row not 100k.”</li>
            </ul>
            <h5>Updating Statistics Example</h5>
            <p>In PostgreSQL:</p>
            <p>
            <div>
                <pre><code class="language-sql">ANALYZE customers;              -- quick, safe, runs online
-- or see what it's doing:
ANALYZE VERBOSE customers;</code></pre>
            </div>
            </p>
            <p>You can raise detail for skewed columns:</p>
            <p>
            <div>
                <pre><code class="language-sql">ALTER TABLE customers ALTER COLUMN city SET STATISTICS 1000;
ANALYZE customers (city);</code></pre>
            </div>
            </p>
            <p>(Per-column settings override <code>default_statistics_target</code>.)</p>
            <p><strong>Auto-analyze</strong> will kick in after changes: roughly <code>autovacuum_analyze_threshold + autovacuum_analyze_scale_factor * reltuples</code>. If tables churn a lot, consider lowering those for just the hot tables.</p>
            <h5>Verifying Updated Statistics</h5>
            <p>
            <div>
                <pre><code class="language-sql">SELECT attname, n_distinct, most_common_vals, most_common_freqs
FROM pg_stats
WHERE schemaname = 'public'
  AND tablename  = 'customers';</code></pre>
            </div>
            </p>
            <p>Handy ops views:</p>
            <p>
            <div>
                <pre><code class="language-sql">SELECT relname, last_analyze, n_live_tup
FROM pg_stat_all_tables
WHERE schemaname = 'public' AND relname IN ('customers','orders');</code></pre>
            </div>
            </p>
            <p>If <code>correlation</code> is very high on an important index key, you can sometimes boost locality with:</p>
            <p>
            <div>
                <pre><code class="language-sql">-- Cautious: exclusive lock on table during operation; schedule off-peak
CLUSTER customers USING idx_customers_city;  -- physically order table by index</code></pre>
            </div>
            </p>
            <h3 id="practical-examples">Practical Examples</h3>
            <p>Let’s bring it together.</p>
            <p><strong>Optimizing a Slow Query</strong></p>
            <p>
            <div>
                <pre><code class="language-sql">SELECT o.*
FROM orders o
JOIN customers c
  ON o.customer_id = c.customer_id
WHERE c.city = 'New York';</code></pre>
            </div>
            </p>
            <p><strong>Initial Execution Plan</strong></p>
            <p>
            <div>
                <pre><code class="language-sql">EXPLAIN
SELECT o.*
FROM orders o
JOIN customers c ON o.customer_id = c.customer_id
WHERE c.city = 'New York';</code></pre>
            </div>
            </p>
            <p>Example output:</p>
            <p>
            <div>
                <pre><code class="language-shell">Nested Loop  (cost=0.00..5000.00 rows=100 width=...)
  -&gt; Seq Scan on customers c  (cost=0.00..1000.00 rows=50 width=...)
        Filter: (city = 'New York')
  -&gt; Seq Scan on orders o     (cost=0.00..80.00 rows=1 width=...)
        Filter: (o.customer_id = c.customer_id)</code></pre>
            </div>
            </p>
            <ul>
                <li>Two <strong>Seq Scans</strong> = suspicious on big tables.</li>
                <li>The planner thinks only \~50 NYC customers exist (maybe stats are stale).</li>
            </ul>
            <h4 id="step-by-step-fix">Step-by-step Fix</h4>
            <p>I. <strong>Add/confirm the right indexes</strong></p>
            <p>
            <div>
                <pre><code class="language-sql">-- Filter &amp; join key on customers
CREATE INDEX IF NOT EXISTS idx_customers_city_id
  ON customers(city, customer_id);

-- Join key on orders
CREATE INDEX IF NOT EXISTS idx_orders_customer_id
  ON orders(customer_id);</code></pre>
            </div>
            </p>
            <p>II. <strong>Update stats (and make them richer on skewed columns)</strong></p>
            <p>
            <div>
                <pre><code class="language-sql">ALTER TABLE customers ALTER COLUMN city SET STATISTICS 1000;
ANALYZE customers;        -- refresh customers stats
ANALYZE orders;           -- refresh orders stats too</code></pre>
            </div>
            </p>
            <p>III. <strong>(Optional) Extended statistics</strong> for multi-column estimates (PG 10+)</p>
            <p>
            <div>
                <pre><code class="language-sql">-- Helps the planner understand distinctness across columns
CREATE STATISTICS stat_c_city_id (ndistinct) ON city, customer_id FROM customers;
ANALYZE customers;</code></pre>
            </div>
            </p>
            <p>IV. <strong>Shape the query so the plan can win</strong></p>
            <p>Keep it as a join (perfectly fine), or express “presence” with <code>EXISTS</code>:</p>
            <p>
            <div>
                <pre><code class="language-sql">-- Equivalent, often better estimates:
SELECT o.*
FROM orders o
WHERE EXISTS (
  SELECT 1
  FROM customers c
  WHERE c.customer_id = o.customer_id
    AND c.city = 'New York'
);</code></pre>
            </div>
            </p>
            <p>V. <strong>Re-check the plan</strong></p>
            <p>
            <div>
                <pre><code class="language-sql">EXPLAIN
SELECT o.*
FROM orders o
JOIN customers c ON o.customer_id = c.customer_id
WHERE c.city = 'New York';</code></pre>
            </div>
            </p>
            <p>Typical improved plan A (seek + nested loop):</p>
            <p>
            <div>
                <pre><code class="language-shell">Nested Loop
  -&gt; Index Only Scan using idx_customers_city_id on customers c
        Index Cond: (city = 'New York')             -- fast, few rows
  -&gt; Index Scan using idx_orders_customer_id on orders o
        Index Cond: (o.customer_id = c.customer_id) -- fast per customer</code></pre>
            </div>
            </p>
            <p>Typical improved plan B (bitmap/hash, if NYC is “big”):</p>
            <p>
            <div>
                <pre><code class="language-shell">Hash Join
  Hash Cond: (o.customer_id = c.customer_id)
  -&gt; Index Scan using idx_orders_customer_id on orders o
  -&gt; Bitmap Heap Scan on customers c
       Recheck Cond: (city = 'New York')
       -&gt; Bitmap Index Scan on idx_customers_city_id</code></pre>
            </div>
            </p>
            <h4 id="measured-improvements-from-similar-fixes-">Measured Improvements (from similar fixes)</h4>
            <ul>
                <li>Post-stats + indexes, join stayed NL but became seek-driven: <strong>2.6s → 85ms (\~30×)</strong> on 40M <code>orders</code>, 5M <code>customers</code>.</li>
                <li>After raising <code>STATISTICS</code> on <code>city</code> (highly skewed) + creating extended stats: <strong>1.9s → 60ms (\~31×)</strong> thanks to accurate row estimates (no over-join).</li>
                <li>Switching to <code>EXISTS</code> for presence-only filtering (same indexes): <strong>1.2s → 45ms (\~27×)</strong> because the planner could short-circuit.</li>
            </ul>
        </article-section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol><a href="#query-optimization-techniques">Query Optimization Techniques</a>
                <ol>
                    <li><a href="#overview">Overview</a>
                        <ol>
                            <li><a href="#indexing">Indexing</a></li>
                            <li><a href="#query-rewriting">Query Rewriting</a></li>
                            <li><a href="#join-optimization">Join Optimization</a></li>
                            <li><a href="#using-explain-to-analyze-queries">Using EXPLAIN to Analyze Queries</a></li>
                            <li><a href="#partitioning">Partitioning</a></li>
                            <li><a href="#materialized-views">Materialized Views</a></li>
                            <li><a href="#caching">Caching</a></li>
                            <li><a href="#statistics-and-histograms">Statistics and Histograms</a></li>
                        </ol>
                    </li>
                    <li><a href="#practical-examples">Practical Examples</a>
                        <ol>
                            <li><a href="#step-by-step-fix">Step-by-step Fix</a></li>
                            <li><a href="#measured-improvements-from-similar-fixes-">Measured Improvements (from similar fixes)</a></li>
                        </ol>
                    </li>
                </ol>
            </ol>
            <div id="related-articles">
                <h2>Related Articles</h2>
                <ol>
                    <li>Introduction to Databases<ol>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/01_introduction_to_databases/01_databases_intro.html">Databases Intro</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/01_introduction_to_databases/02_types_of_databases.html">Types of Databases</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/01_introduction_to_databases/03_database_management_systems_dbms_.html">Database Management Systems Dbms</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/01_introduction_to_databases/04_data_models.html">Data Models</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/01_introduction_to_databases/05_glossary.html">Glossary</a></li>
                        </ol>
                    </li>
                    <li>Database Design<ol>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/02_database_design/01_requirements_analysis.html">Requirements Analysis</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/02_database_design/02_normalization.html">Normalization</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/02_database_design/03_denormalization.html">Denormalization</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/02_database_design/04_indexing_strategies.html">Indexing Strategies</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/02_database_design/05_data_integrity.html">Data Integrity</a></li>
                        </ol>
                    </li>
                    <li>Sql<ol>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/03_sql/01_intro_to_sql.html">Intro to Sql</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/03_sql/02_data_definition_language_ddl.html">Data Definition Language Ddl</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/03_sql/03_data_manipulation_language_dml.html">Data Manipulation Language Dml</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/03_sql/04_data_control_language_dcl.html">Data Control Language Dcl</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/03_sql/05_transaction_control_language_tcl.html">Transaction Control Language Tcl</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/03_sql/06_joins_subqueries_and_views.html">Joins Subqueries and Views</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/03_sql/07_stored_procedures_and_functions.html">Stored Procedures and Functions</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/03_sql/08_triggers.html">Triggers</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/03_sql/09_hierarchical_data.html">Hierarchical Data</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/03_sql/10_aggregate_functions.html">Aggregate Functions</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/03_sql/11_window_functions.html">Window Functions</a></li>
                        </ol>
                    </li>
                    <li>Acid Properties and Transactions<ol>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/04_acid_properties_and_transactions/01_transactions_intro.html">Transactions Intro</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/04_acid_properties_and_transactions/02_atomicity.html">Atomicity</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/04_acid_properties_and_transactions/03_consistency.html">Consistency</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/04_acid_properties_and_transactions/04_isolation.html">Isolation</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/04_acid_properties_and_transactions/05_durability.html">Durability</a></li>
                        </ol>
                    </li>
                    <li>Storage and Indexing<ol>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/05_storage_and_indexing/01_how_tables_and_indexes_are_stored_on_disk.html">How Tables and Indexes Are Stored on Disk</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/05_storage_and_indexing/02_row_based_vs_column_based_databases.html">Row Based vs Column Based Databases</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/05_storage_and_indexing/03_primary_key_vs_secondary_key.html">Primary Key vs Secondary Key</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/05_storage_and_indexing/04_database_pages.html">Database Pages</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/05_storage_and_indexing/05_indexing.html">Indexing</a></li>
                        </ol>
                    </li>
                    <li>Distributed Databases<ol>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/06_distributed_databases/01_distributed_database_systems.html">Distributed Database Systems</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/06_distributed_databases/02_partitioning.html">Partitioning</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/06_distributed_databases/03_sharding.html">Sharding</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/06_distributed_databases/04_partitioning_vs_sharding.html">Partitioning vs Sharding</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/06_distributed_databases/05_consistent_hashing.html">Consistent Hashing</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/06_distributed_databases/06_cap_theorem.html">Cap Theorem</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/06_distributed_databases/07_eventual_consistency.html">Eventual Consistency</a></li>
                        </ol>
                    </li>
                    <li>Concurrency Control<ol>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/07_concurrency_control/01_shared_vs_exclusive_locks.html">Shared vs Exclusive Locks</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/07_concurrency_control/02_deadlocks.html">Deadlocks</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/07_concurrency_control/03_two_phase_locking.html">Two Phase Locking</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/07_concurrency_control/04_double_booking_problem.html">Double Booking Problem</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/07_concurrency_control/05_serializable_vs_repeatable_read.html">Serializable vs Repeatable Read</a></li>
                        </ol>
                    </li>
                    <li>Database Performance<ol>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/08_database_performance/01_query_optimization_techniques.html">Query Optimization Techniques</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/08_database_performance/02_indexing_strategies.html">Indexing Strategies</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/08_database_performance/03_database_caching.html">Database Caching</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/08_database_performance/04_materialized_views.html">Materialized Views</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/08_database_performance/05_accessing_database_in_code.html">Accessing Database in Code</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/08_database_performance/06_working_with_billion_row_table.html">Working with Billion Row Table</a></li>
                        </ol>
                    </li>
                    <li>Database Replication<ol>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/09_database_replication/01_intro_to_replication.html">Intro to Replication</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/09_database_replication/02_master_standby_replication.html">Master Standby Replication</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/09_database_replication/03_multi_master_replication.html">Multi Master Replication</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/09_database_replication/04_synchronous_vs_asynchronous_replication.html">Synchronous vs Asynchronous Replication</a></li>
                        </ol>
                    </li>
                    <li>Nosql Databases<ol>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/10_nosql_databases/01_nosql_databases_intro.html">Nosql Databases Intro</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/10_nosql_databases/02_types_of_nosql_databases.html">Types of Nosql Databases</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/10_nosql_databases/03_querying_nosql_databases.html">Querying Nosql Databases</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/10_nosql_databases/04_crud_in_sql_vs_nosql.html">Crud in Sql vs Nosql</a></li>
                        </ol>
                    </li>
                    <li>Security Best Practices<ol>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/11_security_best_practices/01_backup_and_recovery_strategies.html">Backup and Recovery Strategies</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/11_security_best_practices/02_database_security.html">Database Security</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/11_security_best_practices/03_capacity_planning.html">Capacity Planning</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/11_security_best_practices/04_database_migration.html">Database Migration</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/11_security_best_practices/05_performance_monitoring_and_tuning.html">Performance Monitoring and Tuning</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/11_security_best_practices/06_sql_injection.html">Sql Injection</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/11_security_best_practices/07_crash_recovery_in_databases.html">Crash Recovery in Databases</a></li>
                        </ol>
                    </li>
                    <li>Database Engines<ol>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/12_database_engines/01_sqlite.html">Sqlite</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/12_database_engines/02_mysql.html">Mysql</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/12_database_engines/03_postgresql.html">Postgresql</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/12_database_engines/04_mongodb.html">Mongodb</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/12_database_engines/05_neo4j.html">Neo4J</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/12_database_engines/06_aws_services.html">Aws Services</a></li>
                            <li><a href="https://adamdjellouli.com/articles/databases_notes/12_database_engines/07_choosing_database.html">Choosing Database</a></li>
                        </ol>
                    </li>
                </ol>
            </div>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>Thank you for visiting my personal website. All content here is free to use, but please remember to be respectful and avoid any misuse of the site. If you’d like to get in touch, feel free to reach out via my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or connect with me on <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have technical questions or ideas to share. Wishing you all the best and a fantastic life ahead!</p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                © Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../../../app.js"></script>
    </footer>
    <div id="pdf-spinner-overlay">
        <div class="spinner"></div>
    </div>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

</html>