<!DOCTYPE html>

<html lang="en">

<head>
    <script async="" crossorigin="anonymous" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089"></script>
    <meta charset="utf-8" />
    <title>Numerical Methods</title>
    <meta content="Gaussian elimination is a fundamental algorithmic procedure in linear algebra used to solve systems of linear equations, find matrix inverses, and determine the rank of matrices." name="description" />
    <meta content="Adam Djellouli" name="author" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <meta content="IE=edge" http-equiv="X-UA-Compatible" />
    <link crossorigin="" href="https://fonts.googleapis.com" rel="preconnect" />
    <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico" rel="icon" />
    <link href="../resources/style.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
</head>

<body>
    <nav aria-label="Main navigation">
        <a class="logo" href="https://adamdjellouli.com">
            <img alt="Adam Djellouli - Home Page Logo" id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" />
        </a>
        <input aria-label="Toggle navigation menu" id="navbar-toggle" type="checkbox" />
        <ul aria-labelledby="navbar-toggle" role="menu">
            <li role="menuitem">
                <a href="../index.html" title="Go to Home Page"> Home </a>
            </li>
            <li role="menuitem">
                <a class="active" href="../core/blog.html" title="Read Adam Djellouli Blog on Programming and Technology"> Blog </a>
            </li>
            <li role="menuitem">
                <a href="../core/tools.html" title="Discover Tools Created by Adam Djellouli"> Tools </a>
            </li>
            <li role="menuitem">
                <a href="../core/projects.html" title="Explore Projects Developed by Adam Djellouli"> Projects </a>
            </li>
            <li role="menuitem">
                <a href="../core/resume.html" title="View Adam Djellouli Professional Resume"> Resume </a>
            </li>
            <li>
                <script async="" src="https://cse.google.com/cse.js?cx=8160ef9bb935f4f68"></script>
                <div class="gcse-search"></div>
            </li>
            <li>
                <button aria-label="Toggle dark mode" id="dark-mode-button"></button>
            </li>
        </ul>
    </nav>
    <!--  ======================= Start Banner Area =======================  -->
    <section class="banner blog-page-banner">
    </section>
    <!--  ======================= End Banner Area =======================  -->
    <section class="blog-page">
        <div class="article-list">
            <div class="article-list">
                <h1>Numerical Methods</h1>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/2_systems_of_equations/gaussian_elimination.html">Gaussian Elimination ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/2_systems_of_equations/gaussian_elimination.html">Gaussian elimination is a fundamental algorithmic procedure in linear algebra used to solve systems of linear equations, find matrix inverses, and determine the rank of matrices. The procedure systematically applies elementary row operations to transform a given matrix into an upper-triangular form ...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/2_systems_of_equations/gauss_seidel.html">Gauss Seidel ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/2_systems_of_equations/gauss_seidel.html">The Gauss-Seidel method is a classical iterative method for solving systems of linear equations of the form $A\mathbf{x} = \mathbf{b}$, where $A$ is an $n \times n$ matrix, $\mathbf{x}$ is the vector of unknowns $(x_1, x_2, \ldots, x_n)$, and $\mathbf{b}$ is a known vector. Unlike direct methods suc...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/2_systems_of_equations/systems_of_equations.html">Systems of Equations ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/2_systems_of_equations/systems_of_equations.html">Linear systems of equations can be represented in a matrix form, which enables the use of a variety of numerical methods for solving them. ...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/2_systems_of_equations/inverse_matrix.html">Inverse Matrix ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/2_systems_of_equations/inverse_matrix.html">The inverse of a matrix A is denoted as A^-1. It is a unique matrix such that when it is multiplied by the original matrix A, the result is the identity matrix I. Mathematically, this is expressed as...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/2_systems_of_equations/lu_decomposition.html">Lu Decomposition ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/2_systems_of_equations/lu_decomposition.html">LU Decomposition (or LU Factorization) is a powerful and widely used technique in numerical linear algebra for solving systems of linear equations, computing inverses, and determining determinants. The core idea is to factorize a given square matrix $A$ into the product of a lower-triangular matrix ...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/2_systems_of_equations/jacobi_method.html">Jacobi Method ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/2_systems_of_equations/jacobi_method.html">The Jacobi method is a classical iterative algorithm used to approximate the solution of a system of linear equations $A\mathbf{x} = \mathbf{b}$. Instead of attempting to solve the system directly using methods such as Gaussian elimination, the Jacobi method iteratively refines an initial guess for ...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/6_regression/cubic_spline_interpolation.html">Cubic Spline Interpolation ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/6_regression/cubic_spline_interpolation.html">Cubic spline interpolation is a refined mathematical tool frequently used within numerical analysis. It's an approximation technique that employs piecewise cubic polynomials, collectively forming a cubic spline. These cubic polynomials are specifically engineered to pass through a defined set of dat...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/6_regression/linear_interpolation.html">Linear Interpolation ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/6_regression/linear_interpolation.html">Linear interpolation is one of the most basic and commonly used interpolation methods. The idea is to approximate the value of a function between two known data points by assuming that the function behaves linearly (like a straight line) between these points. Although this assumption may be simplist...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/6_regression/interpolation.html">Interpolation ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/6_regression/interpolation.html">Interpolation is a method of constructing new data points within the range of a discrete set of known data points. It plays a crucial role in data analysis by helping to predict unknown values for any point within the given range...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/6_regression/gaussian_interpolation.html">Gaussian Interpolation ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/6_regression/gaussian_interpolation.html">Gaussian Interpolation, often associated with Gaussâ€™s forward and backward interpolation formulas, is a technique that refines the approach of polynomial interpolation when data points are equally spaced. Instead of using the Newton forward or backward interpolation formulas directly from one end of...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/6_regression/thin_plate_spline_interpolation.html">Thin Plate Spline Interpolation ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/6_regression/thin_plate_spline_interpolation.html">Thin Plate Spline (TPS) Interpolation is a non-parametric, spline-based method for interpolating scattered data in two or more dimensions. Originally arising in the context of fitting a smooth surface through a set of points in $\mathbb{R}^2$, thin plate splines can be generalized to higher dimensio...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/6_regression/regression.html">Regression ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/6_regression/regression.html">Regression analysis and curve fitting are critical methods in statistical analysis and machine learning. Both aim to find a function that best approximates a set of data points, yet their typical applications may vary slightly. They are particularly useful in understanding relationships among variab...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/6_regression/newton_polynomial.html">Newton Polynomial ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/6_regression/newton_polynomial.html">Newtonâ€™s Polynomial, often referred to as Newtonâ€™s Interpolation Formula, is another classical approach to polynomial interpolation. Given a set of data points $(x_0,y_0),(x_1,y_1),\dots,(x_n,y_n)$ with distinct $x_i$ values, Newtonâ€™s method constructs an interpolating polynomial in a form that make...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/6_regression/least_squares.html">Least Squares ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/6_regression/least_squares.html">Least Squares Regression is a fundamental technique in statistical modeling and data analysis used for fitting a model to observed data. The primary goal is to find a set of parameters that minimize the discrepancies (residuals) between the modelâ€™s predictions and the actual observed data. The "leas...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/6_regression/lagrange_polynomial_interpolation.html">Lagrange Polynomial Interpolation ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/6_regression/lagrange_polynomial_interpolation.html">Lagrange Polynomial Interpolation is a widely used technique for determining a polynomial that passes exactly through a given set of data points. Suppose we have a set of $(n+1)$ data points $(x_0, y_0), (x_1, y_1), \ldots, (x_n, y_n)$ where all $x_i$ are distinct. The aim is to find a polynomial $L...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/1_root_and_extrema_finding/newtons_method.html">Newtons Method ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/1_root_and_extrema_finding/newtons_method.html">Newton's method (or the Newton-Raphson method) is a powerful root-finding algorithm that exploits both the value of a function and its first derivative to rapidly refine approximations to its roots. Unlike bracketing methods that work by enclosing a root between two points, Newton's method is an ope...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/1_root_and_extrema_finding/bisection_method.html">Bisection Method ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/1_root_and_extrema_finding/bisection_method.html">The bisection method is a classical root-finding technique used extensively in numerical analysis to locate a root of a continuous function $f(x)$ within a specified interval $[a, b]$. It belongs to the family of bracketing methods, which use intervals known to contain a root and systematically redu...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/1_root_and_extrema_finding/secant_method.html">Secant Method ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/1_root_and_extrema_finding/secant_method.html">The Secant Method is a root-finding algorithm used in numerical analysis to approximate the zeros of a given function $f(x)$. It can be regarded as a derivative-free variant of Newton's method. Instead of computing the derivative $f'(x)$ at each iteration (as done in Newtonâ€™s method), it approximate...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/1_root_and_extrema_finding/gradient_descent.html">Gradient Descent ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/1_root_and_extrema_finding/gradient_descent.html">Gradient Descent is a fundamental first-order optimization algorithm widely used in mathematics, statistics, machine learning, and artificial intelligence. Its principal aim is to find the minimum of a given differentiable function $f(x)$. Instead of searching blindly, it uses gradient information â€”...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/1_root_and_extrema_finding/root_finding.html">Root Finding ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/1_root_and_extrema_finding/root_finding.html">Root: A root of a function $f(x)$ is a value $x = r$ such that $f(r) = 0$. This applies not only to linear functions but can also extend to nonlinear systems, such as those including powers ($x^2, x^3, \ldots$), roots, radicals and non-integer polynomials ($\sqrt{x}, x^{3/5}, x^{pi} \ldots$), or tri...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/1_root_and_extrema_finding/golden_ratio_search.html">Golden Ratio Search ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/1_root_and_extrema_finding/golden_ratio_search.html">The Golden Ratio Search is a technique employed for locating the extremum (minimum or maximum) of a unimodal function over a given interval. Unlike gradient-based or derivative-requiring methods, this approach uses only function evaluations, making it broadly applicable even when derivatives are dif...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/1_root_and_extrema_finding/relaxation_method.html">Relaxation Method ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/1_root_and_extrema_finding/relaxation_method.html">The relaxation method, commonly referred to as the fixed-point iteration method, is an iterative approach used to find solutions (roots) to nonlinear equations of the form $f(x) = 0$. Instead of directly solving for the root, the method involves rewriting the original equation in the form...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/3_differentiation/taylor_series.html">Taylor Series ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/3_differentiation/taylor_series.html">The Taylor series is a fundamental tool in calculus and mathematical analysis, offering a powerful way to represent and approximate functions. By expanding a function around a specific point, known as the "center" or "point of expansion," we can express it as an infinite sum of polynomial terms deri...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/3_differentiation/central_difference.html">Central Difference ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/3_differentiation/central_difference.html">...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/3_differentiation/backward_difference.html">Backward Difference ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/3_differentiation/backward_difference.html">The backward difference approximation of the first derivative of a function $f$ at a point $x$ with step size $h$ is given by...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/3_differentiation/forward_difference.html">Forward Difference ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/3_differentiation/forward_difference.html">...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/3_differentiation/differentiation.html">Differentiation ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/3_differentiation/differentiation.html">The classical definition of the derivative of a function $f(x)$ at a point $x_0$...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/4_integration/integration_introduction.html">Integration Introduction ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/4_integration/integration_introduction.html">$$\int_{1}^{2} x^2 dx \approx \sum_{i=1}^{10} h \cdot f(1 + 0.1i)$...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/4_integration/trapezoidal_rule.html">Trapezoidal Rule ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/4_integration/trapezoidal_rule.html">The Trapezoidal Rule operates by assuming the region under the graph of the function as a trapezoid, then calculating its area...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/4_integration/simpsons_rule.html">Simpsons Rule ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/4_integration/simpsons_rule.html">The foundation of Simpson's Rule lies in the concept of estimating the integral of a function $f(x)$ over a specified interval $[a, b]$ by the area beneath a quadratic polynomial. This polynomial passes through the points $(a, f(a))$, $((a+b)/2, f((a+b)/2))$, and $(b, f(b))$...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/4_integration/monte_carlo.html">Monte Carlo ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/4_integration/monte_carlo.html">Monte Carlo integration is a numerical technique for approximating integrals using randomness. Rather than systematically sampling a function at predetermined points, as done in methods like the trapezoidal rule or Simpsonâ€™s rule, Monte Carlo methods rely on random samples drawn from a prescribed do...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/4_integration/midpoint_rule.html">Midpoint Rule ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/4_integration/midpoint_rule.html">For a function $f(x)$ defined over an interval $[a, b]$, the Midpoint Rule provides the following approximation for the integral...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/5_matrices/matrix_methods.html">Matrix Methods ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/5_matrices/matrix_methods.html">$$A = LU$...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/5_matrices/power_method.html">Power Method ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/5_matrices/power_method.html">The power method is a fundamental iterative algorithm for estimating the eigenvalue of largest magnitude and its associated eigenvector for a given matrix. This technique is particularly appealing when dealing with large and sparse matrices, where direct eigenvalue computations (e.g., via the charac...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/5_matrices/eigen_value_decomposition.html">Eigen Value Decomposition ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/5_matrices/eigen_value_decomposition.html">Eigenvalue Decomposition (EVD), also known as Eigendecomposition, is a fundamental operation in linear algebra that breaks down a square matrix into a simpler form defined by its eigenvalues and eigenvectors. This decomposition provides deep insights into the properties and structure of a matrix, en...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/5_matrices/qr_method.html">Qr Method ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/5_matrices/qr_method.html">The QR method is a widely used algorithm in numerical linear algebra for determining the eigenvalues of a given square matrix. Unlike direct methods such as solving the characteristic polynomial, which can be complicated and unstable numerically for large matrices, the QR method leverages iterative ...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/5_matrices/singular_value_decomposition.html">Singular Value Decomposition ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/5_matrices/singular_value_decomposition.html">Singular Value Decomposition (SVD) is a fundamental matrix decomposition technique widely used in numerous areas of science, engineering, and data analysis. Unlike the Eigenvalue Decomposition (EVD), which is restricted to square and diagonalizable matrices, SVD applies to any rectangular matrix. It...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/5_matrices/eigenvalues_and_eigenvectors.html">Eigenvalues and Eigenvectors ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/5_matrices/eigenvalues_and_eigenvectors.html">Eigenvalues and eigenvectors are foundational concepts in linear algebra, with extensive applications across various domains such as physics, computer graphics, and machine learning. These concepts are instrumental in decomposing complex matrix transformations, thereby simplifying numerical computat...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/7_ordinary_differential_equations/picards_method.html">Picards Method ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/7_ordinary_differential_equations/picards_method.html">Picard's method, alternatively known as the method of successive approximations, is a tool primarily used for solving initial-value problems for first-order ordinary differential equations (ODEs). The approach hinges on an iterative process that approximates the solution of an ODE. Though this metho...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/7_ordinary_differential_equations/eulers_method.html">Eulers Method ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/7_ordinary_differential_equations/eulers_method.html">Euler's Method is a numerical technique applied in the realm of initial value problems for ordinary differential equations (ODEs). The simplicity of this method makes it a popular choice in cases where the differential equation lacks a closed-form solution. The method might not always provide the mo...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/7_ordinary_differential_equations/heuns_method.html">Heuns Method ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/7_ordinary_differential_equations/heuns_method.html">Heun's method is an improved version of Euler's method that enhances accuracy by using an average of the slope at the beginning and the predicted slope at the end of the interval...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/7_ordinary_differential_equations/runge_kutta.html">Runge Kutta ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/7_ordinary_differential_equations/runge_kutta.html">The Runge-Kutta method is part of a family of iterative methods, both implicit and explicit, which are frequently employed for the numerical integration of ordinary differential equations (ODEs). This family encompasses widely recognized methods like the Euler Method, Heun's method (a.k.a., the 2nd...</a></p>
                </div>
                <div class="article-list-element">
                    <h2><a href="../articles/numerical_methods/7_ordinary_differential_equations/ordinary_differential_equations.html">Ordinary Differential Equations ðŸ‡ºðŸ‡¸</a></h2>
                    <div class="article-date">December 22, 2024</div>
                    <div class="article-category">Category: <a href="numerical_methods.html">Numerical Methods</a></div>
                    <p><a href="../articles/numerical_methods/7_ordinary_differential_equations/ordinary_differential_equations.html">Ordinary Differential Equations (ODEs) are equations that involve one independent variable and the derivatives of one dependent variable with respect to the independent variable. They are called "ordinary" to distinguish them from partial differential equations (PDEs), which involve partial derivati...</a></p>
                </div>
            </div>
        </div>
        <div class="pagination"></div>
    </section>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img alt="Adam Djellouli Symbol" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" />
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>
                <p>
                    Thank you for visiting my personal website. All of the <br />
                    content on this site is free to use, but please remember <br />
                    to be a good human being and refrain from any abuse<br />
                    of the site. If you would like to contact me, please use <br />
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br />
                    issues or ideas to share. I wish you the best and hope you <br />
                    have a fantastic life. <br />
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a class="fa fa-youtube" href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a class="fa fa-linkedin" href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a class="fa fa-instagram" href="https://www.instagram.com/linuxchallenges/" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a class="fa fa-github" href="https://github.com/djeada" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                Â© Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../app.js"></script>
    </footer>
</body>

</html>