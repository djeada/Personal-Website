<!DOCTYPE html>
<html lang="en">

<head>
    <title>Adam Djellouli - Blog</title>
    <meta charset="utf-8">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" />
    <link rel="icon" href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico">
    <link rel="stylesheet" type="text/css" href="../resources/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie-edge" />
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089" crossorigin="anonymous"></script>
</head>

<body>
    <nav>
        <a class="logo" href="../index.html">
            <img id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" alt="Adam Djellouli">
        </a>
        <input id="navbar-toggle" type="checkbox" />
        <ul>
            <li> <a href="../index.html"> Home </a> </li>
            <li> <a href="../core/blog.html" class="active"> Blog </a> </li>
            <li> <a href="../core/tools.html"> Tools </a> </li>
            <li> <a href="../core/projects.html"> Projects </a> </li>
            <li> <a href="../core/resume.html"> Resume </a> </li>
            <li> <a href="../core/about.html"> About </a> </li>
            <button id="dark-mode-button"></button>
        </ul>
    </nav>
    <section id="article-body">
        <p style='text-align: right;'><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>

        <h2>Logistic Regression</h2>
        <p>In machine learning, classification is the task of predicting a discrete value for a given input. An example of this is determining whether an email is spam or not spam. Logistic regression is a popular method for classification, which uses the sigmoid function to predict the probability that a given input belongs to a specific class. The decision boundary is the line that separates the different classes and can be either linear or non-linear. By adding polynomial features to the input data, we can create a non-linear decision boundary that is better able to fit and classify complex data sets.</p>
        <h2>Classification</h2>
        <ul>
            <li>Y can only have discrete values.</li>
            <li>For example: 0 = negative class (absence of something) and 1 = positive
                class (presence of something).</li>
            <li>Email âˆ’ &gt; spam/not spam?</li>
            <li>Online transactions âˆ’ &gt; fraudulent?</li>
            <li>Tumor âˆ’ &gt; Malignant/benign?</li>
        </ul>
        <p>Letâ€™s go back to the cancer example from the Week 1 and try to apply linear regression:</p>
        <p><img alt="cancer_classification" src="https://user-images.githubusercontent.com/37275728/201496614-36ec47d4-437e-4d25-82bf-27289489a5a7.png" /></p>
        <p>We see that it wasnâ€™t the best idea. Of course, we could attempt another approach to find a straight line that would better separate the points, but a straight line isnâ€™t our sole choice. There are more appropriate models for that job.</p>
        <h2>Hypothesis representation</h2>
        <ul>
            <li>We want our classifier to output values between 0 and 1.</li>
            <li>For classification hypothesis representation we have: $h_{\theta}(x) = g((\theta^Tx))$.</li>
            <li>$g(z)$ is called the sigmoid function, or the logistic function.
                $$g(z) = \frac{1}{1 + e^{-z}}$$</li>
            <li>If we combine these equations we can write out the hypothesis as:</li>
        </ul>
        <p>$$h_{\theta}(x) = \frac{1}{1+e^{-\theta^Tx}}$$</p>
        <p><img alt="sigmoid" src="https://user-images.githubusercontent.com/37275728/201496643-38a45685-61a5-4af4-bf24-2acaa22ef1ff.png" /></p>
        <p>When our hypothesis $(h_{\theta}(x))$ outputs a number, we treat that value as the estimated probability that $y=1$ on input $x$.</p>
        <p>$$h_{\theta}(x) = P(y=1|x\ ;\ \theta)$$</p>
        <p>Example:</p>
        <p>$h_{\theta}(x) = 0.7$ and</p>
        <p>$$
            X = \begin{bmatrix}
            1 \\
            tumourSize
            \end{bmatrix}
            $$</p>
        <p>Informs a patient that a tumor has a $70\%$ likelihood of being malignant.</p>
        <h3>Decision boundary</h3>
        <p>One way of using the sigmoid function is:</p>
        <ul>
            <li>When the probability of y being 1 is greater than 0.5 then we can predict y = 1.</li>
            <li>Else we predict y = 0.</li>
        </ul>
        <p><img alt="decision_boundary" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/decision_boundary.png" /></p>
        <ul>
            <li>The hypothesis predicts $y = 1$ when $\theta^T x &gt;= 0$.</li>
            <li>When $\theta^T x &lt;= 0$ then the hypothesis predicts y = 0.</li>
        </ul>
        <p>Example</p>
        <p>$$h_{\theta}(x) = g(\theta_0 + \theta_1x_1 + \theta_2x_2)$$</p>
        <p>$$
            \theta = \begin{bmatrix}
            -3 \\
            1 \\
            1
            \end{bmatrix}
            $$</p>
        <p>We predict $y = 1$ if:</p>
        <p>$$-3x_0 + 1x_1 + 1x_2 \geq 0$$
            $$-3 + x_1 + x_2 \geq 0$$</p>
        <p>As a result, the straight line equation is as follows:</p>
        <p>$$x_2 = -x_1 + 3$$</p>
        <p><img alt="linear_decision_boundary" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/linear_decision_boundary.png" /></p>
        <ul>
            <li>Blue = false</li>
            <li>Magenta = true</li>
            <li>Line = decision boundary</li>
        </ul>
        <h2>Non-linear decision boundaries</h2>
        <p>Get logistic regression to fit a complex non-linear data set.</p>
        <p>Example</p>
        <p>$$h_{\theta}(x) = g(\theta_0 + \theta_1x_1 + \theta_3x_1^2 + \theta_4x_2^2)$$</p>
        <p>$$
            \theta = \begin{bmatrix}
            -1 \\
            0 \\
            0 \\
            1 \\
            1
            \end{bmatrix}
            $$</p>
        <p>We predict $y = 1$ if:</p>
        <p>$$-1 + x_1^2 + x_2^2 \geq 0$$</p>
        <p>$$x_1^2 + x_2^2 \geq 1$$</p>
        <p>As a result, the circle equation is as follows:</p>
        <p>$$x_1^2 + x_2^2 = 1$$</p>
        <p>This gives us a circle with a radius of 1 around 0.</p>
        <p><img alt="non_linear_decision_boundary" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/non_linear_decision_boundary.png" /></p>
        <h2>Cost function for logistic regression</h2>
        <ul>
            <li>Fit $\theta$ parameters.</li>
            <li>Define the optimization object for the cost function we use the fit the parameters.</li>
        </ul>
        <p>Training set of m training examples:</p>
        <p>$${(x^{(1)}, y^{(1)}), (x^{(1)}, y^{(1)}), ..., (x^{(m)}, y^{(m)})}$$</p>
        <p>$$<br />
            x = \begin{bmatrix}
            x_0 \\
            x_1 \\
            ... \\
            x_n
            \end{bmatrix}
            $$</p>
        <p>$$x_0 =1,\quad y \in {0,1}$$</p>
        <p>Linear regression uses the following function to determine $\theta$:</p>
        <p>$$J(\theta) = \frac{1}{2m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2$$</p>
        <p>We define "cost()" as:</p>
        <p>$$cost(h_{\theta}(x^{(i)}), y^{(i)}) = \frac{1}{2} (h_{\theta}(x^{(i)}) - y^{(i)})^2$$</p>
        <p>We can now redefine $J(\theta)$ as:</p>
        <p>$$J(\theta) = \frac{1}{m} \sum_{i=1}^{m}cost(h_{\theta}(x^{(i)}), y^{(i)})$$</p>
        <ul>
            <li>This is the cost you want the learning algorithm to pay if the outcome is $h_{\theta}(x)$ but the actual outcome is y.</li>
            <li>This function is a non-convex function for parameter optimization when used for logistic regression.</li>
            <li>If you take $h_{\theta}(x)$ and plug it into the Cost() function, and them plug the Cost() function into $J(\theta)$ and plot $J(\theta)$ we find many local optimum.</li>
        </ul>
        <p>$$
            [ cost(h_{\theta}(x), y) = \begin{cases}
            -log(h_{\theta}(x)) &amp; if\ y=1 \\
            -log(1 - h_{\theta}(x)) &amp; if\ y=0
            \end{cases}
            ]
            $$</p>
        <p>Finally:</p>
        <p>$$J(\theta) = \frac{1}{m} \sum_{i=1}^{m}[-y^{(i)}log(h_{\theta}(x^{(i)})) - (1-y^{(i)})log(1 - h_{\theta}(x^{(i)}))]$$</p>
        <p>$$\frac{\partial}{\partial \theta_j} J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$$</p>
        <p>Note that while this gradient looks identical to the linear regression gra-
            dient, the formula is actually different because linear and logistic regression
            have different definitions of hÎ¸ (x).</p>
        <h2>Multiclass classification problems</h2>
        <p>Getting logistic regression for multiclass classification using one vs. all.</p>
        <p><img alt="multiclass_classification" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/multiclass_classification.png" /></p>
        <p>Split the training set into three separate binary classification problems.</p>
        <ul>
            <li>Triangle (1) vs crosses and squares (0) $h_{\theta}^{(1)}(x)$.</li>
            <li>Crosses (1) vs triangle and square (0) $h_{\theta}^{(2)}(x)$.</li>
            <li>Square (1) vs crosses and square (0) $h_{\theta}^{(3)}(x)$.</li>
        </ul>
        <p><img alt="one_vs_all" src="https://raw.githubusercontent.com/djeada/Stanford-Machine-Learning/main/slides/resources/one_vs_all.png" /></p>

    </section>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" alt="Adam Djellouli">

            </div>
            <div class="footer-column">

                <p>
                    Thank you for visiting my personal website. All of the </br>
                    content on this site is free to use, but please remember </br>
                    to be a good human being and refrain from any abuse</br>
                    of the site. If you would like to contact me, please use </br>
                    my LinkedIn profile or my GitHub if you have any technical </br>
                    issues or ideas to share. I wish you the best and hope you </br>
                    have a fantastic life. </br>
                </p>

            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" class="fa fa-youtube" target="_blank">

                        </a>YouTube
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" class="fa fa-linkedin" target="_blank">

                        </a>LinkedIn
                    </li>
                    <li>
                        <a href="https://www.instagram.com/addjellouli/" class="fa fa-instagram" target="_blank">
                        </a>Instagram

                    </li>
                    <li>
                        <a href="https://github.com/djeada" class="fa fa-github">
                        </a>Github

                    </li>

                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                &copy; Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../app.js"></script>
    </footer>
</body>

</html>

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"], ["\(","\)"] ], displayMath: [ ["$$","$$"], ["\[", "\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>

<script type="text/javascript" id="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>