<!DOCTYPE html>

<html lang="en">

<head>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5593122079896089" crossorigin="anonymous"></script>
    <title>Hardware in Parallel Computing</title>
    <meta charset="utf-8">
    <meta name="description" content="Parallel computing involves dividing a task into smaller parts that can be processed simultaneously by multiple processors.">
    <meta name="keywords" content="Adam Djellouli">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/themes/prism.min.css" />
    <link rel="icon" href="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/icon.ico">
    <link rel="stylesheet" type="text/css" href="../resources/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie-edge" />
</head>

<body>
    <nav>
        <a class="logo" href="../index.html" title="Adam Djellouli - Home">
            <img id="logo-image" src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/logo.PNG" alt="Adam Djellouli Logo">
        </a>
        <input id="navbar-toggle" type="checkbox" aria-label="Toggle navigation menu" />
        <ul>
            <li> <a href="../index.html" title="Home"> Home </a> </li>
            <li> <a href="../core/blog.html" class="active" title="Adam Djellouli's Blog - Programming, technology and more"> Blog </a> </li>
            <li> <a href="../core/tools.html" title="Useful Tools by Adam Djellouli"> Tools </a> </li>
            <li> <a href="../core/projects.html" title="Projects by Adam Djellouli"> Projects </a> </li>
            <li> <a href="../core/resume.html" title="Adam Djellouli's Resume"> Resume </a> </li>
            <li> <a href="../core/about.html" title="About Adam Djellouli"> About </a> </li>
            <button id="dark-mode-button" aria-label="Toggle dark mode"></button>
        </ul>
    </nav>
    <section id="article-body"></section>
    <p style="text-align: right;"><i>This article is written in: ðŸ‡ºðŸ‡¸</i></p>
    <div id="article-wrapper">
        <section id="article-body">

            <header>Hardware in Parallel Computing</header>
            <p>Parallel computing involves dividing a task into smaller parts that can be processed simultaneously by multiple processors. We will explore the different ways of achieving parallelism in hardware and how they affect the performance of parallel computing.</p>
            <h2 id="ways-of-achieving-parallelism">Ways of Achieving Parallelism</h2>
            <p>There are different ways of achieving parallelism:</p>
            <ul>
                <li>Within the processor (instruction-level parallelism, multicore).</li>
                <li>Using several processors in the same machine (multiprocessing).</li>
                <li>Using various machines (distributed computing, multicomputer).</li>
            </ul>
            <h2 id="single-core-cpu">Single-Core CPU</h2>
            <p>In a single-core CPU, every step of the program is converted to a binary instruction(s), which is tailored to the individual CPU architecture. </p>
            <ol>
                <li>The program is first loaded into system memory (RAM) from the hard drive, </li>
                <li>and the instructions are then sent from system memory to the CPU via a bus. </li>
                <li>Once a program is loaded into the CPU, it moves down the queue (pipeline) of instructions, which are executed one at a time. </li>
                <li>To execute instructions, the CPU employs many components (ALU, for example), and the clock determines the speed of the CPU.</li>
            </ol>
            <p>A single-core processor is limited when it comes to parallel computing because it can only perform one task at a time. </p>
            <h2 id="multi-core-cpu">Multi-Core CPU</h2>
            <p>A multi-core CPU is intended to handle multithreading easily. </p>
            <ul>
                <li>The term "multi-core" refers to the fact that everything is replicated (pipeline and execution engine). </li>
                <li>Hyper-threading was one of the early concepts for improving multithreading efficiency. The idea is to use one physical core to run more than one thread at the same time. The pipelines for the threads were being duplicated in hyper-threading, making it quicker, but not everything could be executed in parallel.</li>
            </ul>
            <h2 id="graphics-processing-unit-gpu-">Graphics Processing Unit (GPU)</h2>
            <ul>
                <li>Graphics Processing Units (GPUs) are specialized processors designed for parallel computing. They are optimized to handle thousands of tasks in parallel.</li>
                <li>GPUs were originally designed for rendering graphics in video games and other applications, but they have since been adapted for use in a wide range of scientific and engineering applications that require high-performance computing.</li>
                <li>GPUs are particularly well-suited to data-parallelism because they contain many small processing cores that can work on different parts of the same problem simultaneously.</li>
                <li>To take advantage of the parallel processing power of GPUs, software developers typically use specialized programming languages and libraries such as CUDA, OpenCL, and OpenGL.</li>
                <li>By utilizing GPUs for parallel computing, developers can achieve significant performance gains over traditional CPU-based systems. However, this requires a strong understanding of parallel programming concepts and the ability to design algorithms that can take advantage of the parallelism provided by GPUs.</li>
            </ul>
            <h2 id="shared-memory-architectures">Shared Memory Architectures</h2>
            <p>Shared memory architectures have all processors act independently but access the same global address space. Changes in one memory location are visible for all others. There are two types of shared memory architectures:</p>
            <ul>
                <li>Uniform Memory Access (UMA): Equal load and store access for all processors to all memory. It was the default approach for the majority of SMP systems in the past.</li>
                <li>Non-Uniform Memory Access (NUMA): Delay on memory access according to the accessed region. It is typically realized by a processor interconnection network and local memories. Cache-coherent NUMA (CC-NUMA) is completely implemented in hardware and has become the standard approach with recent X86 chips.</li>
            </ul>
            <h2 id="data-parallel-simd-vs-task-parallel-mimd">Data Parallel / SIMD vs Task Parallel / MIMD</h2>
            <p>There are two types of parallelism:</p>
            <ul>
                <li>Data Parallel / SIMD: This involves performing the same operation on multiple data sets simultaneously. Examples of hardware that use this approach include GPUs, Cell processors, SSE, AltiVec, and vector processors.</li>
                <li>Task Parallel / MIMD: This involves performing different operations on different data sets simultaneously. Examples of hardware that use this approach include many-core/SMP systems, processor-array systems, systolic arrays, Hadoop, cluster systems, and MPP systems.</li>
            </ul>
        </section>
        <div id="table-of-contents">
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#ways-of-achieving-parallelism">Ways of Achieving Parallelism</a></li>
                <li><a href="#single-core-cpu">Single-Core CPU</a></li>
                <li><a href="#multi-core-cpu">Multi-Core CPU</a></li>
                <li><a href="#graphics-processing-unit-gpu-">Graphics Processing Unit (GPU)</a></li>
                <li><a href="#shared-memory-architectures">Shared Memory Architectures</a></li>
                <li><a href="#data-parallel-simd-vs-task-parallel-mimd">Data Parallel / SIMD vs Task Parallel / MIMD</a></li>
            </ol>
        </div>
    </div>
    <footer>
        <div class="footer-columns">
            <div class="footer-column">
                <img src="https://raw.githubusercontent.com/djeada/Personal-Website/master/images/symbol.png" alt="Adam Djellouli Symbol">
            </div>
            <div class="footer-column">
                <h2><a href="https://adamdjellouli.com/core/privacy_policy.html" title="Privacy Policy">Our Privacy Policy</a></h2>

                <p>
                    Thank you for visiting my personal website. All of the <br>
                    content on this site is free to use, but please remember <br>
                    to be a good human being and refrain from any abuse<br>
                    of the site. If you would like to contact me, please use <br>
                    my <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" title="LinkedIn Profile">LinkedIn profile</a> or my <a href="https://github.com/djeada" title="GitHub Profile">GitHub</a> if you have any technical <br>
                    issues or ideas to share. I wish you the best and hope you <br>
                    have a fantastic life. <br>
                </p>
            </div>
            <div class="footer-column">
                <h2>Follow me</h2>
                <ul class="social-media">
                    <li>
                        <a href="https://www.youtube.com/channel/UCGPoHTVjMN77wcGknXPHl1Q" class="fa fa-youtube" target="_blank" title="YouTube">
                        </a>YouTube
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/adam-djellouli-1bb54619a/" class="fa fa-linkedin" target="_blank" title="LinkedIn">
                        </a>LinkedIn
                    </li>
                    <li>
                        <a href="https://www.instagram.com/addjellouli/" class="fa fa-instagram" target="_blank" title="Instagram">
                        </a>Instagram
                    </li>
                    <li>
                        <a href="https://github.com/djeada" class="fa fa-github" title="GitHub">
                        </a>Github
                    </li>
                </ul>
            </div>
        </div>
        <div>
            <p id="copyright">
                &copy; Adam Djellouli. All rights reserved.
            </p>
        </div>
        <script>
            document.getElementById("copyright").innerHTML = "&copy; " + new Date().getFullYear() + " Adam Djellouli. All rights reserved.";
        </script>
        <script src="../app.js"></script>
    </footer>
</body>

</html>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-cpp.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
extensions: ["tex2jax.js"],
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
tex2jax: { inlineMath: [ ["$", "$"] ], displayMath: [ ["$$","$$"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
messageStyle: "none"
});
</script>
<script async="" id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>