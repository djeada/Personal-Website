{
    "category": "Statistics",
    "subcategories": [
        {
            "cards": [
                {
                    "back": "<p><strong>ARIMA</strong> stands for <strong>AutoRegressive Integrated Moving Average</strong>:\n1. <strong>AR (AutoRegressive)</strong>: Uses past values (lags) to predict future values.\n2. <strong>I (Integrated)</strong>: Involves differencing the data to make it stationary.\n3. <strong>MA (Moving Average)</strong>: Uses past forecast errors to improve predictions.</p>\n<p>ARIMA models are commonly used for time series forecasting when the data shows no strong seasonal component.</p>",
                    "front": "<p>What does ARIMA stand for?</p>"
                },
                {
                    "back": "<p>An ARIMA model is defined by three parameters:\n1. <strong>p</strong>: The number of autoregressive (AR) terms.\n2. <strong>d</strong>: The number of differences needed to make the series stationary.\n3. <strong>q</strong>: The number of moving average (MA) terms.</p>\n<p>The notation ARIMA(p, d, q) specifies how many terms are included for each component.</p>",
                    "front": "<p>What are the parameters of an ARIMA model?</p>"
                },
                {
                    "back": "<p><strong>SARIMA (Seasonal ARIMA)</strong> extends ARIMA by incorporating seasonality into the model. SARIMA includes additional parameters for seasonal autoregressive, seasonal differencing, and seasonal moving average components, denoted by (P, D, Q, m), where $m$ is the length of the seasonal cycle. The full SARIMA model is expressed as ARIMA(p, d, q)(P, D, Q)_m.</p>",
                    "front": "<p>What is the difference between ARIMA and SARIMA?</p>"
                }
            ],
            "name": "ARIMA Models"
        },
        {
            "cards": [
                {
                    "back": "<p>The key assumptions of linear regression are:\n1. <strong>Linearity</strong>: The relationship between the independent and dependent variables is linear.\n2. <strong>Independence</strong>: The observations are independent of each other.\n3. <strong>Homoscedasticity</strong>: The variance of residuals is constant across all levels of the independent variables.\n4. <strong>Normality</strong>: The residuals are normally distributed.\n5. <strong>No multicollinearity</strong> (for multiple regression): The independent variables are not highly correlated with each other.</p>",
                    "front": "<p>What are the key assumptions of linear regression?</p>"
                },
                {
                    "back": "<p><strong>Homoscedasticity</strong> refers to the assumption that the variance of the residuals is constant across all levels of the independent variables. In other words, the spread of the residuals should be roughly the same regardless of the value of the independent variables. A violation of this assumption can lead to inefficient estimates in the regression model.</p>",
                    "front": "<p>What is homoscedasticity?</p>"
                },
                {
                    "back": "<p>To check for normality of residuals, you can:\n1. <strong>Plot a histogram</strong> of the residuals: If the residuals are normally distributed, the histogram should resemble a bell curve.\n2. <strong>Use a Q-Q plot</strong>: In a Q-Q (quantile-quantile) plot, the points should fall along a straight line if the residuals are normally distributed.\n3. <strong>Perform a normality test</strong>: Tests like the Shapiro-Wilk test or Kolmogorov-Smirnov test can be used to assess the normality of residuals.</p>",
                    "front": "<p>How do you check for normality of residuals in a regression model?</p>"
                }
            ],
            "name": "Assumptions of Regression"
        },
        {
            "cards": [
                {
                    "back": "<p><strong>Autocorrelation</strong> is the correlation of a time series with a lagged version of itself. It measures how current values of the series relate to past values. Positive autocorrelation indicates that high values tend to follow high values, and negative autocorrelation indicates that high values follow low values.</p>",
                    "front": "<p>What is autocorrelation?</p>"
                },
                {
                    "back": "<p>The <strong>partial autocorrelation function (PACF)</strong> measures the correlation between the current time series value and its lagged values, controlling for the values of the time series at shorter lags. PACF helps identify the number of autoregressive (AR) terms needed in an ARIMA model.</p>",
                    "front": "<p>What is a partial autocorrelation function (PACF)?</p>"
                },
                {
                    "back": "<p><strong>ACF (Autocorrelation Function) plots</strong> show how the correlation between time series values decreases as the lag increases, while <strong>PACF plots</strong> show the partial correlation at each lag, adjusting for earlier lags. These plots are used to identify the appropriate parameters for ARIMA models.</p>",
                    "front": "<p>What are ACF and PACF plots used for?</p>"
                }
            ],
            "name": "Autocorrelation and Partial Autocorrelation"
        },
        {
            "cards": [
                {
                    "back": "<p>Bayes' Theorem is a fundamental formula in probability theory that allows you to update the probability of a hypothesis based on new evidence. It is written as:  </p>\n<p>$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$<br />\nwhere:\n- $P(A|B)$ is the <strong>posterior probability</strong> (the probability of event A occurring given that B is true),\n- $P(B|A)$ is the <strong>likelihood</strong> (the probability of event B occurring given that A is true),\n- $P(A)$ is the <strong>prior probability</strong> of A,\n- $P(B)$ is the <strong>marginal probability</strong> of B.</p>\n<p>Bayes' Theorem is widely used in fields such as medical testing, where it helps calculate the probability of a disease given a positive test result.</p>",
                    "front": "<p>What is Bayes' Theorem?</p>"
                },
                {
                    "back": "<p>Suppose a test for a certain disease has a 99% accuracy rate, meaning the probability of testing positive given the disease is 99% $P(Pos|Disease) = 0.99$. However, if the disease only affects 1% of the population, the <strong>prior probability</strong> of the disease $P(Disease) = 0.01$. If a person tests positive, Bayes' Theorem can be used to update the probability that they actually have the disease, factoring in the false positive rate and the disease prevalence.</p>\n<p>Bayes' Theorem in this case would provide a more accurate probability based on available data rather than assuming a positive test always means the presence of the disease.</p>",
                    "front": "<p>What is an example of applying Bayes' Theorem in medical testing?</p>"
                },
                {
                    "back": "<p>Classical probability often deals with frequencies of events, whereas Bayes' Theorem introduces the concept of <strong>updating</strong> a probability based on new evidence. Classical probability is static and does not consider past events, while Bayes' Theorem evolves with new data and is used to update <strong>beliefs</strong> or <strong>hypotheses</strong> dynamically.</p>",
                    "front": "<p>How does Bayes' Theorem differ from classical probability?</p>"
                }
            ],
            "name": "Bayes' Theorem"
        },
        {
            "cards": [
                {
                    "back": "<p>The <strong>coefficient of determination (R²)</strong> measures the proportion of the variance in the dependent variable that is</p>\n<p>explained by the independent variables in the model. It ranges from 0 to 1:\n- <strong>R² = 1</strong>: Perfect fit (the model explains 100% of the variance).\n- <strong>R² = 0</strong>: The model explains none of the variance.</p>",
                    "front": "<p>What is the coefficient of determination (R²)?</p>"
                },
                {
                    "back": "<ul>\n<li>A higher <strong>R²</strong> value indicates that a larger proportion of the variance in the dependent variable is explained by the independent variable(s).</li>\n<li>For example, an R² of 0.80 means that 80% of the variation in the dependent variable is explained by the independent variable(s), and 20% is unexplained.</li>\n</ul>",
                    "front": "<p>How do you interpret R²?</p>"
                },
                {
                    "back": "<p>While <strong>R²</strong> indicates the goodness of fit of a model, it has limitations:\n- <strong>R² increases</strong> as more variables are added to the model, even if those variables do not improve the model’s predictive power. This can lead to overfitting.\n- It does not indicate whether the independent variables are meaningful predictors or whether the model assumptions are met.</p>\n<p>For multiple regression, <strong>adjusted R²</strong> is used to address these limitations.</p>",
                    "front": "<p>What are the limitations of R²?</p>"
                }
            ],
            "name": "Coefficient of Determination (R²)"
        },
        {
            "cards": [
                {
                    "back": "<p>Conditional probability is the probability of an event occurring given that another event has already occurred. It helps in understanding how the probability of one event is affected by the presence or absence of another event. The formula for conditional probability is:  </p>\n<p>$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$<br />\nwhere $P(A|B)$ is the probability of A occurring given that B has occurred, and $P(A \\cap B)$ is the joint probability of A and B happening together.</p>",
                    "front": "<p>What is conditional probability?</p>"
                },
                {
                    "back": "<p>Consider the probability of it raining given that the sky is cloudy. The probability of rain (A) is influenced by the presence of clouds (B). The conditional probability $P(A|B)$ will likely be higher than the unconditional probability of rain, as clouds increase the likelihood of rain.</p>",
                    "front": "<p>What is an example of conditional probability in everyday life?</p>"
                },
                {
                    "back": "<p>If two events are <strong>independent</strong>, the occurrence of one does not affect the probability of the other, meaning:</p>\n<p>$$P(A|B) = P(A)$$  </p>\n<p>For independent events, the conditional probability is the same as the unconditional probability, as there is no relationship between A and B.</p>",
                    "front": "<p>How does conditional probability relate to independent events?</p>"
                },
                {
                    "back": "<p>For dependent events, the probability of multiple events occurring in sequence (e.g., A then B) is the product of the first event's probability and the conditional probability of the second event given the first:</p>\n<p>$$P(A \\cap B) = P(A) \\times P(B|A)$$  </p>\n<p>This formula accounts for the fact that the occurrence of A affects the probability of B.</p>",
                    "front": "<p>How do you calculate the probability of multiple dependent events?</p>"
                }
            ],
            "name": "Conditional Probability"
        },
        {
            "cards": [
                {
                    "back": "<p>A <strong>confidence interval</strong> is a range of values, derived from the sample data, that is likely to contain the true population parameter with a specified level of confidence. For example, a 95% confidence interval means that if the experiment were repeated many times, approximately 95% of the intervals would contain the true population parameter.</p>",
                    "front": "<p>What is a confidence interval?</p>"
                },
                {
                    "back": "<p>A 95% confidence interval indicates that you are 95% confident that the true population parameter lies within the calculated range. It does not mean that there is a 95% chance the parameter is within the interval; rather, it reflects the confidence in the method used to estimate the interval.</p>",
                    "front": "<p>How do you interpret a 95% confidence interval?</p>"
                },
                {
                    "back": "<p>The formula for a confidence interval for the population mean, assuming a normally distributed population and known standard deviation, is:</p>\n<p>$$\\text{CI} = \\bar{X} \\pm Z \\times \\frac{\\sigma}{\\sqrt{n}}$$</p>\n<p>where:\n- $\\bar{X}$ is the sample mean,\n- $Z$ is the critical value from the standard normal distribution corresponding to the desired confidence level (e.g., 1.96 for 95% confidence),\n- $\\sigma$ is the population standard deviation (or sample standard deviation if $\\sigma$ is unknown),\n- $n$ is the sample size.</p>",
                    "front": "<p>What is the formula for a confidence interval for the mean?</p>"
                },
                {
                    "back": "<p>The width of a confidence interval depends on:\n1. <strong>Sample size</strong>: Larger sample sizes result in narrower confidence intervals.\n2. <strong>Confidence level</strong>: Higher confidence levels (e.g., 99% vs. 95%) result in wider intervals.\n3. <strong>Standard deviation</strong>: Larger variability (standard deviation) increases the width of the confidence interval.</p>",
                    "front": "<p>What factors affect the width of a confidence interval?</p>"
                },
                {
                    "back": "<p>Confidence intervals and hypothesis testing are closely related. If the value specified in the null hypothesis lies outside the confidence interval, the null hypothesis can be rejected at the corresponding significance level. For example, if a 95% confidence interval for a mean does not include the null hypothesis value, you would reject the null hypothesis at $\\alpha = 0.05$.</p>",
                    "front": "<p>What is the relationship between confidence intervals and hypothesis testing?</p>"
                }
            ],
            "name": "Confidence Intervals"
        },
        {
            "cards": [
                {
                    "back": "<p><strong>Correlation</strong> is a statistical measure that describes the strength and direction of a linear relationship between two variables. It is quantified by the <strong>correlation coefficient</strong> (denoted by $r$), which ranges from -1 to 1:\n- <strong>$r = 1$</strong>: Perfect positive correlation.\n- <strong>$r = -1$</strong>: Perfect negative correlation.\n- <strong>$r = 0$</strong>: No linear correlation.</p>",
                    "front": "<p>What is correlation?</p>"
                },
                {
                    "back": "<p>A <strong>positive correlation</strong> occurs when an increase in one variable is associated with an increase in another variable. For example, in a dataset, if higher temperatures tend to correspond with higher ice cream sales, this indicates a positive correlation.</p>",
                    "front": "<p>What is a positive correlation?</p>"
                },
                {
                    "back": "<p>A <strong>negative correlation</strong> occurs when an increase in one variable is associated with a decrease in another variable. For instance, if higher temperatures lead to lower heating costs, this reflects a negative correlation.</p>",
                    "front": "<p>What is a negative correlation?</p>"
                },
                {
                    "back": "<p><strong>Correlation</strong> indicates that two variables are related, but it does not imply that one variable causes the other. <strong>Causation</strong>, on the other hand, means that changes in one variable directly cause changes in the other. Correlation does not establish causality without further analysis.</p>",
                    "front": "<p>What is the difference between correlation and causation?</p>"
                },
                {
                    "back": "<p>The <strong>Pearson correlation coefficient</strong> (denoted $r$) measures the strength and direction of a linear relationship between two variables:\n- <strong>$r = 1$</strong>: Perfect positive linear relationship.\n- <strong>$r = -1$</strong>: Perfect negative linear relationship.\n- <strong>$0.7 \\leq r &lt; 1$</strong>: Strong positive correlation.\n- <strong>$0.3 \\leq r &lt; 0.7$</strong>: Moderate positive correlation.\n- <strong>$0 &lt; r &lt; 0.3$</strong>: Weak positive correlation.\n- <strong>$r = 0$</strong>: No correlation.</p>\n<p>The same scale applies for negative values.</p>",
                    "front": "<p>How do you interpret the value of the Pearson correlation coefficient?</p>"
                }
            ],
            "name": "Correlation"
        },
        {
            "cards": [
                {
                    "back": "<p>A <strong>frequency distribution</strong> is a summary of how often different values or categories occur within a dataset. It can be visualized using tables or charts such as histograms, showing the frequency of each data point or category.</p>",
                    "front": "<p>What is a frequency distribution?</p>"
                },
                {
                    "back": "<p>A <strong>histogram</strong> is a type of bar chart that represents the distribution of numerical data. The data is divided into intervals, or bins, and the height of each bar represents the frequency of data points within that bin. Histograms are useful for identifying the shape of data distributions.</p>",
                    "front": "<p>What is a histogram?</p>"
                },
                {
                    "back": "<p><strong>Skewness</strong> refers to the asymmetry in the distribution of data. A distribution is said to be:\n- <strong>Positively skewed</strong> (right-skewed) if the tail on the right side is longer, indicating that there are more high outliers.\n- <strong>Negatively skewed</strong> (left-skewed) if the tail on the left side is longer, indicating that there are more low outliers.</p>\n<p>If the distribution is symmetrical, skewness is zero.</p>",
                    "front": "<p>What is skewness in a distribution?</p>"
                },
                {
                    "back": "<p><strong>Kurtosis</strong> measures the \"tailedness\" of a distribution, or the tendency of data points to cluster in the tails or near the mean. Distributions with high kurtosis have more data points in the tails (leptokurtic), while those with low kurtosis have fewer points in the tails (platykurtic). A normal distribution has a kurtosis of 3 (mesokurtic).</p>",
                    "front": "<p>What is kurtosis?</p>"
                }
            ],
            "name": "Data Distribution"
        },
        {
            "cards": [
                {
                    "back": "<p><strong>Mean Absolute Error (MAE)</strong> is the average of the absolute differences between forecasted and actual values. It gives a linear measure of the magnitude of the errors, without considering their direction. The formula is:</p>\n<p>$$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} | y_i - \\hat{y}_i |$$</p>\n<p>where $y_i$ are the actual values and $\\hat{y}_i$ are the forecasted values.</p>",
                    "front": "<p>What is Mean Absolute Error (MAE)?</p>"
                },
                {
                    "back": "<p><strong>Root Mean Squared Error (RMSE)</strong> is the square root of the average squared differences between forecasted and actual values. RMSE gives more weight to larger errors, making it sensitive to outliers. The formula is:</p>\n<p>$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$</p>",
                    "front": "<p>What is Root Mean Squared Error (RMSE)?</p>"
                },
                {
                    "back": "<p><strong>Mean Absolute Percentage Error (MAPE)</strong> expresses the accuracy of a forecast as a percentage, by taking the absolute difference between actual and forecasted values relative to the actual values. The formula is:</p>\n<p>$$\\text{MAPE} = \\frac{100}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right|$$</p>\n<p>MAPE is useful when comparing forecasting errors across different time series.</p>",
                    "front": "<p>What is Mean Absolute Percentage Error (MAPE)?</p>"
                },
                {
                    "back": "<p>The best model for time series forecasting is chosen based on several criteria, including:\n1. <strong>Low forecasting error</strong>: Measured using metrics like MAE, RMSE, or MAPE.\n2. <strong>Fit to the data</strong>: Evaluating how well the model captures the trend, seasonality, and residual components.\n3. <strong>Simplicity</strong>: A simpler model with good performance may be preferred over a more complex model.\n4. <strong>Domain knowledge</strong>: Understanding the context and characteristics of the data helps in choosing the right model (e.g., seasonal patterns).</p>",
                    "front": "<p>How do you choose the best model for time series forecasting?</p>"
                }
            ],
            "name": "Evaluation Metrics"
        },
        {
            "cards": [
                {
                    "back": "<p><strong>Time series forecasting</strong> involves predicting future values based on historical time series data. Techniques like <strong>ARIMA</strong>, <strong>SARIMA</strong>, <strong>Exponential Smoothing</strong>, and <strong>Prophet</strong> are commonly used to generate forecasts by modeling the trend, seasonality, and noise in the data.</p>",
                    "front": "<p>What is time series forecasting?</p>"
                },
                {
                    "back": "<p><strong>Exponential smoothing</strong> is a time series forecasting method that assigns exponentially decreasing weights to past observations. The most common forms are:\n1. <strong>Simple Exponential Smoothing (SES)</strong>: For series with no trend or seasonality.\n2. <strong>Holt’s Linear Trend Model</strong>: For series with a trend but no seasonality.\n3. <strong>Holt-Winters Exponential Smoothing</strong>: For series with both trend and seasonality.</p>",
                    "front": "<p>What is exponential smoothing?</p>"
                },
                {
                    "back": "<ul>\n<li><strong>ARIMA</strong> is preferred for non-seasonal data that requires differencing to achieve stationarity and for data with autocorrelated residuals.</li>\n<li><strong>Exponential Smoothing</strong> (especially Holt-Winters) is more suited to data with a clear trend and seasonal components that are not autocorrelated.</li>\n</ul>\n<p>The choice depends on the characteristics of the time series and the goal of the forecasting.</p>",
                    "front": "<p>When should you use ARIMA vs Exponential Smoothing for forecasting?</p>"
                }
            ],
            "name": "Forecasting"
        },
        {
            "cards": [
                {
                    "back": "<p>A <strong>bar plot</strong> is a graphical representation of categorical data where the length of each bar corresponds to the frequency or count of each category. Bar plots are useful for comparing different categories or groups within a dataset.</p>",
                    "front": "<p>What is a bar plot?</p>"
                },
                {
                    "back": "<p>A <strong>box plot</strong> (or box-and-whisker plot) is a graphical summary that shows the distribution of a dataset based on five summary statistics: minimum, first quartile (Q1), median, third quartile (Q3), and maximum. The \"whiskers\" extend to the smallest and largest values, and any points outside this range may be considered outliers.</p>",
                    "front": "<p>What is a box plot?</p>"
                },
                {
                    "back": "<p>A <strong>histogram</strong> helps visualize the distribution of numerical data by showing the frequency of data points within predefined intervals or bins. It provides insight into the shape, center, and spread of the data, as well as whether the data is skewed or symmetric.</p>",
                    "front": "<p>What is the purpose of using a histogram?</p>"
                }
            ],
            "name": "Graphical Summaries"
        },
        {
            "cards": [
                {
                    "back": "<p>The <strong>null hypothesis</strong> (denoted $H_0$) is a statement that there is no effect or no difference, and it serves as the default assumption in hypothesis testing. It is the hypothesis that researchers aim to test or reject. For example, in a test comparing two means, the null hypothesis might state that the means are equal:</p>\n<p>$$H_0: \\mu_1 = \\mu_2$$</p>",
                    "front": "<p>What is a null hypothesis?</p>"
                },
                {
                    "back": "<p>The <strong>alternative hypothesis</strong> (denoted $H_a$ or $H_1$) is the statement that contradicts the null hypothesis. It proposes that there is an effect or a difference. For example, in the same test comparing two means, the alternative hypothesis might state:</p>\n<p>$$H_a: \\mu_1 \\neq \\mu_2$$</p>\n<p>The alternative hypothesis is what researchers aim to support through evidence.</p>",
                    "front": "<p>What is an alternative hypothesis?</p>"
                },
                {
                    "back": "<p>The basic steps in hypothesis testing are:\n1. <strong>State the null and alternative hypotheses</strong>.\n2. <strong>Choose a significance level</strong> ($\\alpha$), commonly set at 0.05.\n3. <strong>Select the appropriate test</strong> (e.g., t-test, z-test, chi-square test).\n4. <strong>Calculate the test statistic</strong> based on the sample data.\n5. <strong>Determine the p-value</strong> or critical value for the test statistic.\n6. <strong>Make a decision</strong>: Reject or fail to reject the null hypothesis based on the comparison of the p-value and the significance level.\n7. <strong>Interpret the results</strong> in the context of the original research question.</p>",
                    "front": "<p>What are the steps in hypothesis testing?</p>"
                },
                {
                    "back": "<p>The null hypothesis is rejected if the p-value is less than or equal to the significance level ($\\alpha$), indicating that the observed data is unlikely to occur if the null hypothesis were true. For example, if $\\alpha = 0.05$ and the p-value is 0.03, you would reject the null hypothesis.</p>",
                    "front": "<p>When should you reject the null hypothesis?</p>"
                },
                {
                    "back": "<p>A <strong>one-tailed test</strong> is a hypothesis test where the alternative hypothesis specifies a direction of the effect (e.g., greater than or less than). It tests whether the parameter is either larger or smaller than the null hypothesis value, but not both. For example:</p>\n<p>$$H_a: \\mu_1 &gt; \\mu_2$$</p>\n<p>This test is used when the research question has a specific directional expectation.</p>",
                    "front": "<p>What is a one-tailed test?</p>"
                },
                {
                    "back": "<p>A <strong>two-tailed test</strong> is a hypothesis test where the alternative hypothesis does not specify a direction, meaning it checks for any difference from the null hypothesis value in either direction. For example:</p>\n<p>$$H_a: \\mu_1 \\neq \\mu_2$$</p>\n<p>It tests for both possibilities (greater or less), and is used when there is no specific directional assumption in the research.</p>",
                    "front": "<p>What is a two-tailed test?</p>"
                }
            ],
            "name": "Hypothesis Testing"
        },
        {
            "cards": [
                {
                    "back": "<p>Two events are independent if the occurrence of one does not affect the probability of the other. Mathematically, events A and B are independent if:</p>\n<p>$$P(A \\cap B) = P(A) \\times P(B)$$  </p>\n<p>In this case, knowing that event B has occurred gives no additional information about the likelihood of A occurring, and vice versa.</p>",
                    "front": "<p>What does it mean for two events to be independent?</p>"
                },
                {
                    "back": "<p>To test for independence, check if the joint probability $P(A \\cap B)$ equals the product of the individual probabilities $P(A) \\times P(B)$. If the equality holds, the events are independent; otherwise, they are dependent.</p>",
                    "front": "<p>How can you test if two events are independent?</p>"
                }
            ],
            "name": "Independence"
        },
        {
            "cards": [
                {
                    "back": "<p>Joint probability is the probability that two or more events occur together. For two events A and B, the joint probability $P(A \\cap B)$ represents the likelihood that both A and B happen. If the events are <strong>independent</strong>, the joint probability is the product of their individual probabilities:</p>\n<p>$$P(A \\cap B) = P(A) \\times P(B)$$</p>",
                    "front": "<p>What is joint probability?</p>"
                },
                {
                    "back": "<p>Suppose you are rolling two dice. The probability of rolling a 6 on the first die and a 6 on the second die is the joint probability $P(6 \\cap 6)$. Since these events are independent, you can calculate this by multiplying the probabilities:</p>\n<p>$$P(6 \\cap 6) = \\frac{1}{6} \\times \\frac{1}{6} = \\frac{1}{36}$$</p>",
                    "front": "<p>What is an example of joint probability?</p>"
                },
                {
                    "back": "<p>For dependent events, the joint probability is not simply the product of individual probabilities. Instead, you must use the conditional probability formula:</p>\n<p>$$P(A \\cap B) = P(A) \\times P(B|A)$$  </p>\n<p>This accounts for the fact that the occurrence of event A influences the probability of event B.</p>",
                    "front": "<p>How does joint probability differ for dependent events?</p>"
                }
            ],
            "name": "Joint Probability"
        },
        {
            "cards": [
                {
                    "back": "<p>The Law of Total Probability states that if you have a set of mutually exclusive events $B_1, B_2, ..., B_n$ that cover all possible outcomes, the probability of any event A can be expressed as:</p>\n<p>$$P(A) = \\sum_{i} P(A|B_i) P(B_i)$$</p>\n<p>This law is useful for breaking down complex probability problems by considering all possible conditions.</p>",
                    "front": "<p>What is the Law of Total Probability?</p>"
                }
            ],
            "name": "Law of Total Probability"
        },
        {
            "cards": [
                {
                    "back": "<p><strong>Simple linear regression</strong> is a statistical method used to model the relationship between two variables: one <strong>independent variable</strong> (predictor) and one <strong>dependent variable</strong> (outcome). The goal is to find the best-fitting straight line (regression line) that minimizes the distance between observed values and predicted values. The equation of the regression line is:</p>\n<p>$$Y = \\beta_0 + \\beta_1 X + \\epsilon$$</p>\n<p>where $Y$ is the dependent variable, $X$ is the independent variable, $\\beta_0$ is the intercept, $\\beta_1$ is the slope, and $\\epsilon$ is the error term.</p>",
                    "front": "<p>What is simple linear regression?</p>"
                },
                {
                    "back": "<ul>\n<li>The <strong>slope</strong> ($\\beta_1$) represents the change in the dependent variable ($Y$) for every one-unit increase in the independent variable ($X$). If $\\beta_1$ is positive, $Y$ increases as $X$ increases; if $\\beta_1$ is negative, $Y$ decreases as $X$ increases.</li>\n<li>The <strong>intercept</strong> ($\\beta_0$) is the predicted value of $Y$ when $X = 0$. It indicates the starting point of the regression line on the Y-axis.</li>\n</ul>",
                    "front": "<p>How do you interpret the slope and intercept in linear regression?</p>"
                },
                {
                    "back": "<p>A <strong>residual</strong> is the difference between an observed value and the value predicted by the regression model. It is calculated as:</p>\n<p>$$\\text{Residual} = Y_{\\text{observed}} - Y_{\\text{predicted}}$$</p>\n<p>Residuals are used to evaluate the fit of the model, with smaller residuals indicating a better fit.</p>",
                    "front": "<p>What is the residual in a regression model?</p>"
                },
                {
                    "back": "<p>The <strong>least squares method</strong> aims to minimize the sum of the squared residuals (the differences between observed and predicted values). By minimizing this quantity, the regression line is optimized to fit the data as closely as possible.</p>",
                    "front": "<p>What is the objective of the least squares method in linear regression?</p>"
                }
            ],
            "name": "Linear Regression"
        },
        {
            "cards": [
                {
                    "back": "<p><strong>Logistic regression</strong> is used when the dependent variable is binary (e.g., 0 or 1, yes or no). It models the probability that a certain event will occur, based on one or more independent variables. The logistic regression model uses the <strong>logit function</strong> to map predicted values to a probability between 0 and 1:</p>\n<p>$$\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_n X_n$$</p>\n<p>where $p$ is the probability of the event occurring.</p>",
                    "front": "<p>What is logistic regression?</p>"
                },
                {
                    "back": "<p>In <strong>logistic regression</strong>, the coefficients ($\\beta$) represent the change in the <strong>log-odds</strong> of the dependent variable occurring for a one-unit change in the independent variable. The exponentiated coefficient ($e^{\\beta}$) gives the <strong>odds ratio</strong>, which can be interpreted as the change in the odds of the event occurring for a one-unit increase in the independent variable.</p>",
                    "front": "<p>How do you interpret coefficients in logistic regression?</p>"
                },
                {
                    "back": "<ul>\n<li><strong>Linear regression</strong> is used to predict a continuous dependent variable, while <strong>logistic regression</strong> is used to predict a binary outcome.</li>\n<li>In linear regression, the relationship between the independent and dependent variables is linear, whereas logistic regression models the probability of an event occurring using the logit function, producing outputs between 0 and 1.</li>\n</ul>",
                    "front": "<p>What is the difference between linear and logistic regression?</p>"
                }
            ],
            "name": "Logistic Regression"
        },
        {
            "cards": [
                {
                    "back": "<p>Marginal probability refers to the probability of a single event occurring, without regard to other events. It is derived from the joint probabilities by summing over all possible outcomes for other variables. For example, the marginal probability of A can be written as:</p>\n<p>$$P(A) = \\sum_{B} P(A \\cap B)$$</p>",
                    "front": "<p>What is marginal probability?</p>"
                },
                {
                    "back": "<p>Marginal probability is the result of <strong>summing</strong> or <strong>integrating</strong> over the joint probabilities of related events. For example, if A and B are two events, the marginal probability of A is obtained by summing the joint probabilities of A occurring with each possible outcome of B.</p>",
                    "front": "<p>How does marginal probability relate to joint probability?</p>"
                }
            ],
            "name": "Marginal Probability"
        },
        {
            "cards": [
                {
                    "back": "<p>The <strong>mean</strong> is the sum of all data values divided by the number of data points. It is the most commonly used measure of central tendency and represents the average of a dataset. The formula for the mean is:</p>\n<p>$$\\text{Mean} = \\frac{\\sum X_i}{n}$$</p>\n<p>where $X_i$ are the individual data points, and $n$ is the number of data points.</p>",
                    "front": "<p>What is the mean?</p>"
                },
                {
                    "back": "<p>The mean can be heavily influenced by outliers, or extremely high or low values, which can distort the representation of the central tendency. Additionally, the mean is not always appropriate for skewed data distributions, where the median may be a better measure.</p>",
                    "front": "<p>What are some limitations of using the mean?</p>"
                },
                {
                    "back": "<p>The <strong>median</strong> is the middle value of a dataset when the data is ordered from smallest to largest. If there is an even number of data points, the median is the average of the two middle values. Unlike the mean, the median is less affected by outliers and skewed data.</p>",
                    "front": "<p>What is the median?</p>"
                },
                {
                    "back": "<p>The <strong>mode</strong> is the value that appears most frequently in a dataset. A dataset can have no mode, one mode (unimodal), or multiple modes (bimodal, multimodal) if there are several values that occur with the same highest frequency.</p>",
                    "front": "<p>How do you calculate the mode?</p>"
                }
            ],
            "name": "Measures of Central Tendency"
        },
        {
            "cards": [
                {
                    "back": "<p>The <strong>range</strong> is the difference between the largest and smallest values in a dataset. It provides a simple measure of data dispersion, though it can be heavily influenced by outliers. The formula for the range is:</p>\n<p>$$\\text{Range} = \\text{Max} - \\text{Min}$$</p>",
                    "front": "<p>What is range?</p>"
                },
                {
                    "back": "<p><strong>Variance</strong> measures the average squared deviation of each data point from the mean. It quantifies how spread out the data points are. The formula for variance is:</p>\n<p>$$\\text{Variance} (\\sigma^2) = \\frac{\\sum (X_i - \\mu)^2}{n}$$</p>\n<p>where $X_i$ are the data points, $\\mu$ is the mean, and $n$ is the number of data points.</p>",
                    "front": "<p>How do you calculate variance?</p>"
                },
                {
                    "back": "<p><strong>Standard deviation</strong> is the square root of the variance. It is a widely used measure of dispersion that provides insight into the average distance of data points from the mean. A smaller standard deviation indicates that data points are clustered close to the mean, while a larger standard deviation suggests greater spread.</p>\n<p>The formula is:</p>\n<p>$$\\text{Standard Deviation} (\\sigma) = \\sqrt{\\frac{\\sum (X_i - \\mu)^2}{n}}$$</p>",
                    "front": "<p>What is standard deviation?</p>"
                },
                {
                    "back": "<p>Standard deviation is often preferred over variance because it is in the same units as the original data, making it easier to interpret. Variance, on the other hand, is in squared units, which can make the measure less intuitive.</p>",
                    "front": "<p>Why is standard deviation preferred over variance?</p>"
                }
            ],
            "name": "Measures of Dispersion"
        },
        {
            "cards": [
                {
                    "back": "<p>A <strong>simple moving average (SMA)</strong> is the unweighted mean of a fixed number of past observations. It smooths out fluctuations in a time series by averaging over a sliding window of previous values, making it easier to identify trends.</p>\n<p>The formula is:</p>\n<p>$$\\text{SMA}<em i=\"t-n+1\">t = \\frac{1}{n} \\sum</em>^{t} X_i$$</p>\n<p>where $n$ is the window size.</p>",
                    "front": "<p>What is a simple moving average?</p>"
                },
                {
                    "back": "<p>An <strong>exponential moving average (EMA)</strong> gives more weight to recent observations compared to older ones, making it more responsive to changes in the time series. The EMA is calculated recursively, using a smoothing factor that controls how much weight is given to recent data points.</p>",
                    "front": "<p>What is an exponential moving average (EMA)?</p>"
                },
                {
                    "back": "<p><strong>Moving averages</strong> smooth out short-term fluctuations in time series data, making it easier to identify underlying trends and seasonality. In forecasting, they are often used to estimate future values by extrapolating current trends or to remove noise from the data.</p>",
                    "front": "<p>How are moving averages used in time series forecasting?</p>"
                }
            ],
            "name": "Moving Averages"
        },
        {
            "cards": [
                {
                    "back": "<p><strong>Multiple linear regression</strong> is an extension of simple linear regression that models the relationship between a dependent variable and two or more independent variables. The regression equation is:</p>\n<p>$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n + \\epsilon$$</p>\n<p>where $X_1, X_2, ..., X_n$ are the independent variables, and $\\beta_1, \\beta_2, ..., \\beta_n$ are their respective coefficients.</p>",
                    "front": "<p>What is multiple linear regression?</p>"
                },
                {
                    "back": "<p><strong>Multicollinearity</strong> occurs when two or more independent variables in a multiple regression model are highly correlated with each other. This can make it difficult to determine the individual effect of each variable on the dependent variable, leading to inflated standard errors and unreliable coefficient estimates.</p>",
                    "front": "<p>What does multicollinearity mean in multiple regression?</p>"
                },
                {
                    "back": "<p>Multicollinearity can be detected using several methods:\n1. <strong>Variance Inflation Factor (VIF)</strong>: A VIF value greater than 10 indicates high multicollinearity.\n2. <strong>Correlation matrix</strong>: High correlations between independent variables suggest multicollinearity.\n3. <strong>Eigenvalues</strong>: Small eigenvalues of the correlation matrix can indicate multicollinearity.</p>",
                    "front": "<p>How can you detect multicollinearity?</p>"
                },
                {
                    "back": "<p>The <strong>adjusted R²</strong> is a modified version of the R² that accounts for the number of independent variables in the model. It adjusts for the fact that adding more variables can artificially increase the R², even if those variables do not improve the model. Adjusted R² provides a more accurate measure of the model's explanatory power.</p>",
                    "front": "<p>What is the adjusted R² in multiple regression?</p>"
                }
            ],
            "name": "Multiple Regression"
        },
        {
            "cards": [
                {
                    "back": "<p>An <strong>outlier</strong> is a data point that differs significantly from the rest of the data. Outliers can be caused by measurement errors, variability in the data, or rare occurrences. They can have a significant impact on statistical measures such as the mean and variance.</p>",
                    "front": "<p>What is an outlier?</p>"
                },
                {
                    "back": "<p>Outliers can be identified using various methods:\n- <strong>Z-score method</strong>: Data points with Z-scores greater than 3 or less than -3 are often considered outliers.\n- <strong>IQR method</strong>: Data points that are below $Q1 - 1.5 \\times \\text{IQR}$ or above $Q3 + 1.5 \\times \\text{IQR}$ are considered outliers.</p>",
                    "front": "<p>How do you identify outliers?</p>"
                },
                {
                    "back": "<p>Outliers can skew the results of data analysis by inflating or deflating measures like the mean and standard deviation. It is important to identify and address outliers, as they may represent errors or rare but significant data points.</p>",
                    "front": "<p>What is the impact</p>\n<p>of outliers on data analysis?</p>"
                }
            ],
            "name": "Outliers"
        },
        {
            "cards": [
                {
                    "back": "<p>A <strong>p-value</strong> is the probability of obtaining results as extreme as, or more extreme than, the observed data under the assumption that the null hypothesis is true. It quantifies the strength of evidence against the null hypothesis. A smaller p-value indicates stronger evidence to reject the null hypothesis.</p>",
                    "front": "<p>What is a p-value?</p>"
                },
                {
                    "back": "<ul>\n<li>If the <strong>p-value</strong> is less than or equal to the significance level ($\\alpha$), you reject the null hypothesis.</li>\n<li>If the p-value is greater than $\\alpha$, you fail to reject the null hypothesis.</li>\n</ul>\n<p>For example, with $\\alpha = 0.05$, a p-value of 0.02 suggests significant evidence against the null hypothesis.</p>",
                    "front": "<p>How do you interpret a p-value?</p>"
                },
                {
                    "back": "<p>Common misconceptions include:\n- A <strong>p-value</strong> does not measure the probability that the null hypothesis is true.\n- A <strong>p-value</strong> does not indicate the size or importance of an effect.\n- A <strong>p-value</strong> greater than 0.05 does not \"prove\" that the null hypothesis is true; it only suggests insufficient evidence to reject it.</p>",
                    "front": "<p>What are some common misconceptions about p-values?</p>"
                }
            ],
            "name": "P-values"
        },
        {
            "cards": [
                {
                    "back": "<p>A <strong>percentile</strong> is a measure that indicates the value below which a given percentage of data points fall. For example, the 90th percentile is the value below which 90% of the data points lie. Percentiles are useful for comparing individual data points to the overall dataset.</p>",
                    "front": "<p>What is a percentile?</p>"
                },
                {
                    "back": "<p><strong>Quartiles</strong> divide a dataset into four equal parts, each containing 25% of the data. The <strong>first quartile (Q1)</strong> is the 25th percentile, the <strong>second quartile (Q2)</strong> is the median or 50th percentile, and the <strong>third quartile (Q3)</strong> is the 75th percentile. Quartiles help in understanding the spread and distribution of data.</p>",
                    "front": "<p>How do you calculate quartiles?</p>"
                },
                {
                    "back": "<p>The <strong>interquartile range (IQR)</strong> is the range between the first quartile (Q1) and the third quartile (Q3). It measures the spread of the middle 50% of the data and is a useful measure of dispersion that is not affected by outliers. The formula is:</p>\n<p>$$\\text{IQR} = Q3 - Q1$$</p>",
                    "front": "<p>What is the interquartile range (IQR)?</p>"
                }
            ],
            "name": "Percentiles and Quartiles"
        },
        {
            "cards": [
                {
                    "back": "<p>The <strong>power</strong> of a test is the probability that the test correctly rejects the null hypothesis when it is false (i.e., it avoids a Type II error). Power is calculated as $1 - \\beta$, where $\\beta$ is the probability of a Type II error. High power means the test is more likely to detect an effect if it exists.</p>",
                    "front": "<p>What is the power of a test?</p>"
                },
                {
                    "back": "<p>Factors that affect the power of a test include:\n1. <strong>Sample size</strong>: Larger sample sizes increase power.\n2. <strong>Effect size</strong>: Larger effects are easier to detect, increasing power.\n3. <strong>Significance level ($\\alpha$)</strong>: A higher $\\alpha$ increases power but also increases the chance of a Type I error.\n4. <strong>Variance</strong>: Lower variability in the data increases power.</p>",
                    "front": "<p>What factors affect the power of a test?</p>"
                },
                {
                    "back": "<p>The power of a test is important because it determines the likelihood of detecting an effect if one truly exists. Low-power tests have a higher risk of producing false negatives (Type II errors), potentially missing important findings. Power analysis is often conducted before an experiment to ensure sufficient sample size.</p>",
                    "front": "<p>Why is the power of a test important?</p>"
                }
            ],
            "name": "Power of a Test"
        },
        {
            "cards": [
                {
                    "back": "<p><strong>Residual analysis</strong> involves examining the residuals (the differences between observed and predicted values) to evaluate the fit of a regression model. Residual analysis helps in identifying violations of assumptions like linearity, homoscedasticity, and normality, and can uncover outliers or influential data points.</p>",
                    "front": "<p>What is residual analysis in regression?</p>"
                },
                {
                    "back": "<p><strong>Heteroscedasticity</strong> occurs when the variance of the residuals is not constant. To detect heteroscedasticity, you can:\n1. <strong>Plot residuals vs. predicted values</strong>: If the spread of residuals increases or decreases as the predicted values change, heteroscedasticity may be present.\n2. <strong>Breusch-Pagan test</strong>: A statistical test that can be used to detect heteroscedasticity.\n3. <strong>White’s test</strong>: Another test for heteroscedasticity that does not assume a specific form for the relationship between the residual variance and predictors.</p>",
                    "front": "<p>How do you detect heteroscedasticity in residuals?</p>"
                }
            ],
            "name": "Residual Analysis"
        },
        {
            "cards": [
                {
                    "back": "<p><strong>Seasonal decomposition</strong> breaks down a time series into its main components: <strong>trend</strong>, <strong>seasonality</strong>, and <strong>residuals</strong> (random fluctuations). Methods like <strong>STL (Seasonal-Trend Decomposition using Loess)</strong> are commonly used to separate these components, making it easier to understand and model the series.</p>",
                    "front": "<p>What is seasonal decomposition of time series?</p>"
                },
                {
                    "back": "<p>Decomposing a time series allows you to better understand its structure by separating the trend, seasonal, and noise components. This is useful for improving forecasts and identifying underlying patterns, especially when the series has complex seasonal effects.</p>",
                    "front": "<p>Why is it important to decompose a time series?</p>"
                }
            ],
            "name": "Seasonal Decomposition"
        },
        {
            "cards": [
                {
                    "back": "<p>The <strong>significance level</strong> (denoted $\\alpha$) is the threshold used to decide whether to reject the null hypothesis. It represents the probability of making a Type I error (rejecting a true null hypothesis). Common significance levels include 0.05, 0.01, and 0.10.</p>",
                    "front": "<p>What is a significance level?</p>"
                },
                {
                    "back": "<p>The choice of <strong>significance level</strong> depends on the context of the research:\n- In fields like medicine, a lower $\\alpha$ (e.g., 0.01) may be used to minimize the risk of Type I errors (false positives).\n- In exploratory research, a higher $\\alpha$ (e.g., 0.10) may be acceptable to balance the risk of Type II errors (false negatives).</p>",
                    "front": "<p>How do you choose an appropriate significance level?</p>"
                }
            ],
            "name": "Significance Levels"
        },
        {
            "cards": [
                {
                    "back": "<p>A <strong>stationary time series</strong> is one where the statistical properties (mean, variance, and autocorrelation) are constant over time. A stationary series does not exhibit trends or seasonality and is essential for many time series models, including ARIMA.</p>",
                    "front": "<p>What is a stationary time series?</p>"
                },
                {
                    "back": "<p>The <strong>Augmented Dickey-Fuller (ADF) test</strong> is commonly used to test for stationarity. If the p-value of the test is less than a chosen significance level (e.g., 0.05), the null hypothesis (that the series is non-stationary) can be rejected, indicating that the series is stationary.</p>",
                    "front": "<p>How do you test for stationarity?</p>"
                },
                {
                    "back": "<p>To make a time series stationary, you can:\n1. <strong>Differencing</strong>: Subtracting each observation from the previous one to remove trends.\n2. <strong>De-trending</strong>: Removing the trend component.\n3. <strong>Log transformation</strong>: Reducing variance by applying a logarithmic transformation to the data.\n4. <strong>Seasonal differencing</strong>: Subtracting observations from the corresponding value in the previous season to remove seasonality.</p>",
                    "front": "<p>How do you make a time series stationary?</p>"
                }
            ],
            "name": "Stationarity"
        },
        {
            "cards": [
                {
                    "back": "<p>A time series can be decomposed into three main components:\n1. <strong>Trend</strong>: The long-term direction of the series (upward, downward, or flat).\n2. <strong>Seasonality</strong>: Regular, repeating patterns that occur over specific intervals (e.g., quarterly sales increases).\n3. <strong>Residual (Noise)</strong>: Random fluctuations that are not explained by trend or seasonality.</p>",
                    "front": "<p>What are the main components of a time series?</p>"
                },
                {
                    "back": "<p>An example of seasonality would be retail sales that peak every December due to holiday shopping. This repeating annual pattern in sales data is a typical seasonal component.</p>",
                    "front": "<p>What is an example of seasonality in time series data?</p>"
                },
                {
                    "back": "<p>A <strong>trend</strong> represents the long-term movement in the data over time. It can be upward (increasing values), downward (decreasing values), or stationary (no consistent change). For example, a rising trend in stock prices over several years indicates an overall upward movement despite short-term fluctuations.</p>",
                    "front": "<p>How can a trend affect a time series?</p>"
                }
            ],
            "name": "Time Series Components"
        },
        {
            "cards": [
                {
                    "back": "<p>A <strong>Type I error</strong> occurs when the null hypothesis is rejected when it is actually true. This is also known as a \"false positive\" or rejecting a true null hypothesis. The probability of making a Type I error is denoted by $\\alpha$, which is the significance level of the test (e.g., $\\alpha = 0.05$ means a 5% chance of a Type I error).</p>",
                    "front": "<p>What is a Type I error?</p>"
                },
                {
                    "back": "<p>A <strong>Type II error</strong> occurs when the null hypothesis is not rejected when it is actually false. This is also known as a \"false negative\" or failing to reject a false null hypothesis. The probability of making a Type II error is denoted by $\\beta$.</p>",
                    "front": "<p>What is a Type II error?</p>"
                },
                {
                    "back": "<p>There is an inverse relationship between Type I and Type II errors. Reducing the risk of a Type I error (lowering $\\alpha$) increases the risk of a Type II error ($\\beta$), and vice versa. Balancing the two types of errors often involves choosing an appropriate significance level based on the context of the research.</p>",
                    "front": "<p>What is the relationship between Type I and Type II errors?</p>"
                },
                {
                    "back": "<p>You can reduce Type I and Type II errors by:\n1. <strong>Increasing the sample size</strong>: This increases the power of the test, reducing $\\beta$.\n2. <strong>Adjusting the significance level</strong>: Lowering $\\alpha$ reduces Type I errors, though it may increase Type II errors.\n3. <strong>Improving the precision of measurements</strong>: Better data collection reduces variability, which can lower both types of errors.</p>",
                    "front": "<p>How can you reduce Type I and Type II errors?</p>"
                }
            ],
            "name": "Type I and Type II Errors"
        },
        {
            "cards": [
                {
                    "back": "<p>A <strong>Z-score</strong> represents how many standard deviations a data point is from the mean of a dataset. It is used to standardize data, allowing for comparisons across different datasets or variables with different units. The formula is:</p>\n<p>$$Z = \\frac{X - \\mu}{\\sigma}$$</p>\n<p>where $X$ is the data point, $\\mu$ is the mean, and $\\sigma$ is the standard deviation.</p>",
                    "front": "<p>What is a Z-score?</p>"
                },
                {
                    "back": "<p>Z-scores are useful for standardizing data, which allows for comparison between data points from different distributions. For example, you can use Z-scores to compare exam scores across different subjects, even if the exams have different means and standard deviations.</p>",
                    "front": "<p>Why are Z-scores useful?</p>"
                },
                {
                    "back": "<p>A <strong>positive Z-score</strong> indicates that the data point is above the mean, while a <strong>negative Z-score</strong> indicates that the data point is below the mean. A Z-score of 0 means that the data point is exactly at the mean.</p>",
                    "front": "<p>What does a positive or negative Z-score indicate?</p>"
                }
            ],
            "name": "Z-scores"
        }
    ]
}