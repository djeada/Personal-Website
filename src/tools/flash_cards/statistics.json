{
    "category": "Statistics",
    "subcategories": [
        {
            "name": "Bayes' Theorem",
            "cards": [
                {
                    "front": "What is Bayes' Theorem?",
                    "back": "Bayes' Theorem is a fundamental formula in probability theory that allows you to update the probability of a hypothesis based on new evidence. It is written as: \\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\] where: - $P(A|B)$ is the <b>posterior probability</b> (the probability of event A occurring given that B is true), - $P(B|A)$ is the <b>likelihood</b> (the probability of event B occurring given that A is true), - $P(A)$ is the <b>prior probability</b> of A, - $P(B)$ is the <b>marginal probability</b> of B. Bayes' Theorem is widely used in fields such as medical testing, where it helps calculate the probability of a disease given a positive test result."
                },
                {
                    "front": "What is an example of applying Bayes' Theorem in medical testing?",
                    "back": "Suppose a test for a certain disease has a 99% accuracy rate, meaning the probability of testing positive given the disease is 99% $P(Pos|Disease) = 0.99$. However, if the disease only affects 1% of the population, the <b>prior probability</b> of the disease $P(Disease) = 0.01$. If a person tests positive, Bayes' Theorem can be used to update the probability that they actually have the disease, factoring in the false positive rate and the disease prevalence. Bayes' Theorem in this case would provide a more accurate probability based on available data rather than assuming a positive test always means the presence of the disease."
                },
                {
                    "front": "How does Bayes' Theorem differ from classical probability?",
                    "back": "Classical probability often deals with frequencies of events, whereas Bayes' Theorem introduces the concept of <b>updating</b> a probability based on new evidence. Classical probability is static and does not consider past events, while Bayes' Theorem evolves with new data and is used to update <b>beliefs</b> or <b>hypotheses</b> dynamically."
                }
            ]
        },
        {
            "name": "Conditional Probability",
            "cards": [
                {
                    "front": "What is conditional probability?",
                    "back": "Conditional probability is the probability of an event occurring given that another event has already occurred. It helps in understanding how the probability of one event is affected by the presence or absence of another event. The formula for conditional probability is: \\[ P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\] where $P(A|B)$ is the probability of A occurring given that B has occurred, and $P(A \\cap B)$ is the joint probability of A and B happening together."
                },
                {
                    "front": "What is an example of conditional probability in everyday life?",
                    "back": "Consider the probability of it raining given that the sky is cloudy. The probability of rain (A) is influenced by the presence of clouds (B). The conditional probability $P(A|B)$ will likely be higher than the unconditional probability of rain, as clouds increase the likelihood of rain."
                },
                {
                    "front": "How does conditional probability relate to independent events?",
                    "back": "If two events are <b>independent</b>, the occurrence of one does not affect the probability of the other, meaning: \\[ P(A|B) = P(A) \\] For independent events, the conditional probability is the same as the unconditional probability, as there is no relationship between A and B."
                },
                {
                    "front": "How do you calculate the probability of multiple dependent events?",
                    "back": "For dependent events, the probability of multiple events occurring in sequence (e.g., A then B) is the product of the first event's probability and the conditional probability of the second event given the first: \\[ P(A \\cap B) = P(A) \\times P(B|A) \\] This formula accounts for the fact that the occurrence of A affects the probability of B."
                }
            ]
        },
        {
            "name": "Joint Probability",
            "cards": [
                {
                    "front": "What is joint probability?",
                    "back": "Joint probability is the probability that two or more events occur together. For two events A and B, the joint probability $P(A \\cap B)$ represents the likelihood that both A and B happen. If the events are <b>independent</b>, the joint probability is the product of their individual probabilities: \\[ P(A \\cap B) = P(A) \\times P(B) \\]"
                },
                {
                    "front": "What is an example of joint probability?",
                    "back": "Suppose you are rolling two dice. The probability of rolling a 6 on the first die and a 6 on the second die is the joint probability $P(6 \\cap 6)$. Since these events are independent, you can calculate this by multiplying the probabilities: \\[ P(6 \\cap 6) = \\frac{1}{6} \\times \\frac{1}{6} = \\frac{1}{36} \\]"
                },
                {
                    "front": "How does joint probability differ for dependent events?",
                    "back": "For dependent events, the joint probability is not simply the product of individual probabilities. Instead, you must use the conditional probability formula: \\[ P(A \\cap B) = P(A) \\times P(B|A) \\] This accounts for the fact that the occurrence of event A influences the probability of event B."
                }
            ]
        },
        {
            "name": "Marginal Probability",
            "cards": [
                {
                    "front": "What is marginal probability?",
                    "back": "Marginal probability refers to the probability of a single event occurring, without regard to other events. It is derived from the joint probabilities by summing over all possible outcomes for other variables. For example, the marginal probability of A can be written as: \\[ P(A) = \\sum_{B} P(A \\cap B) \\]"
                },
                {
                    "front": "How does marginal probability relate to joint probability?",
                    "back": "Marginal probability is the result of <b>summing</b> or <b>integrating</b> over the joint probabilities of related events. For example, if A and B are two events, the marginal probability of A is obtained by summing the joint probabilities of A occurring with each possible outcome of B."
                }
            ]
        },
        {
            "name": "Independence",
            "cards": [
                {
                    "front": "What does it mean for two events to be independent?",
                    "back": "Two events are independent if the occurrence of one does not affect the probability of the other. Mathematically, events A and B are independent if: \\[ P(A \\cap B) = P(A) \\times P(B) \\] In this case, knowing that event B has occurred gives no additional information about the likelihood of A occurring, and vice versa."
                },
                {
                    "front": "How can you test if two events are independent?",
                    "back": "To test for independence, check if the joint probability $P(A \\cap B)$ equals the product of the individual probabilities $P(A) \\times P(B)$. If the equality holds, the events are independent; otherwise, they are dependent."
                }
            ]
        },
        {
            "name": "Law of Total Probability",
            "cards": [
                {
                    "front": "What is the Law of Total Probability?",
                    "back": "The Law of Total Probability states that if you have a set of mutually exclusive events $B_1, B_2, ..., B_n$ that cover all possible outcomes, the probability of any event A can be expressed as: \\[ P(A) = \\sum_{i} P(A|B_i) P(B_i) \\] This law is useful for breaking down complex probability problems by considering all possible conditions."
                }
            ]
        }
    ]
}